<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>数学 - 标签 - 暮瞻</title>
        <link>https://blog.ralvines.top/tags/%E6%95%B0%E5%AD%A6/</link>
        <description>数学 - 标签 - 暮瞻</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 30 Oct 2023 20:20:40 &#43;0800</lastBuildDate><atom:link href="https://blog.ralvines.top/tags/%E6%95%B0%E5%AD%A6/" rel="self" type="application/rss+xml" /><item>
    <title>数学前沿专题讨论</title>
    <link>https://blog.ralvines.top/qianyan/</link>
    <pubDate>Mon, 30 Oct 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/qianyan/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/11/01/pinHqnH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2023-2024 秋冬<br>
🧑‍🏫 毕惟红<br>
📝 50%读书报告，10%考勤，10%编程，30%两次展示</div>
        </div>
    </div>
<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><ol>
<li>浙大 计算数学</li>
<li>研究生
<ul>
<li>数字模拟 投影法求曲面面积</li>
<li>随机数生成 数值代数 多元非线性方程组求解</li>
</ul>
</li>
</ol>
<h3 id="研究方向" class="headerLink">
    <a href="#%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91" class="header-mark"></a>研究方向</h3><ol>
<li>图像处理
<ul>
<li>图像分割 图像识别</li>
<li>图像加密 做的比较好</li>
</ul>
</li>
<li>语义识别
<ul>
<li>三维点式数据（无人驾驶、激光雷达）</li>
</ul>
</li>
<li>社区发现
<ul>
<li>复杂网络</li>
<li>拟牛顿</li>
</ul>
</li>
</ol>
<h2 id="遗传算法" class="headerLink">
    <a href="#%e9%81%97%e4%bc%a0%e7%ae%97%e6%b3%95" class="header-mark"></a>遗传算法</h2><h2 id="统计学习方法" class="headerLink">
    <a href="#%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95" class="header-mark"></a>统计学习方法</h2><h2 id="首次展示knn" class="headerLink">
    <a href="#%e9%a6%96%e6%ac%a1%e5%b1%95%e7%a4%baknn" class="header-mark"></a>首次展示：kNN</h2><h3 id="分类问题1" class="headerLink">
    <a href="#%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%981" class="header-mark"></a>分类问题<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h3><ul>
<li>
<p>一种监督学习问题，旨在对数据分类</p>
</li>
<li>
<p>将输入数据映射到预定义的类别或标签</p>
</li>
<li>
<p>从已知的训练数据中学习一个分类模型，然后将该模型应用于新的、未知的数据，以预测其所属的类别</p>
</li>
<li>
<p>垃圾邮件过滤、金融风险评估</p>
</li>
<li>
<p>医学诊断、生物信息学</p>
</li>
<li>
<p>情感分析、客户分类</p>
</li>
<li>
<p>图像识别</p>
</li>
</ul>
<h3 id="knn模型构建" class="headerLink">
    <a href="#knn%e6%a8%a1%e5%9e%8b%e6%9e%84%e5%bb%ba" class="header-mark"></a>kNN模型构建</h3><h4 id="提出2" class="headerLink">
    <a href="#%e6%8f%90%e5%87%ba2" class="header-mark"></a>提出<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></h4><p>$$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$
$$y_i\in\mathcal{Y}={c_1,c_2,\cdots,c_K}$$
$$y=\text{arg}\max\limits_{c_j} \sum\limits_{x_i\in N_k(x)} I(y_i=c_j)$$</p>
<ul>
<li>
<p>输入：特征向量（空间点）</p>
</li>
<li>
<p>输出：类别（可以取多类）</p>
</li>
<li>
<p>已标注的训练集</p>
</li>
<li>
<p>预测：多数表决（“近朱者赤” ）</p>
</li>
<li>
<p>不具有显式的学习过程</p>
</li>
</ul>
<p><strong>适用范围</strong></p>
<ul>
<li>数值型和标称型<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></li>
</ul>
<p><strong>优点</strong><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<ol>
<li>直观、非参数化</li>
<li>对异常值不敏感</li>
<li>支持多类别</li>
</ol>
<p><strong>缺点</strong><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<ol>
<li>时间复杂度高</li>
<li>存储成本高</li>
<li>“维度灾难”和数据不平衡</li>
</ol>
<h4 id="构建流程" class="headerLink">
    <a href="#%e6%9e%84%e5%bb%ba%e6%b5%81%e7%a8%8b" class="header-mark"></a>构建流程</h4><p>给定距离度量，k值与决策规则 [输入训练集T]</p>
<ol>
<li>在训练集 T 中找出与 x 最邻近的 k 个点，涵盖这 个点的 x 的邻域记作 $N_k(a)$</li>
<li>在 $N_k(a)$ 中根据分类决策规则决定 x 的类别 y</li>
</ol>
<p><strong>基本要素</strong></p>
<ol>
<li>k 值选择</li>
<li>距离度量</li>
<li>决策规则</li>
</ol>
<a class="lightgallery" href="https://z1.ax1x.com/2023/11/01/piuKE6K.png" title="特征空间划分" data-thumbnail="https://z1.ax1x.com/2023/11/01/piuKE6K.png">
</a>
<p class="img-desc" style="text-align: center">▲ 特征空间划分。</p>
<p>特殊情况：最近邻（k=1）</p>
<h3 id="模型要素" class="headerLink">
    <a href="#%e6%a8%a1%e5%9e%8b%e8%a6%81%e7%b4%a0" class="header-mark"></a>模型要素</h3><h4 id="k-值选择" class="headerLink">
    <a href="#k-%e5%80%bc%e9%80%89%e6%8b%a9" class="header-mark"></a>k 值选择</h4><table>
<thead>
<tr>
<th>k值</th>
<th>偏小</th>
<th>偏大</th>
</tr>
</thead>
<tbody>
<tr>
<td>近似误差</td>
<td>减小</td>
<td>增大</td>
</tr>
<tr>
<td>估计误差</td>
<td>增大</td>
<td>减小</td>
</tr>
</tbody>
</table>
<p>交叉验证以提高泛化性能。<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></p>
<h4 id="距离度量9" class="headerLink">
    <a href="#%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f9" class="header-mark"></a>距离度量<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></h4><p>对于
$$x_i=(x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})$$</p>
<ul>
<li>$L_p$ 距离
$$L_p(x_i,x_j)=(\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{1/p}$$</li>
<li>欧氏距离
$$L_2(x_i,x_j)=(\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^2)^{1/2}$$</li>
<li>曼哈顿距离
$$L_1(x_i,x_j)=\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|$$</li>
<li>$L_\infty$ 距离
$$L_\infty (x_i,x_j)=\max\limits_l |x_i^{(l)}-x_j^{(l)}|$$</li>
</ul>
<h4 id="决策规则" class="headerLink">
    <a href="#%e5%86%b3%e7%ad%96%e8%a7%84%e5%88%99" class="header-mark"></a>决策规则</h4><p><strong>多数表决</strong>
由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。</p>
<ul>
<li>分类函数
$$f:\mathbb{R}^n\rightarrow {c_1,c_2,\cdots,c_K}$$</li>
<li>误分类概率
$$P(Y\ne f(X))=1-P(Y=f(X))$$</li>
<li>等价于风险经验最小化
$$\frac{1}{k}\sum\limits_{x_i\in N_k(x)} I(y_i\ne c_j)=1-\frac{1}{k}\sum\limits_{x_i\in N_k(x)} I(y_i=c_j)$$</li>
</ul>
<h3 id="模型预测" class="headerLink">
    <a href="#%e6%a8%a1%e5%9e%8b%e9%a2%84%e6%b5%8b" class="header-mark"></a>模型预测</h3><h4 id="预测流程" class="headerLink">
    <a href="#%e9%a2%84%e6%b5%8b%e6%b5%81%e7%a8%8b" class="header-mark"></a>预测流程</h4><p>首先引入最简单的思路：线性扫描的方法。</p>
<a class="lightgallery" href="https://z1.ax1x.com/2023/11/01/piuKkSx.png" title="kNN算法流程可视化" data-thumbnail="https://z1.ax1x.com/2023/11/01/piuKkSx.png">
</a>
<p class="img-desc" style="text-align: center">▲ kNN算法流程可视化。</p>
<ol>
<li>对未知类别的数据集中的每个点：
<ul>
<li>计算已知类别数据集众多点与当前点之间的距离；</li>
<li>按照距离递增次序排序。</li>
</ul>
</li>
<li>选取与当前点距离最小的k个点：
<ul>
<li>选定前k个点所在类别的出现频率</li>
<li>返回前k个点出现频率最高的类别作为当前点的预测分类</li>
</ul>
</li>
<li>重复步骤，完成对所有点的预测分类</li>
</ol>
<h4 id="python实现" class="headerLink">
    <a href="#python%e5%ae%9e%e7%8e%b0" class="header-mark"></a>Python实现</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_nearest_labels</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">most_common</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">k_nearest_labels</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">most_common</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 一个简单的例子：</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">clf</span> <span class="o">=</span> <span class="n">KNN</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="改进" class="headerLink">
    <a href="#%e6%94%b9%e8%bf%9b" class="header-mark"></a>改进</h3><h4 id="主要挑战" class="headerLink">
    <a href="#%e4%b8%bb%e8%a6%81%e6%8c%91%e6%88%98" class="header-mark"></a>主要挑战</h4><ol>
<li>前置处理：特征的选择</li>
<li>模型
<ul>
<li>合适的度量函数</li>
<li>合适的K值</li>
<li>降低训练和预测的复杂度</li>
</ul>
</li>
</ol>
<h4 id="kd树" class="headerLink">
    <a href="#kd%e6%a0%91" class="header-mark"></a>kd树</h4><p>一种二叉树数据结构，用于优化搜索算法。</p>
<p><strong>优势：</strong></p>
<ol>
<li>降低搜索维度</li>
<li>提高搜索效率</li>
<li>更少的存储需求</li>
<li>支持范围搜索</li>
</ol>
<p>可能因数据的特定分布而表现不佳。<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<h5 id="构造11" class="headerLink">
    <a href="#%e6%9e%84%e9%80%a011" class="header-mark"></a>构造<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></h5><a class="lightgallery" href="https://z1.ax1x.com/2023/11/01/piuKif1.png" title="kd树的构造" data-thumbnail="https://z1.ax1x.com/2023/11/01/piuKif1.png">
</a>
<p class="img-desc" style="text-align: center">▲ kd树的构造。</p>
<ol>
<li>构造根结点，使根结点对应于 k 维空间中包含所有实例点的超矩形区域。</li>
<li>递归（生成子结点）：
<ul>
<li>选择坐标轴和切分点，确定一个超平面</li>
<li>将当前超矩形区域切分为左右两个子区域</li>
<li>直到子区域内没有实例时终止。</li>
</ul>
</li>
<li>实例保存在相应的结点上。</li>
</ol>
<p><strong>如何选择：</strong></p>
<ul>
<li>空间切分参照：坐标轴</li>
<li>切分点的选择：中位数</li>
</ul>
<h5 id="搜索" class="headerLink">
    <a href="#%e6%90%9c%e7%b4%a2" class="header-mark"></a>搜索</h5><a class="lightgallery" href="https://z1.ax1x.com/2023/11/01/piuKAl6.png" title="kd树的搜索" data-thumbnail="https://z1.ax1x.com/2023/11/01/piuKAl6.png">
</a>
<p class="img-desc" style="text-align: center">▲ kd树的搜索。</p>
<h5 id="算法" class="headerLink">
    <a href="#%e7%ae%97%e6%b3%95" class="header-mark"></a>算法</h5><p>[输入] 已构造的 kd 树，目标点 x;</p>
<p>[输出] x 的 k 近邻。</p>
<ol>
<li>在 kd 树中找出包含目标点 x 的叶结点：从根结点出发，递归地向下访问 kd 树。若目标点 x 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止。</li>
<li>构建“当前 k 近邻点集”，将该叶结点插入“当前 k 近邻点集”，并计算该结点到目标点 x 的距离。</li>
<li>递归地向上回退，在每个结点进行以下操作:
<ul>
<li>如果“当前 k 近邻点集”的元素数量 &lt; k，则将该结点插入“当前 k 近邻点集”，并计算该结点到目标点 x 的距离;</li>
<li>如果“当前 k 近邻点集”的元素数量 = k，但该结点到目标点 x 的距离小于“当前 k 近邻点集”中最远 点到目标点 x 的距离，则将该结点插入“当前 k 近邻点集”，并删除原先的最远点。</li>
<li>检查另一子结点对应的区域是否与以目标点 x 为球心、以目标点 x 与“当前 k 近邻点集”中最远点的距离为半径的超球体相交。 如果相交，可能在另一个子结点对应的区域内存在距离目标点更近的点，移动到另一个子结点，接着，递归地进行 k 近邻搜索; 如果不相交，向上回退。</li>
</ul>
</li>
<li>当回退到根结点时，搜索结束(若此时“当前 k 近邻点集”中的元素不足 k 个，则需要访问另一半树的结点)。</li>
<li>最后的“当前 k 近邻点集”中的 k 个点即为 x 的 k 近邻点。</li>
</ol>
<h5 id="python实现-1" class="headerLink">
    <a href="#python%e5%ae%9e%e7%8e%b0-1" class="header-mark"></a>Python实现</h5><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">left</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">right</span> <span class="o">=</span> <span class="n">right</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">KdTree</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">create_Tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">dataset</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">mid_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">axis</span> <span class="o">=</span> <span class="n">depth</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span>
</span></span><span class="line"><span class="cl">        <span class="n">sort_dataset</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="n">axis</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">        <span class="n">mid_data</span> <span class="o">=</span> <span class="n">sort_dataset</span><span class="p">[</span><span class="n">mid_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_node</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">mid_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">left_data</span> <span class="o">=</span> <span class="n">sort_dataset</span><span class="p">[:</span><span class="n">mid_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">right_data</span> <span class="o">=</span> <span class="n">sort_dataset</span><span class="p">[</span><span class="n">mid_index</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_node</span><span class="o">.</span><span class="n">left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_Tree</span><span class="p">(</span><span class="n">left_data</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_node</span><span class="o">.</span><span class="n">right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_Tree</span><span class="p">(</span><span class="n">right_data</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">cur_node</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">new_data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">near_point</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="n">node</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span>
</span></span><span class="line"><span class="cl">            <span class="n">axis</span> <span class="o">=</span> <span class="n">depth</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">new_data</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">[</span><span class="n">axis</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dfs</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dfs</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">right</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span> <span class="ow">or</span> <span class="n">dist</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span> <span class="o">=</span> <span class="n">dist</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">near_point</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_data</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">-</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">new_data</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">[</span><span class="n">axis</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">dfs</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">right</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">dfs</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dfs</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">near_point</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">point_1</span><span class="p">,</span> <span class="n">point_2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">res</span> <span class="o">+=</span> <span class="p">(</span><span class="n">point_1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">point_2</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">res</span> <span class="o">**</span> <span class="mf">0.5</span>
</span></span><span class="line"><span class="cl"><span class="n">data_set</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">new_data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_set</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">kd_tree</span> <span class="o">=</span> <span class="n">KdTree</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">our_tree</span> <span class="o">=</span> <span class="n">kd_tree</span><span class="o">.</span><span class="n">create_Tree</span><span class="p">(</span><span class="n">data_set</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">predict</span> <span class="o">=</span> <span class="n">kd_tree</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">our_tree</span><span class="p">,</span> <span class="n">new_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="马氏距离" class="headerLink">
    <a href="#%e9%a9%ac%e6%b0%8f%e8%b7%9d%e7%a6%bb" class="header-mark"></a>马氏距离</h4><p>由P.C. Mahalanobis提出；基于样本分布的一种距离测量。</p>
<ul>
<li>考虑特征之间的相关性</li>
<li>对数据的缩放不敏感</li>
<li>考虑协方差结构</li>
<li>适用于异常值和噪声数据<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></li>
</ul>
<p>广泛用于分类和聚类分析。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>垃圾邮件过滤：自动将电子邮件分为垃圾邮件和非垃圾邮件。<br>
医学诊断：基于患者的症状数据来诊断疾病或预测病人的疾病风险。<br>
金融风险评估：根据客户的财务和信用记录来评估客户的信用风险。<br>
情感分析：根据文本数据中的情感内容对文本进行情感分类，如积极、消极或中性。<br>
图像识别：对图像进行分类，例如识别数字、物体或人脸等。<br>
生物信息学：基因序列分类，如预测蛋白质功能或基因表达模式。<br>
客户分类：根据客户的行为和偏好将客户分成不同的市场细分。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>KNN算法于1948年由Cover和Hart提出。<br>
存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，只选择样本数据集中前k个最相似的数据。k一般不大于20，最后，选择k个中出现次数最多的分类，作为新数据的分类。<br>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>数值型数据是指具有数量意义的数据，可以进行数学运算和比较。这种数据通常表示为数字，例如年龄、温度、身高等。在机器学习中，数值型数据常用于回归分析和连续变量的预测。标称型数据则是指无序分类的数据，其中每个值代表一个类别而没有数量意义。标称型数据通常表示为符号或字符串，例如血型、性别、品种等。在机器学习中，标称型数据通常用于分类问题，其中算法需要将输入数据映射到预定义的类别或标签。<br>
k最近邻算法 (kNN) 适用于处理这两种类型的数据。对于数值型数据，它可以基于数值之间的距离进行分类；对于标称型数据，它可以根据邻近样本的标签进行投票，并将测试样本分类为获得最多投票的类别。因此，kNN 算法对于这两种数据类型都有较好的适用性。<br>
还有其他类型：<br>
顺序型数据：顺序型数据是一种具有顺序或等级关系的数据类型，其中数据值之间存在某种顺序关系，但没有明确的数值差异。例如，学历等级（如小学、初中、高中、大学等）可以被视为顺序型数据。<br>
时间序列数据：时间序列数据是按照时间顺序排列的数据集合，通常是在一系列连续时间点上收集的数据。例如，股票价格、天气数据、经济指标等都属于时间序列数据。<br>
区间型数据：区间型数据是指数据值表示某个范围内的值，而不是特定的数值。这种数据类型通常用于表示测量的范围。例如，温度范围、年龄段等可以被视为区间型数据。<br>
比率型数据：比率型数据是具有固定比例关系的数据类型，其中数据之间存在明确的比率关系。比率型数据具有绝对零点，可以进行比较和数学运算。例如，长度、重量、时间间隔等都属于比率型数据。<br>
文本数据：文本数据是指以自然语言形式表示的数据，通常包含语句、段落或文档。处理文本数据通常需要使用自然语言处理技术来提取、转换和分析文本信息。<br>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>老师反馈：这里的表述并不严谨，模型需要通过交叉验证来确认参数 k&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>简单直观，非参数化：kNN 是一种非参数化方法，不对数据的分布做任何假设。因此，在处理复杂的数据集和未知的数据分布时，它通常具有很好的适应性。<br>
对异常值鲁棒：kNN 对异常值比较鲁棒，因为它基于周围数据点的多数投票来确定分类，可以减少异常值对结果的影响。<br>
适应多类别问题：kNN 能够很好地适应多类别分类问题，因为它可以通过投票的方式来确定一个实例所属的类别。<br>
但是对高维数据的处理效率较低，需要大量的存储空间和计算时间；在数据不平衡或噪声较多的情况下，它可能会产生较差的分类结果。<br>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>计算成本高：kNN 算法需要计算每个测试点与所有训练点之间的距离，因此在处理大规模数据集时，计算成本会变得非常高。<br>
存储成本高：除了计算成本高外，kNN 算法还需要存储整个训练集，这对于大规模数据集来说会占用大量的存储空间。<br>
维度灾难：随着数据维度的增加，kNN 算法的性能可能会下降，因为在高维空间中，数据点之间的距离变得更加稀疏，导致算法的效率降低。<br>
数据不平衡问题：在处理数据不平衡或噪声较多的数据集时，kNN 算法可能会受到数据分布的影响，从而导致分类性能下降。<br>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>近似误差是指模型用于近似真实关系的误差，通常表示模型与真实值之间的差异。<br>
估计误差是指使用样本数据估计整体数据集特征时产生的误差。在 kNN 中，估计误差通常与样本的选择和样本的分布有关。<br>
K值的减小：模型变得复杂，容易发生过拟合。相当于用较小的邻域中的训练实例进行预测，只有与输入实例较近的(相似的)训练实例才会对预测结果起作用，对噪声敏感，即预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。<br>
K值的增大：就意味着整体的模型变得简单.产生更平滑的决策边界，但可能会忽略数据的局部特征。这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误。k 值的增大就意味着整体的模型变得简单。<br>
k 值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k 值。具体可以取部分训练集作为测试集，在不同取值条件下观察最优值。&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>特征空间中两个实例点的距离是两个实例点相似程度的反映。k 近邻模型的特征空间一般是 n 维实数向量空间 $R^n$，使用的距离是欧氏距离，但也可以是其他距离，如更一般的 $L_p$ 距离或 Minkowski 距离。&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>实现 k 近邻法时，主要考虑的问题是如何对训练数据进行快速 k 近邻搜索。这点在特征空间的维数大及训练数据容量大时尤其必要。k 近邻法最简单的实现方法是线性扫描。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。为了提高k 近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。具体方法很多，下面介绍其中的 kd 树方法。<br>
使用 kd 树相比直接计算方法的主要好处在于它可以有效地减少计算量。kd 树是一种二叉树数据结构，它可以用于优化搜索算法，特别是在高维空间中。<br>
以下是 kd 树相对于直接计算方法的一些优势：<br>
降低搜索维度：kd 树能够将搜索范围缩小到与搜索点最近的局部区域，从而避免不必要的计算。<br>
提高搜索效率：在具有大量数据点的高维空间中，kd 树可以更快地定位最近邻居，因为它可以避免对所有数据点进行逐一比较。<br>
更少的存储需求：相对于直接计算方法，kd 树通常需要更少的存储空间，因为它可以通过二叉树结构有效地组织数据。<br>
支持范围搜索：除了最近邻搜索之外，kd 树还可以很容易地扩展到支持范围搜索，以查找在给定半径内的所有邻居。<br>
尽管 kd 树具有这些优势，但它可能会因数据的特定分布而表现不佳。例如，在存在大量密集聚集数据点的区域，kd 树的性能可能会下降。因此，在实际应用中，应该根据数据集的特点选择合适的算法来进行近邻搜索。&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>kd 树是一种对 k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd 树是二叉树，表示对 k 维空间的一个划分。构造 kd 树相当于不断地用垂直于坐标轴的超平面将k 维空间切分，构成一系列的飞 维超矩形区域。kd树的每个结点对应于一个k 维超矩形区域。<br>
构造 kd 树的方法如下:构造根结点，使根结点对应于 k 维空间中包含所有实例点的超矩形区域:通过下面的递归方法，不断地对 k 维空间进行切分，生成子结点。在超矩形区域(结点)上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。<br>
平衡树：使用中位数作为划分点可以保证树的相对平衡，避免出现极端情况下的不平衡树结构，从而使得搜索效率总体比较高，但未必最优。考虑一些离群点。&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>考虑特征之间的相关性：马氏距离能够考虑数据特征之间的相关性，而欧氏距离只考虑各个维度之间的直线距离。这意味着马氏距离在具有相关特征的数据集上能够提供更加准确的距离度量。<br>
对数据的缩放不敏感：在某些情况下，数据的不同特征可能具有不同的度量单位或尺度。马氏距离能够对数据的缩放不敏感，因此可以更好地处理这种情况，而欧氏距离可能受到数据尺度的影响。<br>
考虑协方差结构：马氏距离考虑了数据的协方差结构，因此可以更好地捕捉数据特征之间的线性关系。这使得马氏距离在处理多元正态分布数据时能够提供更加准确的距离度量。<br>
适用于异常值和噪声数据：马氏距离能够对异常值和噪声数据具有更好的鲁棒性，因为它考虑了数据的协方差结构，可以减少这些异常值对距离度量的影响。&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>]]></description>
</item><item>
    <title>高等代数学习与备考</title>
    <link>https://blog.ralvines.top/gdds/</link>
    <pubDate>Tue, 01 Aug 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/gdds/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/11/05/piQylSs.png" referrerpolicy="no-referrer">
            </div><h2 id="大纲" class="headerLink">
    <a href="#%e5%a4%a7%e7%ba%b2" class="header-mark"></a>大纲</h2><h3 id="书目" class="headerLink">
    <a href="#%e4%b9%a6%e7%9b%ae" class="header-mark"></a>书目</h3><ul>
<li><i class="far fa-square fa-fw"></i> 北大</li>
<li><i class="far fa-square fa-fw"></i> 李炯生</li>
<li><i class="far fa-square fa-fw"></i> 丘维声</li>
<li><i class="far fa-square fa-fw"></i> 谢启鸿</li>
<li><i class="far fa-square fa-fw"></i> 李杨
<ul>
<li><i class="far fa-square fa-fw"></i> 每日一题</li>
<li><i class="far fa-square fa-fw"></i> 强化讲义</li>
<li><i class="far fa-square fa-fw"></i> 重点</li>
</ul>
</li>
<li><i class="far fa-square fa-fw"></i> 樊启斌</li>
<li><i class="far fa-square fa-fw"></i> 真题</li>
</ul>
<h3 id="重点" class="headerLink">
    <a href="#%e9%87%8d%e7%82%b9" class="header-mark"></a>重点</h3><ul>
<li>行列式：计算</li>
<li>矩阵：加乘运算，初等变换，分块，逆矩阵，伴随矩阵，秩，迹</li>
<li>线性空间：线性相关，基，过渡矩阵，子空间</li>
<li>线性方程组：高斯消元法，解空间性质</li>
<li>线性变换：像空间核空间，维数公式，不变子空间，线性同构</li>
<li>多项式：整除，带余除法，公因式，互素，不可约多项式，对称多项式，结式，判别式</li>
<li>特征值：求解，相似对角化，特征子空间，特征多项式，极小多项式，</li>
<li>相似标准型：λ矩阵，不变因子，初等因子，Jordan标准型，特征根子空间，矩阵函数</li>
<li>二次型：正定，合同，实对称矩阵，Hermite矩阵</li>
<li>内积空间：内积，范数，正交，正规算子，奇异值分解，镜面反射</li>
</ul>
<h2 id="ch1-线性方程组" class="headerLink">
    <a href="#ch1-%e7%ba%bf%e6%80%a7%e6%96%b9%e7%a8%8b%e7%bb%84" class="header-mark"></a>CH1. 线性方程组</h2><ul>
<li>矩阵消元
<ul>
<li>Def 矩阵
<ul>
<li>系数 增广</li>
<li>初等变换 行变换</li>
<li>阶梯</li>
<li>简化 主元1，其余为0</li>
</ul>
</li>
<li>Th 任意初等阶梯化 归纳法
<ul>
<li>Col 推论 简化</li>
</ul>
</li>
<li>Def 解
<ul>
<li>无解 唯一解 无穷解</li>
<li>一般解 主变量 自由未知量</li>
<li>可行解</li>
</ul>
</li>
</ul>
</li>
<li>G-J 解方程组
<ul>
<li>Def
<ul>
<li>有解 相容 齐次</li>
<li><strong>r&amp;n</strong> 列</li>
<li>零解</li>
</ul>
</li>
<li>Th 判别有解/无解/无穷解
<ul>
<li>r&amp;n</li>
</ul>
</li>
<li>Col 判别齐次方程非零解
<ul>
<li>充要条件</li>
<li>必要条件 方程数目</li>
</ul>
</li>
</ul>
</li>
<li>数域 K
<ul>
<li>Def
<ul>
<li>单位元和封闭性</li>
<li>Q R C</li>
</ul>
</li>
<li>Th 有理数域最小 复数域最大</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>Notes
<ul>
<li>矩阵计算草稿打清楚</li>
</ul>
</li>
<li>题型技巧
<ul>
<li>初等变换 阶梯化</li>
<li>解方程组
<ul>
<li>各行和相同
<ul>
<li>某系数移位 如对角 y=sum 直求 <em>补充1</em></li>
<li>各系数错位 y=sum 邻行相减 同上 <em>补充2</em></li>
</ul>
</li>
<li>自变量阶梯型 从n+1开始移项 向前相邻相减 <em>补充3</em></li>
</ul>
</li>
<li>应用题</li>
<li>判断解的情况</li>
<li>证明数域</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="ch2-行列式" class="headerLink">
    <a href="#ch2-%e8%a1%8c%e5%88%97%e5%bc%8f" class="header-mark"></a>CH2. 行列式</h2><ul>
<li>引入
<ul>
<li>二元一次方程组唯一解 充要 二阶行列式不为0</li>
<li>全排列 n元排列 顺序 逆序 逆序数 $\tau$</li>
<li>奇/偶排列 对换改变奇偶性</li>
<li><em>1-n ab组合逆序数关系</em> 例4结论</li>
<li><em>全部n元排列中奇偶排列各占一半</em> 例6</li>
</ul>
</li>
<li>n阶
<ul>
<li>正负由指标奇偶排列引入 $n!$项</li>
<li>完全展开式 由纵指标n元排列导出</li>
</ul>
</li>
<li>性质</li>
<li>按行展开
<ul>
<li>化上三角</li>
<li>拆成若干和</li>
<li>各行元素相同时 各列加到首列上</li>
<li>按行展开</li>
<li>归纳</li>
<li>递推关系</li>
<li>加边升阶</li>
<li>利用范德蒙行列式</li>
</ul>
</li>
<li>Cramer</li>
<li>按k行展开</li>
</ul>
<h2 id="ch3-解集结构" class="headerLink">
    <a href="#ch3-%e8%a7%a3%e9%9b%86%e7%bb%93%e6%9e%84" class="header-mark"></a>CH3. 解集结构</h2><ul>
<li>向量空间</li>
<li>线性相关向量组</li>
<li>向量组秩</li>
<li>子空间基 维数</li>
<li>矩阵秩</li>
<li>有解充要条件</li>
<li>齐次解集结构</li>
<li>非齐次解集</li>
</ul>
<h2 id="ch4-矩阵运算" class="headerLink">
    <a href="#ch4-%e7%9f%a9%e9%98%b5%e8%bf%90%e7%ae%97" class="header-mark"></a>CH4. 矩阵运算</h2><ul>
<li>计算</li>
<li>特殊矩阵</li>
<li>乘积的秩 行列式</li>
<li>可逆</li>
<li>分块</li>
<li>正交</li>
<li>线性映射</li>
</ul>
<h2 id="ch5-相抵相似" class="headerLink">
    <a href="#ch5-%e7%9b%b8%e6%8a%b5%e7%9b%b8%e4%bc%bc" class="header-mark"></a>CH5. 相抵相似</h2><ul>
<li>等价关系 集合划分</li>
<li>相抵</li>
<li>广义逆</li>
<li>相似</li>
<li>特征值 特征向量</li>
<li>可对角化条件</li>
<li>实对称对角化</li>
</ul>
<h2 id="ch6-二次型-合同" class="headerLink">
    <a href="#ch6-%e4%ba%8c%e6%ac%a1%e5%9e%8b-%e5%90%88%e5%90%8c" class="header-mark"></a>CH6. 二次型 合同</h2><ul>
<li>二次型 标准形</li>
<li>实二次型 规范形</li>
<li>正定二次型 正定矩阵</li>
</ul>
<h2 id="ch7-多项式环" class="headerLink">
    <a href="#ch7-%e5%a4%9a%e9%a1%b9%e5%bc%8f%e7%8e%af" class="header-mark"></a>CH7. 多项式环</h2><ul>
<li>一元环</li>
<li>整除 带余除法</li>
<li>最大公因式</li>
<li>不可约多项式 唯一因式分解</li>
<li>重因式</li>
<li>多项式根 复数域不可约多项式</li>
<li>实数域不可约多项式 实系数多项式根</li>
<li>有理数域不可约多项式</li>
<li>多元环</li>
<li>对称多项式</li>
<li>域 域上一元环</li>
</ul>
<h2 id="ch8-线性空间" class="headerLink">
    <a href="#ch8-%e7%ba%bf%e6%80%a7%e7%a9%ba%e9%97%b4" class="header-mark"></a>CH8. 线性空间</h2><ul>
<li>F上基 维数</li>
<li>子空间交 和 直和</li>
<li>F上同构</li>
<li>商空间</li>
</ul>
<h2 id="ch9-线性映射" class="headerLink">
    <a href="#ch9-%e7%ba%bf%e6%80%a7%e6%98%a0%e5%b0%84" class="header-mark"></a>CH9. 线性映射</h2><ul>
<li>运算</li>
<li>核 象</li>
<li>线性变换 矩阵表示</li>
<li>特征值 特征向量 可对角化条件</li>
<li>不变子空间 H-C</li>
<li>线性变换 矩阵最小多项式</li>
<li>幂零变换Jordan标准形</li>
<li>线性变换Jordan标准形</li>
<li>线性函数 对偶空间</li>
</ul>
<h2 id="ch10-度量线性空间" class="headerLink">
    <a href="#ch10-%e5%ba%a6%e9%87%8f%e7%ba%bf%e6%80%a7%e7%a9%ba%e9%97%b4" class="header-mark"></a>CH10. 度量线性空间</h2><ul>
<li>双线性函数</li>
<li>欧氏空间</li>
<li>正交补 正交投影</li>
<li>正交变换 对称变换</li>
<li>酉空间 酉变换 Hermite变换 正规变换</li>
<li>多项式</li>
</ul>]]></description>
</item><item>
    <title>数学分析学习与备考</title>
    <link>https://blog.ralvines.top/sxfx/</link>
    <pubDate>Tue, 01 Aug 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/sxfx/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/11/05/piQy1ln.png" referrerpolicy="no-referrer">
            </div><h2 id="大纲" class="headerLink">
    <a href="#%e5%a4%a7%e7%ba%b2" class="header-mark"></a>大纲</h2><h3 id="书目" class="headerLink">
    <a href="#%e4%b9%a6%e7%9b%ae" class="header-mark"></a>书目</h3><ul>
<li><i class="far fa-square fa-fw"></i> 华师大</li>
<li><i class="far fa-square fa-fw"></i> 史济怀</li>
<li><i class="far fa-square fa-fw"></i> 裴礼文</li>
<li><i class="far fa-square fa-fw"></i> 谢惠民</li>
<li><i class="far fa-square fa-fw"></i> 李杨
<ul>
<li><i class="far fa-square fa-fw"></i> 每日一题</li>
<li><i class="far fa-square fa-fw"></i> 强化讲义</li>
<li><i class="far fa-square fa-fw"></i> 重点</li>
</ul>
</li>
<li><i class="far fa-square fa-fw"></i> 真题</li>
<li><i class="far fa-square fa-fw"></i> 方法及例题选讲</li>
</ul>
<h3 id="重点" class="headerLink">
    <a href="#%e9%87%8d%e7%82%b9" class="header-mark"></a>重点</h3><ul>
<li>极限：定义，Cauchy收敛，单调有界，致密性，Stolz定理，递推数列极限，等价无穷小（大），函数的阶</li>
<li>实数完备性：七个定理互证，用七个定理证明其他的定理，上下极限</li>
<li>连续函数：定义，介值，最值，一致连续，间断点类型，</li>
<li>一元函数微分：定义，求导，中值定理，达布定理，单侧导数，洛必达法则，泰勒展开，函数的极值与最值，凸函数</li>
<li>一元函数积分：不定积分，黎曼可积，微积分基本定理，定积分计算，积分中值定理，积分不等式，无穷积分，瑕积分</li>
<li>级数：各种判别法，各种收敛，幂级数，Fourier级数</li>
<li>多元函数：高维空间的点集，多元函数极限，连续性，偏导数，方向导数，微分，隐函数，条件极值，泰勒公式，切平面，法线</li>
<li>重积分：计算，变量替换，广义积分，物理应用，含参变量积分</li>
<li>曲线面积分：第一第二型，格林公式，高斯公式，斯托克斯公式</li>
</ul>
<h2 id="ch0-数学基础" class="headerLink">
    <a href="#ch0-%e6%95%b0%e5%ad%a6%e5%9f%ba%e7%a1%80" class="header-mark"></a>CH0. 数学基础</h2><ul>
<li>三角函数
<ul>
<li>特殊三角函数 cot</li>
<li>反三角函数 值域 arccos $[0,\pi]$</li>
<li>万能 倍角 半角</li>
<li>积化和差</li>
<li>交替三角函数</li>
<li>三角恒等式</li>
<li>倍数连乘&amp;连加 利用积化和差</li>
<li>arctan的两条结论 $1/2k^2$, 倒数关系</li>
<li>三角&amp;反三角不等式 $2x/π&lt; sinx$</li>
</ul>
</li>
<li>不等式
<ul>
<li>均值不等式
<ul>
<li>调和 几何 算术 平方</li>
</ul>
</li>
<li>Jensen不等式</li>
<li>对数不等式 调和级数不等式</li>
</ul>
</li>
<li>特殊函数
<ul>
<li>取整符号$<i class="far fa-check-square fa-fw"></i> $ 注意负数 和x的关系</li>
<li>最值的绝对值表达</li>
</ul>
</li>
<li>多项式
<ul>
<li>$x^k-1$, $a^3+b^3$, 四次方和差</li>
<li>次方和 $\sum x^2$</li>
</ul>
</li>
<li>几何
<ul>
<li>仿射变换的体积关系</li>
</ul>
</li>
</ul>
<h2 id="ch1-实数集-函数" class="headerLink">
    <a href="#ch1-%e5%ae%9e%e6%95%b0%e9%9b%86-%e5%87%bd%e6%95%b0" class="header-mark"></a>CH1. 实数集 函数</h2><ul>
<li>实数-集合-函数-特殊函数</li>
<li>实数
<ul>
<li>Def 实数
<ul>
<li>有理数 无理数</li>
<li>新的表示方法
<ul>
<li>有限小数的无限表示</li>
<li>任意实数的小数表示</li>
<li>非负实数大小比较</li>
</ul>
</li>
<li>有理数四则运算封闭性（用于证明无理数）</li>
<li>n位不足近似 $x_n$</li>
<li>n位过剩近似 $\overline{x_n}$</li>
</ul>
</li>
<li>Th 大小比较
<ul>
<li>$x&gt;y$ 实数比较的等价条件 $x_n&gt;\overline{y_n}$
<ul>
<li>Col 之间存在有理数 即<strong>稠密 <em>Ex1</em></strong></li>
</ul>
</li>
<li>$a\le b$ 小于等于的必要条件 $a&lt;b+\epsilon$</li>
</ul>
</li>
<li>Th 实数性质
<ul>
<li>封闭 有序 传递 <strong>阿基米德</strong> 稠密</li>
<li>数轴</li>
</ul>
</li>
<li>绝对值
<ul>
<li>Def</li>
<li>Th 性质
<ul>
<li>非负 相乘 相除 不等式 三角不等式</li>
</ul>
</li>
</ul>
</li>
<li>三角不等式</li>
</ul>
</li>
<li>数集
<ul>
<li>Def
<ul>
<li>区间
<ul>
<li>有限 无限 开 闭 半开半闭</li>
</ul>
</li>
<li>邻域
<ul>
<li>左右 空心</li>
<li>无穷邻域 M 正负</li>
</ul>
</li>
<li>有界集
<ul>
<li>上界 下界 $\le$</li>
<li>上下确界 $\le, &lt;, &gt;$</li>
<li>非正常确界</li>
</ul>
</li>
</ul>
</li>
<li>Th
<ul>
<li><strong>确界原理</strong> 有界$\Rightarrow$有确界
<ul>
<li><strong>证明</strong> 不断十等分 利用n位近似和稠密性</li>
<li>重要结论 下界&lt;=下确界 上界&gt;=上确界</li>
</ul>
</li>
<li>集合元素大小比较$\Rightarrow$确界比较 $\sup A\le\inf B$</li>
<li>并集确界与$\max, \min$的充要条件</li>
<li>推广确界原理 任意非空必有广义确界</li>
</ul>
</li>
</ul>
</li>
<li>函数
<ul>
<li>Def
<ul>
<li>映射=单值对应 <del>多值函数</del></li>
<li>函数相同=定义域和法则</li>
<li>形式多样</li>
<li>表示法
<ul>
<li>只能用语言描述</li>
</ul>
</li>
<li>四则运算 注意存在域是交集</li>
<li>复合函数 注意交域</li>
<li>反函数</li>
<li>算术根 <strong>n次正根</strong> 存在唯一</li>
<li>初等
<ul>
<li>基本初等 常量 幂 指数 对数 三角 反三角</li>
<li>有限次基本运算</li>
</ul>
</li>
<li>有理&amp;<strong>无理指数幂</strong></li>
</ul>
</li>
<li>Ex
<ul>
<li>Dirichlet</li>
<li>sgn</li>
<li>Riemann</li>
</ul>
</li>
<li>Th 算术根唯一性</li>
</ul>
</li>
<li>特殊函数
<ul>
<li>有界
<ul>
<li>Def
<ul>
<li>上下界</li>
<li>有界 无界</li>
</ul>
</li>
<li>加法不等式 严格成立情况</li>
</ul>
</li>
<li>单调 $&lt;, \le$
<ul>
<li>Th <strong>严格单调有严格反函数</strong></li>
<li>Ex 指数&amp;对数函数严格单调性</li>
</ul>
</li>
<li>奇偶</li>
<li>周期
<ul>
<li>Def 基本周期</li>
<li>Ex 常量函数有周期无基本周期</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>补充
<ul>
<li>完全平方数 自身相乘 互质</li>
<li>取整 <i class="far fa-square fa-fw"></i> </li>
</ul>
</li>
<li>题型
<ul>
<li>证明无理数
<ul>
<li>反证法 有理数封闭性</li>
</ul>
</li>
<li>解不等式 数轴表示</li>
<li>实数大小比较 证明
<ul>
<li>利用大小比较的等价条件和必要条件</li>
</ul>
</li>
<li>含绝对值/根式不等式证明
<ul>
<li>两边平方</li>
<li>多项不等式 讨论情况去绝对值</li>
<li>柯西不等式 均值不等式</li>
</ul>
</li>
<li>证明有界无界 或求上下界
<ul>
<li>数集/函数 定义</li>
</ul>
</li>
<li>证明或求确界 确界不等式
<ul>
<li>定义</li>
</ul>
</li>
<li>函数作图</li>
<li>求函数
<ul>
<li>求值</li>
<li>求定义域</li>
<li>求复合函数</li>
<li>求反函数</li>
<li><strong>注意可行域</strong>
<ul>
<li>根号内部</li>
</ul>
</li>
</ul>
</li>
<li>证明初等</li>
<li>函数不等式证明</li>
<li>单调性/奇偶性证明
<ul>
<li>对数函数性质</li>
</ul>
</li>
<li>求周期</li>
</ul>
</li>
</ul>
</blockquote>
<h2 id="ch2-数列极限" class="headerLink">
    <a href="#ch2-%e6%95%b0%e5%88%97%e6%9e%81%e9%99%90" class="header-mark"></a>CH2. 数列极限</h2><ul>
<li>Def
<ul>
<li>$\epsilon-N$ 收敛
<ul>
<li>用定义求极限
<ul>
<li>放缩 $(1+nh)$ 均值不等式二项式展开</li>
<li>对数严格单调性</li>
<li>经典极限
<ul>
<li>$q^n$</li>
<li>$^n\sqrt{a}$</li>
<li>$a^n/n!$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>无穷小 无穷大 正负发散无穷
<ul>
<li>无界但非无穷大量</li>
<li>发散和无穷大的区别</li>
</ul>
</li>
<li>几何意义 邻域等价定义
<ul>
<li>发散 $n^2$ $(-1)^n$</li>
<li>交替排序数列收敛充要条件</li>
<li>有限改变 同时敛散</li>
</ul>
</li>
</ul>
</li>
<li>收敛数列性质
<ul>
<li>唯一</li>
<li>有界 必要</li>
<li>保号 推论比较 严格</li>
<li>保不等式 <em>能否严格</em></li>
<li>迫敛性 求极限
<ul>
<li>$^n\sqrt{n}$</li>
<li>$1/^n\sqrt{n!}$</li>
</ul>
</li>
<li>四则运算
<ul>
<li>和积倒数证明</li>
</ul>
</li>
<li>子列
<ul>
<li>子列收敛充要</li>
<li>用子列判断整体发散
<ul>
<li>发散子列</li>
<li>子列极限不相等</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>存在条件</strong>
<ul>
<li>单调有界定理</li>
<li>证有界
<ul>
<li>拆奇偶 找形式放缩</li>
<li>利用单调性 递推式归纳法</li>
</ul>
</li>
<li>求极限 递推公式两边取极限</li>
<li>单调子列存在性</li>
<li>$(1+1/n)^n$</li>
<li>致密性定理
<ul>
<li>非内部上确界 <em>构造严格增数列逼近</em></li>
</ul>
</li>
<li>充要 柯西准则</li>
<li>重要结论
<ul>
<li>$(1+1/n)^n$ 单增 $(1+1/n)^n+1$ 单减</li>
<li>T7 $a_n/a_{n+1}\rightarrow l&gt;1,\lim a_n=0$</li>
<li>T8 递增有界 lim=sup</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ch3-函数极限" class="headerLink">
    <a href="#ch3-%e5%87%bd%e6%95%b0%e6%9e%81%e9%99%90" class="header-mark"></a>CH3. 函数极限</h2><ul>
<li>
<p>定义-性质-存在条件</p>
</li>
<li>
<p>Def</p>
<ul>
<li>$\rightarrow \infty$ M</li>
<li>$\rightarrow x_0$ $\epsilon-\delta$</li>
<li>集合角度定义</li>
</ul>
</li>
<li>
<p>性质</p>
<ul>
<li>唯一 局部有界 保号 保不等式 迫敛 四则运算</li>
</ul>
</li>
<li>
<p>条件</p>
<ul>
<li>柯西收敛准则</li>
<li>归结原则 左右极限的更强形式</li>
<li>单调有界</li>
</ul>
</li>
<li>
<p>重要极限</p>
<ul>
<li>sinx/x $\rightarrow 0$</li>
<li>(1+1/x)^x $\rightarrow \infty$</li>
</ul>
</li>
<li>
<p>无穷量</p>
<ul>
<li>无穷小量&amp;有界量
<ul>
<li>性质</li>
</ul>
</li>
<li>无穷小量阶比较
<ul>
<li>高阶</li>
<li>同阶</li>
<li>k阶</li>
<li>等价 应用</li>
</ul>
</li>
<li>无穷大量</li>
<li>渐近线</li>
</ul>
</li>
<li>
<p><strong>CH4. 函数连续性</strong> 0910</p>
</li>
<li>
<p>概念</p>
<ul>
<li>在一点的连续性
<ul>
<li>左右连续</li>
<li>间断点
<ul>
<li>第一类
<ul>
<li>可去</li>
<li>跳跃</li>
</ul>
</li>
<li>第二类</li>
</ul>
</li>
</ul>
</li>
<li>区间连续性</li>
</ul>
</li>
<li>
<p>性质</p>
<ul>
<li>局部</li>
<li>闭区间</li>
<li>反函数</li>
<li>一致连续</li>
</ul>
</li>
<li>
<p>指数函数连续性</p>
</li>
<li>
<p>初等函数连续性</p>
</li>
</ul>
<h2 id="ch5-导数微分" class="headerLink">
    <a href="#ch5-%e5%af%bc%e6%95%b0%e5%be%ae%e5%88%86" class="header-mark"></a>CH5. 导数微分</h2><ul>
<li>导数
<ul>
<li>定义</li>
<li>导函数</li>
<li>几何</li>
</ul>
</li>
<li>求导
<ul>
<li>运算</li>
<li>反函数</li>
<li>复合</li>
<li>法则公式</li>
</ul>
</li>
<li>参变量函数</li>
<li>高阶</li>
<li>微分
<ul>
<li>运算</li>
<li>高阶</li>
<li>近似计算</li>
</ul>
</li>
</ul>
<h2 id="ch6-微分中值定理" class="headerLink">
    <a href="#ch6-%e5%be%ae%e5%88%86%e4%b8%ad%e5%80%bc%e5%ae%9a%e7%90%86" class="header-mark"></a>CH6. 微分中值定理</h2><ul>
<li>罗尔 拉格朗日</li>
<li>单调函数</li>
<li>柯西中值</li>
<li>不定式极限</li>
<li>泰勒
<ul>
<li>佩亚诺余项</li>
<li>拉格朗日余项</li>
<li>近似计算</li>
</ul>
</li>
<li>极值判别</li>
<li>最值</li>
<li>凸性 拐点</li>
<li>图像</li>
</ul>
<h2 id="ch7-实数完备性" class="headerLink">
    <a href="#ch7-%e5%ae%9e%e6%95%b0%e5%ae%8c%e5%a4%87%e6%80%a7" class="header-mark"></a>CH7. 实数完备性</h2><ul>
<li>区间套</li>
<li>聚点 有限覆盖</li>
<li>完备性定理等价性</li>
</ul>
<h2 id="ch8-不定积分" class="headerLink">
    <a href="#ch8-%e4%b8%8d%e5%ae%9a%e7%a7%af%e5%88%86" class="header-mark"></a>CH8. 不定积分</h2><ul>
<li>原函数 不定积分</li>
<li>积分表</li>
<li>换元法</li>
<li>分部法</li>
<li>有理函数</li>
<li>三角函数有理式</li>
<li>无理式</li>
</ul>
<h2 id="ch9-定积分" class="headerLink">
    <a href="#ch9-%e5%ae%9a%e7%a7%af%e5%88%86" class="header-mark"></a>CH9. 定积分</h2><ul>
<li>定义</li>
<li>牛顿-莱布尼兹</li>
<li>可积条件
<ul>
<li>必要</li>
<li>充要</li>
<li>函数类</li>
</ul>
</li>
<li>性质</li>
<li>积分中值定理</li>
<li>基本定理
<ul>
<li>变限积分 原函数存在性</li>
<li>换元 分部</li>
<li>泰勒积分型余项</li>
</ul>
</li>
</ul>
<h2 id="ch10-应用定积分" class="headerLink">
    <a href="#ch10-%e5%ba%94%e7%94%a8%e5%ae%9a%e7%a7%af%e5%88%86" class="header-mark"></a>CH10. 应用定积分</h2><ul>
<li>平面图形面积</li>
<li>平行截面求体积</li>
<li>平面曲线
<ul>
<li>弧长</li>
<li>曲率</li>
</ul>
</li>
<li>旋转曲面
<ul>
<li>微元法</li>
<li>面积</li>
</ul>
</li>
<li>物理</li>
</ul>
<h2 id="ch11-反常积分" class="headerLink">
    <a href="#ch11-%e5%8f%8d%e5%b8%b8%e7%a7%af%e5%88%86" class="header-mark"></a>CH11. 反常积分</h2><ul>
<li>定义</li>
<li>无穷积分性质</li>
<li>无穷积分敛散判别
<ul>
<li>非负无穷</li>
<li>一般无穷</li>
</ul>
</li>
<li>瑕积分性质</li>
<li>瑕积分敛散判别</li>
</ul>
<h2 id="ch12-数项级数" class="headerLink">
    <a href="#ch12-%e6%95%b0%e9%a1%b9%e7%ba%a7%e6%95%b0" class="header-mark"></a>CH12. 数项级数</h2><ul>
<li>敛散性</li>
<li>正项级数
<ul>
<li>一般判别原则</li>
<li>比式判别</li>
<li>根式判别</li>
<li>积分判别</li>
</ul>
</li>
<li>一般项级数
<ul>
<li>交错</li>
<li>绝对收敛 性质</li>
<li>阿贝尔判别</li>
<li>狄利克雷判别</li>
</ul>
</li>
</ul>
<h2 id="ch13-函数列-函数项级数" class="headerLink">
    <a href="#ch13-%e5%87%bd%e6%95%b0%e5%88%97-%e5%87%bd%e6%95%b0%e9%a1%b9%e7%ba%a7%e6%95%b0" class="header-mark"></a>CH13. 函数列 函数项级数</h2><ul>
<li>一致收敛
<ul>
<li>函数列</li>
<li>函数项级数</li>
<li>判别法</li>
</ul>
</li>
<li>一致收敛函数列与函数项级数性质</li>
</ul>
<h2 id="ch14-幂级数" class="headerLink">
    <a href="#ch14-%e5%b9%82%e7%ba%a7%e6%95%b0" class="header-mark"></a>CH14. 幂级数</h2><ul>
<li>收敛区间</li>
<li>性质</li>
<li>运算</li>
<li>幂级数展开
<ul>
<li>泰勒级数</li>
<li>初等函数展开式</li>
</ul>
</li>
</ul>
<h2 id="ch15-傅里叶级数" class="headerLink">
    <a href="#ch15-%e5%82%85%e9%87%8c%e5%8f%b6%e7%ba%a7%e6%95%b0" class="header-mark"></a>CH15. 傅里叶级数</h2><ul>
<li>三角级数 正交函数系</li>
<li>$2\pi$周期函数</li>
<li>收敛定理</li>
<li>$2l$周期函数</li>
<li>奇偶函数</li>
<li>收敛定理证明</li>
</ul>
<h2 id="ch16-多元函数极限-连续" class="headerLink">
    <a href="#ch16-%e5%a4%9a%e5%85%83%e5%87%bd%e6%95%b0%e6%9e%81%e9%99%90-%e8%bf%9e%e7%bb%ad" class="header-mark"></a>CH16. 多元函数极限 连续</h2><ul>
<li>平面点集</li>
<li>$\mathbb{R}^2$完备性</li>
<li>二元函数</li>
<li>n元函数</li>
<li>二元函数极限</li>
<li>累次极限</li>
<li>连续性</li>
<li>有界闭域连续函数性质</li>
</ul>
<h2 id="ch17-多元函数微分" class="headerLink">
    <a href="#ch17-%e5%a4%9a%e5%85%83%e5%87%bd%e6%95%b0%e5%be%ae%e5%88%86" class="header-mark"></a>CH17. 多元函数微分</h2><ul>
<li>可微性 全微分</li>
<li>偏导数</li>
<li>可微条件</li>
<li>几何意义</li>
<li>复合函数求导</li>
<li>复合函数全微分</li>
<li>方向导数 梯度</li>
<li>高阶偏导数</li>
<li>中值定理 泰勒</li>
<li>极值问题</li>
</ul>
<h2 id="ch18-隐函数" class="headerLink">
    <a href="#ch18-%e9%9a%90%e5%87%bd%e6%95%b0" class="header-mark"></a>CH18. 隐函数</h2><ul>
<li>概念</li>
<li>存在性条件</li>
<li>定理</li>
<li>求导</li>
<li>隐函数组
<ul>
<li>定理</li>
<li>反函数组</li>
<li>坐标变换</li>
</ul>
</li>
<li>几何
<ul>
<li>平面曲线切线法线</li>
<li>空间曲线切线法平面</li>
<li>曲面切平面法线</li>
</ul>
</li>
<li>条件极值</li>
</ul>
<h2 id="ch19-含参量积分" class="headerLink">
    <a href="#ch19-%e5%90%ab%e5%8f%82%e9%87%8f%e7%a7%af%e5%88%86" class="header-mark"></a>CH19. 含参量积分</h2><ul>
<li>正常积分</li>
<li>反常积分
<ul>
<li>一致收敛 判别</li>
<li>反常积分 性质</li>
</ul>
</li>
<li>欧拉积分
<ul>
<li>$\Gamma$函数</li>
<li>B函数</li>
<li>关系</li>
</ul>
</li>
</ul>
<h2 id="ch20-曲线积分" class="headerLink">
    <a href="#ch20-%e6%9b%b2%e7%ba%bf%e7%a7%af%e5%88%86" class="header-mark"></a>CH20. 曲线积分</h2><ul>
<li>第一型
<ul>
<li>定义</li>
<li>计算</li>
</ul>
</li>
<li>第二型
<ul>
<li>定义</li>
<li>计算</li>
</ul>
</li>
<li>联系</li>
</ul>
<h2 id="ch21-重积分" class="headerLink">
    <a href="#ch21-%e9%87%8d%e7%a7%af%e5%88%86" class="header-mark"></a>CH21. 重积分</h2><ul>
<li>二重
<ul>
<li>平面图形面积</li>
<li>定义 存在性</li>
<li>性质</li>
</ul>
</li>
<li>直角坐标系下计算</li>
<li>格林公式</li>
<li>曲线积分 路径无关性</li>
<li>变量变换
<ul>
<li>公式</li>
<li>极坐标</li>
</ul>
</li>
<li>三重
<ul>
<li>转累次</li>
<li>换元</li>
</ul>
</li>
<li>应用</li>
</ul>
<h2 id="ch22-曲面积分" class="headerLink">
    <a href="#ch22-%e6%9b%b2%e9%9d%a2%e7%a7%af%e5%88%86" class="header-mark"></a>CH22. 曲面积分</h2><ul>
<li>第一型</li>
<li>第二型</li>
<li>联系</li>
<li>高斯公式</li>
<li>斯托克斯公式</li>
</ul>
<h2 id="习题总结" class="headerLink">
    <a href="#%e4%b9%a0%e9%a2%98%e6%80%bb%e7%bb%93" class="header-mark"></a>习题总结</h2><ul>
<li>每日一题 D-E/T</li>
<li>华师大 H-E/T</li>
<li>题型 Ty</li>
</ul>
<h3 id="数列" class="headerLink">
    <a href="#%e6%95%b0%e5%88%97" class="header-mark"></a>数列</h3><ul>
<li>证收敛&amp;求极限
<ul>
<li>D-E1
<ul>
<li>入手递推公式 <strong>裂项相消</strong> 引出极限$x_n$</li>
<li>讨论$x_n$性质 单调性</li>
<li>引充分条件 讨论有界性
<ul>
<li>假设有界 推出收敛</li>
<li><strong>两边等式取极限法</strong></li>
<li>推出无界</li>
</ul>
</li>
</ul>
</li>
<li>D-E2
<ul>
<li>入手递推公式 <strong>不动点法</strong></li>
</ul>
</li>
<li>D-T1
<ul>
<li>入手递推公式 <strong>根式型</strong></li>
<li>讨论单调性 平方展开 引出$x_n\ge 3$猜想</li>
<li>讨论有界性</li>
<li>单调有界推出收敛</li>
</ul>
</li>
<li>D-T2
<ul>
<li>入手递推公式 <strong>取倒数</strong></li>
<li>前置判断$a_n&gt;0$</li>
</ul>
</li>
<li>D-E3
<ul>
<li>迫敛性</li>
</ul>
</li>
<li>D-E4
<ul>
<li>和式放缩 积分法</li>
</ul>
</li>
<li>D-T3
<ul>
<li>化通项放缩 迫敛性</li>
</ul>
</li>
<li>D-T4
<ul>
<li>化通项放缩 积分法</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="方法论" class="headerLink">
    <a href="#%e6%96%b9%e6%b3%95%e8%ae%ba" class="header-mark"></a>方法论</h2><h3 id="数列-1" class="headerLink">
    <a href="#%e6%95%b0%e5%88%97-1" class="header-mark"></a>数列</h3><ul>
<li>
<p>M 公式类型</p>
<ul>
<li>递推公式</li>
<li>通项（求和公式）</li>
<li>未定函数
<ul>
<li>$\sum_{1}^n f(k)$</li>
<li>含积分 $\int_{1}^n f(x)dx$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>T 题型</p>
<ul>
<li>证明收敛</li>
<li>求极限</li>
</ul>
</li>
<li>
<p>Ty-1 证明收敛</p>
<ul>
<li>极限存在条件
<ul>
<li>单调有界定理
<ul>
<li>单调性讨论</li>
<li>有界性讨论</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Ty-2 求极限</p>
<ul>
<li>递推公式 两边取极限</li>
<li>递推公式 求通项</li>
<li>通项 直求极限
<ul>
<li>四则运算法则</li>
</ul>
</li>
<li>迫敛性
<ul>
<li>求和公式 可改写成含$k$通项</li>
<li>放缩
<ul>
<li>通项 利用$k$范围放缩</li>
<li>和式
<ul>
<li>省略部分项 或补充项</li>
<li>所有项目想等于某项</li>
</ul>
</li>
</ul>
</li>
<li>分式首先放缩分母</li>
</ul>
</li>
<li>积分法
<ul>
<li>$k=1,\cdots,n, n\rightarrow \infty$ 形式</li>
<li>等价为定积分</li>
<li>分子比分母小一阶</li>
</ul>
</li>
</ul>
</li>
<li>
<p>M-1 单调性讨论</p>
<ul>
<li>有递推公式
<ul>
<li>直接定义比较求范围</li>
</ul>
</li>
</ul>
</li>
<li>
<p>M-2 递推公式求通项</p>
<ul>
<li>直接观察</li>
<li>数学归纳</li>
<li>求和公式 $S_n-S_{n-1}$</li>
<li>累加 $a_{n+1}=a_n+f(n)$</li>
<li>累乘 $a_{n+1}=a_nf(n)$</li>
<li>待定系数 $a_{n+1}=pa_n+q$
<ul>
<li>$a_{n+1}=Aa_n+Bn+C$</li>
<li>$a_{n+1}=Aa_n+Bn^2+Cn+D$</li>
</ul>
</li>
<li>同除&amp;累加 $a_{n+1}=pa_n+q(n)$</li>
<li>取倒数 $a_{n+1}=\frac{Aa_n}{Ba_n+C}$ （<strong>注意判定不为0</strong>）</li>
<li>高阶常系数 $a_{n+2}=Aa_{n+1}+Ba_n+C$
<ul>
<li>特征方程 重根</li>
<li>二阶情况</li>
</ul>
</li>
<li>取对数 $a_{n+1}=pa_n^q$</li>
<li>不动点 $a_{n+1}=\frac{Aa_n+B}{Ca_n+D}$
<ul>
<li>$\alpha\ne \beta$: $\frac{a_n-\alpha}{a_n-\beta}$ 等比</li>
<li>$\frac{1}{a_n-\alpha}$ 等差</li>
</ul>
</li>
<li>配方法 $a_{n+1}=Aa_n^2+Ba_n+C$</li>
<li>一次根号 根式整体换元</li>
<li>二次根号 移项平方 韦达定理
<ul>
<li>由$a_n,a_{n+2}$为同形态方程两根</li>
<li>或者对$n,n+1$两式作差作商</li>
</ul>
</li>
<li>三角换元</li>
</ul>
</li>
<li>
<p>M-3 引用重要极限结论</p>
<ul>
<li>$q^n$</li>
<li>$^n\sqrt{a}$</li>
<li>$a^n/n!$</li>
<li>$^n\sqrt{n}$</li>
<li>$1/^n\sqrt{n!}$</li>
</ul>
</li>
</ul>]]></description>
</item><item>
    <title>泛函分析</title>
    <link>https://blog.ralvines.top/fhfx/</link>
    <pubDate>Wed, 01 Mar 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/fhfx/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2022-2023 春夏<br>
🧑‍🏫 王伟<br>
📝 20%小测，20%作业，60%期末</div>
        </div>
    </div>
<div class="details admonition note open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw"></i>课程材料<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><ul>
<li>《实变函数与泛函分析概要（第五版）》王声望，郑维行</li>
<li>课程讲义
<ul>
<li>Ch1.1</li>
<li>Ch1.2</li>
<li>Ch2.1</li>
<li>Ch2.2</li>
<li>Ch3.1</li>
<li>Ch3.2</li>
</ul>
</li>
<li>泛函分析笔记 @Reichtum</li>
<li>课后习题讲解 @Reichtum
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/486354129" target="_blank" rel="noopener noreferrer">度量空间</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/524355026" target="_blank" rel="noopener noreferrer">Banach&amp;Hilbert</a></li>
</ul>
</li>
<li><a href="https://classroom.zju.edu.cn/coursedetail?course_id=48021&amp;tenant_code=112" target="_blank" rel="noopener noreferrer"><em>智云课堂回放</em></a></li>
</ul>
</div>
        </div>
    </div>
<h2 id="ch1-距离空间" class="headerLink">
    <a href="#ch1-%e8%b7%9d%e7%a6%bb%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch1. 距离空间</h2><ul>
<li>
<p>定义</p>
<ul>
<li>距离公理：非负、对称、三角不等式</li>
<li>非空即可定义、不唯一</li>
<li>离散距离空间</li>
<li>$\mathbb{R}^n$
<ul>
<li>欧氏距离、复数域</li>
<li><strong>柯西不等式</strong></li>
<li>max定义</li>
</ul>
</li>
<li>连续函数空间$C[a,b]$</li>
<li>$l^p$
<ul>
<li>元素：无限数列、级数绝对收敛</li>
<li>距离定义
<ul>
<li>Holder不等式</li>
<li><strong>Minkowski不等式</strong>、证明</li>
</ul>
</li>
</ul>
</li>
<li>$l^\infty$</li>
<li>$L^p(F)$
<ul>
<li>可测集F</li>
<li>距离定义
<ul>
<li>Holder不等式</li>
<li>Minkowski不等式</li>
</ul>
</li>
</ul>
</li>
<li>$L^\infty$
<ul>
<li>本性有界</li>
<li>本性有界可测、几乎处处相等看作同元素</li>
<li>距离定义$essinf_F$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>收敛</p>
<ul>
<li>点列收敛
<ul>
<li>性质：极限唯一、有界</li>
<li>子列收敛</li>
</ul>
</li>
<li>欧式空间$\mathbb{R}^n$的收敛（如何证）
<ul>
<li>点列收敛、坐标收敛</li>
</ul>
</li>
<li>$C[a,b]$ 的收敛
<ul>
<li>按照距离导出收敛</li>
<li>某距离收敛等价函数列一致收敛</li>
</ul>
</li>
</ul>
</li>
<li>
<p>点集</p>
<ul>
<li>开球、闭球</li>
<li>开集、闭包、闭集</li>
<li>内点、内部</li>
<li>聚点、导集、孤立点</li>
<li>稠密性</li>
<li>可分性
<ul>
<li>$L^\infty[a,b]$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>连续映射</p>
<ul>
<li>连续性等价条件</li>
<li>归结原则、集合描述</li>
<li>同胚、等距</li>
</ul>
</li>
<li>
<p>完备性</p>
<ul>
<li>柯西基本列</li>
<li>完备性
<ul>
<li>三条定理</li>
<li>$C[a,b]$、$l^p$、$L^\infty(F)$ 完备</li>
<li>$S$ 三角不等式及<strong>完备性证明</strong></li>
</ul>
</li>
<li>完备化
<ul>
<li>完备扩展定理</li>
<li>$C[a,b]\rightarrow L^2[a,b]$</li>
<li>$P\rightarrow C[a,b]$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>稀疏集</p>
<ul>
<li>与球的的充要条件</li>
</ul>
</li>
<li>
<p>闭球套定理、第一/二类型集</p>
</li>
<li>
<p>$l_0^p$</p>
<ul>
<li>子空间、不完备、稠密</li>
</ul>
</li>
<li>
<p>$S$、$s$、$P$</p>
</li>
<li>
<p>准紧集、紧集、全有界集</p>
<ul>
<li>$\epsilon-$ 网</li>
<li>相互关系</li>
<li>紧集套</li>
<li>开覆盖</li>
<li>有限交</li>
<li>连续映射</li>
</ul>
</li>
<li>
<p>不动点定理、压缩映射</p>
</li>
<li>
<p><strong>重点梳理</strong></p>
<ul>
<li>常见度量空间，及其距离、收敛、可分性、准紧条件
<ul>
<li>$\mathbb{R}^n$ （所有分量收敛）</li>
<li>$C[a,b]$（一致收敛）、$C^k[a,b]$、$C^\infty [a,b]$</li>
<li>$l^p$、$l^\infty$（不可分）</li>
<li>$L^p$、$L^\infty$（不可分）</li>
<li>$S$（测度收敛）、$s$（按坐标收敛）</li>
</ul>
</li>
<li>重要证明
<ul>
<li>Cauchy、Holder、Minkowski、Young</li>
</ul>
</li>
<li>各类点集、球、稠密、可分</li>
<li>基本列、收敛、完备</li>
<li>连续映射</li>
<li>不动点、压缩映射</li>
</ul>
</li>
</ul>
<h2 id="ch2-巴拿赫空间希尔伯特空间" class="headerLink">
    <a href="#ch2-%e5%b7%b4%e6%8b%bf%e8%b5%ab%e7%a9%ba%e9%97%b4%e5%b8%8c%e5%b0%94%e4%bc%af%e7%89%b9%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch2. 巴拿赫空间、希尔伯特空间</h2><ul>
<li>赋范线性空间
<ul>
<li>距离空间、范数/强收敛</li>
<li>Banach</li>
<li>商空间</li>
<li>直和</li>
</ul>
</li>
<li>内积空间
<ul>
<li>导出范数
<ul>
<li>Schwarz不等式、极化恒等式</li>
<li>平行四边形公式</li>
</ul>
</li>
<li>$l^2,L^2$</li>
<li>正交、正交补、推广勾股</li>
<li>规范正交系
<ul>
<li>Bessel</li>
<li>完备、完全</li>
<li>Schmidt正交化</li>
<li>最佳逼近</li>
</ul>
</li>
</ul>
</li>
<li>Hilbert
<ul>
<li>凸集、正交分解</li>
<li>E.S.Fischer</li>
<li>Parseval</li>
<li>可分同构</li>
</ul>
</li>
</ul>
<h2 id="ch3-有界线性算子巴拿赫空间" class="headerLink">
    <a href="#ch3-%e6%9c%89%e7%95%8c%e7%ba%bf%e6%80%a7%e7%ae%97%e5%ad%90%e5%b7%b4%e6%8b%bf%e8%b5%ab%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch3. 有界线性算子：巴拿赫空间</h2><h2 id="ch4-有界线性算子希尔伯特空间" class="headerLink">
    <a href="#ch4-%e6%9c%89%e7%95%8c%e7%ba%bf%e6%80%a7%e7%ae%97%e5%ad%90%e5%b8%8c%e5%b0%94%e4%bc%af%e7%89%b9%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch4. 有界线性算子：希尔伯特空间</h2><h2 id="参考资料" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99" class="header-mark"></a>参考资料</h2><p>小测：https://www.cc98.org/topic/5321722</p>]]></description>
</item><item>
    <title>复变函数</title>
    <link>https://blog.ralvines.top/fbhs/</link>
    <pubDate>Wed, 01 Mar 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/fbhs/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2022-2023 春夏<br>
🧑‍🏫 齐治<br>
📝 40%作业，60%期末</div>
        </div>
    </div>
<div class="details admonition note open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw"></i>课程材料<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><ul>
<li>《Complex Analysis》, Stein.</li>
<li><a href="https://classroom.zju.edu.cn/coursedetail?course_id=47986&amp;tenant_code=112" target="_blank" rel="noopener noreferrer">智云课堂回放</a></li>
<li>复变函数华师版讲义 @陆俊</li>
</ul>
</div>
        </div>
    </div>
<h2 id="ch1-复分析预备知识" class="headerLink">
    <a href="#ch1-%e5%a4%8d%e5%88%86%e6%9e%90%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86" class="header-mark"></a>Ch1. 复分析预备知识</h2><h2 id="ch2-柯西定理及应用" class="headerLink">
    <a href="#ch2-%e6%9f%af%e8%a5%bf%e5%ae%9a%e7%90%86%e5%8f%8a%e5%ba%94%e7%94%a8" class="header-mark"></a>Ch2. 柯西定理及应用</h2><h2 id="ch3-亚纯函数及对数" class="headerLink">
    <a href="#ch3-%e4%ba%9a%e7%ba%af%e5%87%bd%e6%95%b0%e5%8f%8a%e5%af%b9%e6%95%b0" class="header-mark"></a>Ch3. 亚纯函数及对数</h2><h2 id="ch5-全函数" class="headerLink">
    <a href="#ch5-%e5%85%a8%e5%87%bd%e6%95%b0" class="header-mark"></a>Ch5. 全函数</h2><h2 id="ch7-留数定理" class="headerLink">
    <a href="#ch7-%e7%95%99%e6%95%b0%e5%ae%9a%e7%90%86" class="header-mark"></a>Ch7. 留数定理*</h2><h2 id="ch8-椭圆函数" class="headerLink">
    <a href="#ch8-%e6%a4%ad%e5%9c%86%e5%87%bd%e6%95%b0" class="header-mark"></a>Ch8. 椭圆函数*</h2><h2 id="ch10-theta函数" class="headerLink">
    <a href="#ch10-theta%e5%87%bd%e6%95%b0" class="header-mark"></a>Ch10. $\Theta$函数*</h2><h2 id="参考资料" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99" class="header-mark"></a>参考资料</h2><ul>
<li>华师版</li>
</ul>]]></description>
</item><item>
    <title>数据建模与分析</title>
    <link>https://blog.ralvines.top/sjjm/</link>
    <pubDate>Wed, 01 Mar 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/sjjm/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2022-2023 春夏<br>
🧑‍🏫 郭正初<br>
📝 20%课后作业，15%读书报告，15%编程作业，50%期末考试</div>
        </div>
    </div>
<div class="details admonition note open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw"></i>课程材料<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><ul>
<li>PPT
<ul>
<li>Ch1. 机器学习概论</li>
<li>Ch2. 感知机</li>
<li>Ch3. k近邻</li>
<li>Ch4. 朴素贝叶斯</li>
<li>Ch5. 决策树</li>
<li>Ch6. 逻辑斯蒂回归、最大熵模型</li>
<li>Ch7. 支持向量机</li>
<li>Ch8. AdaBoost</li>
<li>Ch13. 无监督学习概论</li>
<li>Ch14. 聚类方法</li>
<li>谱聚类</li>
<li>Ch15. 奇异值分解</li>
<li>Ch16. 主成分分析</li>
<li>Ch19. 马尔可夫链蒙特卡罗法</li>
</ul>
</li>
<li>《统计学习方法（第二版）》，李航</li>
<li><a href="https://classroom.zju.edu.cn/coursedetail?course_id=51611&amp;tenant_code=112" target="_blank" rel="noopener noreferrer"><em>智云课堂回放</em></a></li>
</ul>
</div>
        </div>
    </div>
<h2 id="ch1-机器学习概论" class="headerLink">
    <a href="#ch1-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e6%a6%82%e8%ae%ba" class="header-mark"></a>(Ch1) 机器学习概论</h2><blockquote>
<p>春一周</p>
</blockquote>
<ul>
<li>人工智能
<ul>
<li>研究目的、内容、表现形式</li>
<li>发展历程、现状</li>
<li>顶刊、顶会</li>
</ul>
</li>
<li>机器学习（统计学习理论）
<ul>
<li>定义（经验）</li>
<li>顶刊、顶会</li>
<li>应用：NLP、CV&hellip;</li>
<li>区别联系
<ul>
<li>数据挖掘（噪声、仓储）</li>
<li>模式识别</li>
</ul>
</li>
</ul>
</li>
<li>大数据
<ul>
<li>4&quot;V&quot;: 量大、类多、实时、密度低</li>
</ul>
</li>
<li>深度学习（ML分支）
<ul>
<li>深度神经网络，假设空间</li>
<li>特征学习</li>
</ul>
</li>
<li>统计机器学习（数据预测与分析）
<ul>
<li>数据驱动</li>
<li><strong>分类</strong>
<ul>
<li>监督/半监督/无监督/强化学习
<ul>
<li>数据标注，概率分布</li>
<li>连续互动</li>
</ul>
</li>
<li>概率/非，线性/非，参数/非</li>
<li>条件概率分布/函数
<ul>
<li>参数维度</li>
</ul>
</li>
<li>在线/批量/离线</li>
<li>贝叶斯/核方法</li>
</ul>
</li>
<li>三要素
<ul>
<li>模型（决策函数/条件概率/参数空间）</li>
<li>策略（损失/风险函数，经验/结构风险最小化）</li>
<li>算法（最优化问题）</li>
</ul>
</li>
<li>模型评估和选择
<ul>
<li>训练误差、测试误差</li>
<li>过拟合、欠拟合</li>
<li>正则化、交叉验证</li>
<li>泛化能力/误差（对未知数据）</li>
<li>集中不等式</li>
</ul>
</li>
<li>生成与判别模型
<ul>
<li>判别方法（直接学习决策函数/概率分布）</li>
<li>生成方法（从联合概率分布到条件概率分布）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ch2-感知机" class="headerLink">
    <a href="#ch2-%e6%84%9f%e7%9f%a5%e6%9c%ba" class="header-mark"></a>(Ch2) 感知机</h2><blockquote>
<p>春二周</p>
</blockquote>
<ul>
<li>线性可分性</li>
<li>点到超平面距离、损失函数</li>
<li>随机梯度下降
<ul>
<li>学习率</li>
<li>不唯一（初值、误分类点顺序）</li>
<li><strong>收敛性证明</strong></li>
</ul>
</li>
<li>对偶形式（优点）
<ul>
<li><strong>Gram 矩阵</strong></li>
</ul>
</li>
</ul>
<h2 id="ch3-k近邻" class="headerLink">
    <a href="#ch3-k%e8%bf%91%e9%82%bb" class="header-mark"></a>(Ch3) k近邻</h2><blockquote>
<p>春三周</p>
</blockquote>
<ul>
<li>三要素：k，度量，决策规则</li>
<li>优点、缺点（复杂度）</li>
<li>选 k （误差最小、k小复杂过拟合）</li>
<li>kd树（k维）
<ul>
<li>构造</li>
<li>对kNN检索</li>
</ul>
</li>
</ul>
<h2 id="ch4-朴素贝叶斯" class="headerLink">
    <a href="#ch4-%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af" class="header-mark"></a>(Ch4) 朴素贝叶斯</h2><blockquote>
<p>春四周</p>
</blockquote>
<ul>
<li>样本空间、全概率公式</li>
<li>期望风险最小化、后验概率最大化</li>
<li>极大似然法
<ul>
<li>对数似然</li>
<li>估计值/量</li>
<li>朴素贝叶斯法的参数估计</li>
</ul>
</li>
<li>贝叶斯估计（极大似然估计、拉普拉斯平滑）</li>
<li></li>
</ul>
<h2 id="ch5-决策树" class="headerLink">
    <a href="#ch5-%e5%86%b3%e7%ad%96%e6%a0%91" class="header-mark"></a>(Ch5) 决策树</h2><blockquote>
<p>春五周</p>
</blockquote>
<ul>
<li>分类和回归</li>
<li>CLS</li>
<li>ID3
<ul>
<li>熵、信息量</li>
<li>条件熵、经验熵/条件熵</li>
<li>信息增益、互信息</li>
<li>计算信息增益、选择最优特征</li>
</ul>
</li>
<li>C4.5（信息增益比）
<ul>
<li>连续属性：二元分割</li>
</ul>
</li>
<li>剪枝
<ul>
<li>损失函数</li>
</ul>
</li>
<li>CART
<ul>
<li>基尼指数</li>
<li>回归树、分类树</li>
<li><strong>剪枝</strong></li>
</ul>
</li>
</ul>
<h2 id="ch6-逻辑斯蒂回归最大熵模型" class="headerLink">
    <a href="#ch6-%e9%80%bb%e8%be%91%e6%96%af%e8%92%82%e5%9b%9e%e5%bd%92%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b" class="header-mark"></a>(Ch6) 逻辑斯蒂回归、最大熵模型</h2><blockquote>
<p>春六周</p>
</blockquote>
<ul>
<li>Logistic分布
<ul>
<li>分布函数、密度函数</li>
<li>Sigmoid、tanh</li>
</ul>
</li>
<li>Logistic回归
<ul>
<li>二项</li>
<li>似然函数</li>
<li>多项</li>
</ul>
</li>
<li>最大熵模型
<ul>
<li>学习</li>
<li>极大似然估计</li>
</ul>
</li>
<li>最优化
<ul>
<li>梯度下降</li>
<li>牛顿、拟牛顿
<ul>
<li>黑塞矩阵</li>
<li>正定矩阵（近似）</li>
</ul>
</li>
<li>DFP</li>
<li>BFGS</li>
<li>Broyden</li>
<li>改进迭代尺度</li>
<li>梯度上升、随机梯度上升</li>
</ul>
</li>
</ul>
<h2 id="ch7-支持向量机" class="headerLink">
    <a href="#ch7-%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba" class="header-mark"></a>(Ch7) 支持向量机</h2><blockquote>
<p>春七周</p>
</blockquote>
<ul>
<li>线性可分、硬间隔最大化</li>
<li>线性不可分、软间隔最大化</li>
<li>非线性、核函数</li>
<li>序列最小化优化算法</li>
<li>误差分析</li>
</ul>
<h2 id="ch8-adaboost" class="headerLink">
    <a href="#ch8-adaboost" class="header-mark"></a>(Ch8) AdaBoost</h2><blockquote>
<p>春八周</p>
</blockquote>
<ul>
<li>强可学习、弱可学习</li>
<li>Boosting、AdaBoost
<ul>
<li>权重、系数</li>
<li>误差分析</li>
<li>前向分步算法</li>
<li>提升树算法
<ul>
<li>回归问题</li>
</ul>
</li>
<li>梯度提升算法</li>
</ul>
</li>
</ul>
<h2 id="ch13-无监督学习概论" class="headerLink">
    <a href="#ch13-%e6%97%a0%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0%e6%a6%82%e8%ae%ba" class="header-mark"></a>(Ch13) 无监督学习概论</h2><blockquote>
<p>夏一周</p>
</blockquote>
<ul>
<li>损失最小压缩
<ul>
<li>聚类：硬、软</li>
<li>降维</li>
</ul>
</li>
<li>概率模型
<ul>
<li>混合、概率图（有向、无向）</li>
<li><strong>估计</strong></li>
</ul>
</li>
<li>三要素：模型、策略、方法</li>
<li>话题分析（LDA）</li>
<li>图分析
<ul>
<li>PageRank 计算</li>
</ul>
</li>
</ul>
<h2 id="ch14-聚类方法谱聚类" class="headerLink">
    <a href="#ch14-%e8%81%9a%e7%b1%bb%e6%96%b9%e6%b3%95%e8%b0%b1%e8%81%9a%e7%b1%bb" class="header-mark"></a>(Ch14) 聚类方法、谱聚类</h2><blockquote>
<p>夏二周，夏三周</p>
</blockquote>
<ul>
<li>距离
<ul>
<li>Minkowski、欧式、曼哈顿、Chebyshev</li>
<li>马氏、协方差矩阵、相关系数、夹角余弦</li>
</ul>
</li>
<li>簇
<ul>
<li>各种定义</li>
<li>特征划分：散布矩阵、协方差矩阵</li>
<li>类间距离（连接）：最短（单）、最长（完全）、中心、平均</li>
</ul>
</li>
<li>层次聚类
<ul>
<li>聚合、分裂</li>
<li>合并规则、停止条件</li>
</ul>
</li>
<li>k均值
<ul>
<li>欧氏距离、损失函数</li>
<li>初始中心选取</li>
<li>k的选取（平均直径不再增加）</li>
</ul>
</li>
</ul>
<h2 id="ch15-奇异值分解" class="headerLink">
    <a href="#ch15-%e5%a5%87%e5%bc%82%e5%80%bc%e5%88%86%e8%a7%a3" class="header-mark"></a>(Ch15) 奇异值分解</h2><blockquote>
<p>夏四周</p>
</blockquote>
<ul>
<li>特征分解/谱分解
<ul>
<li>特征向量、特征值、特征多项式</li>
<li>方阵可对角化&amp;特征向量线性无关</li>
<li>分解: Q, $\Lambda$</li>
<li>实对称情形: 逆</li>
</ul>
</li>
<li>定义
<ul>
<li>分解: U, V, $\Sigma$</li>
<li>奇异值、左右奇异向量</li>
<li>不唯一</li>
<li>存在性、<strong>证明</strong>（从V到U的构造）</li>
</ul>
</li>
<li>类型
<ul>
<li>完全分解</li>
<li>紧凑分解（r，等秩）</li>
<li>截断分解（k，实际）</li>
</ul>
</li>
<li>性质
<ul>
<li>线性变换解释（分解：旋转、缩放、旋转）</li>
<li>等价特征分解（V、U代表的特征向量）</li>
<li>奇异向量构成的标准正交基（由正交性）</li>
</ul>
</li>
<li>计算</li>
<li>矩阵近似
<ul>
<li>F范数</li>
<li>矩阵的F范数与其奇异值的关系</li>
<li>平方损失下的最优近似、<strong>证明</strong></li>
<li>外积展开式、最优近似矩阵的计算</li>
</ul>
</li>
</ul>
<h2 id="ch16-主成分分析" class="headerLink">
    <a href="#ch16-%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90" class="header-mark"></a>(Ch16) 主成分分析</h2><blockquote>
<p>夏五周</p>
</blockquote>
<ul>
<li>数据分析、机器学习预处理</li>
<li>思路
<ul>
<li>规范化：平均值0，方差1</li>
<li>正交变换、线性相关转无关变量（主成分）</li>
<li>方差和最大化的正交变换（椭圆长轴）</li>
</ul>
</li>
<li>定义
<ul>
<li>均值向量$\mu$、协方差矩阵、</li>
</ul>
</li>
<li>总体PCA</li>
<li>样本PCA</li>
</ul>
<h2 id="ch19-马尔可夫链蒙特卡罗法" class="headerLink">
    <a href="#ch19-%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%93%be%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97%e6%b3%95" class="header-mark"></a>(Ch19) 马尔可夫链蒙特卡罗法</h2><ul>
<li>蒙特卡洛法
<ul>
<li>直接抽样</li>
<li>接受-拒绝抽样（建议分布）</li>
<li>期望/积分计算</li>
</ul>
</li>
<li>马尔可夫链
<ul>
<li>时间齐次</li>
<li>高阶</li>
<li>转移概率矩阵、随机矩阵</li>
<li>平稳分布（充要条件）</li>
<li>连续状态、转移核</li>
<li>性质
<ul>
<li>不可约</li>
<li>非周期</li>
<li>正常返</li>
<li>唯一平稳分布（有限、无限）</li>
<li>遍历定理</li>
<li>可逆</li>
</ul>
</li>
</ul>
</li>
<li>马尔可夫链蒙特卡罗法
<ul>
<li>燃烧期</li>
<li>步骤</li>
</ul>
</li>
<li>Metropolis-Hastings
<ul>
<li>单分量</li>
</ul>
</li>
<li>吉布斯抽样
<ul>
<li>抽样计算</li>
</ul>
</li>
</ul>
<h2 id="历年卷" class="headerLink">
    <a href="#%e5%8e%86%e5%b9%b4%e5%8d%b7" class="header-mark"></a>历年卷</h2><h3 id="20-21-春夏" class="headerLink">
    <a href="#20-21-%e6%98%a5%e5%a4%8f" class="header-mark"></a>20-21 春夏</h3><ul>
<li>Kd树 书上例题 找最近邻</li>
<li>熵H(p)的定义，证明H(p)在0到log(n)之间</li>
<li>朴素贝叶斯 书上例题</li>
<li>SVM含义以及与感知机的区别
<ul>
<li>推导出 SVM 的对偶问题</li>
<li>如何通过对偶问题的解得到原问题的解</li>
</ul>
</li>
<li>聚类 书上例题</li>
<li>看图求马尔科夫链的转移概率矩阵和平稳分布</li>
<li>奇异值分解存在性唯一性讨论，并给出分解过程</li>
<li>给了一个矩阵，对其进行主成分分析</li>
<li>决策树中的信息增益g(D,A)的用处
<ul>
<li>剪枝的意义</li>
</ul>
</li>
<li>课程建议</li>
</ul>
<h3 id="21-22-春夏" class="headerLink">
    <a href="#21-22-%e6%98%a5%e5%a4%8f" class="header-mark"></a>21-22 春夏</h3><ul>
<li>简述决策树的一种特征选择准则的定义，说明准则对决策树的影响（大概是这个意思，考信息增益和基尼指数的定义）</li>
<li>kd树构造
<ul>
<li>k近邻模型三要素是什么，k值选择需要注意什么（过拟合和误差）</li>
<li>给定样本数据集，构造kd树</li>
<li>按照构造的kd树求出实例点 （2，4.5）的最近邻</li>
</ul>
</li>
<li>支持向量机：给定线性不可分支持向量机的学习问题
<ul>
<li>软间隔SVM含义</li>
<li>写出对偶形式</li>
<li>求支持向量（好像是，当时只复习了硬间隔，软间隔就摆了）</li>
</ul>
</li>
<li>聚类问题：给定5个样本集合X，选定两个中心点，用k均值聚类算法 将X分成两类 （参考教材例题14.2）</li>
<li>马尔可夫链 和 蒙特卡洛法
<ul>
<li>大致说明 E[f(x)] (概率分布函数为p(x) ) 的计算方法 （大数定理近似样本均值）</li>
<li>给出一条马尔科夫链，求平稳分布（考的书上例题19.7）</li>
</ul>
</li>
<li>主成分分析
<ul>
<li>给定一个m维度的随机变量，求出k个主成分（1&lt;= k &lt;=m)，并且证明</li>
</ul>
</li>
<li>简述感知机，Adaboost，朴素贝叶斯法，logistic模型的学习策略和算法</li>
<li>奇异值分解：矩阵数据忘了 给定一个2*3矩阵A，求A的奇异值分解和紧奇异值分解，并且说明奇异值分解的几何意义</li>
</ul>
<h2 id="论文精读" class="headerLink">
    <a href="#%e8%ae%ba%e6%96%87%e7%b2%be%e8%af%bb" class="header-mark"></a>论文精读</h2><blockquote>
<p>Distance metric learning for large margin nearest neighbor classification.pdf</p>
</blockquote>
<ul>
<li>大边距近邻分类的距离度量学习</li>
</ul>
<h2 id="参考资料" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99" class="header-mark"></a>参考资料</h2><h3 id="前人经验" class="headerLink">
    <a href="#%e5%89%8d%e4%ba%ba%e7%bb%8f%e9%aa%8c" class="header-mark"></a>前人经验</h3><blockquote>
<p>期末考比较中规中矩，无小测。</p>
<p>上课就是讲《统计学习方法》中的几章，作业做书的课后题，没有代码作业。 不过去年很多人提建议说要增加代码训练和作业量，今年可能会有所改变</p>
</blockquote>
<p>习题与代码参考：</p>
<ul>
<li>统计学习方法（第二版）习题解答 <a href="https://github.com/datawhalechina/statistical-learning-method-solutions-manual" target="_blank" rel="noopener noreferrer">https://github.com/datawhalechina/statistical-learning-method-solutions-manual</a></li>
<li><a href="https://blog.csdn.net/qq_42911960/article/details/115255714" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_42911960/article/details/115255714</a></li>
<li><a href="https://blog.csdn.net/qq_41562704/article/details/106540274" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_41562704/article/details/106540274</a></li>
<li><a href="https://blog.csdn.net/wang_xinyu/article/details/111497444" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/wang_xinyu/article/details/111497444</a></li>
<li><a href="https://blog.csdn.net/breeze_blows/article/details/85469944" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/breeze_blows/article/details/85469944</a></li>
</ul>
<p>回忆卷：</p>
<ul>
<li><a href="https://www.cc98.org/topic/5356728" target="_blank" rel="noopener noreferrer">https://www.cc98.org/topic/5356728</a></li>
<li><a href="https://www.cc98.org/topic/5116266" target="_blank" rel="noopener noreferrer">https://www.cc98.org/topic/5116266</a></li>
</ul>
<h3 id="书目" class="headerLink">
    <a href="#%e4%b9%a6%e7%9b%ae" class="header-mark"></a>书目</h3><ul>
<li>《机器学习》，周志华，清华大学出版社，2016.</li>
<li>《The Elements of Statistical Learning》2nd edition, Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springer 2008.</li>
<li>《Pattern Recognition and Machine Learning》, Chris Bishop,  Springer 2006.</li>
<li>《Learning Theory：An Approximation Theory Viewpoint》, Felipe
Cucker and Ding-Xuan Zhou, Cambridge Univesity Press, 2007.</li>
</ul>]]></description>
</item></channel>
</rss>
