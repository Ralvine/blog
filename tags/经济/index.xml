<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>经济 - 标签 - 暮瞻</title>
        <link>https://blog.ralvines.top/tags/%E7%BB%8F%E6%B5%8E/</link>
        <description>经济 - 标签 - 暮瞻</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Mon, 30 Oct 2023 20:20:40 &#43;0800</lastBuildDate><atom:link href="https://blog.ralvines.top/tags/%E7%BB%8F%E6%B5%8E/" rel="self" type="application/rss+xml" /><item>
    <title>数学前沿专题讨论</title>
    <link>https://blog.ralvines.top/qianyan/</link>
    <pubDate>Mon, 30 Oct 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/qianyan/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/11/01/pinHqnH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2023-2024 秋冬<br>
🧑‍🏫 毕惟红<br>
📝 50%读书报告，10%考勤，10%编程，30%两次展示</div>
        </div>
    </div>
<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><ol>
<li>浙大 计算数学</li>
<li>研究生</li>
</ol>
<ul>
<li>数字模拟 投影法求曲面面积</li>
<li>随机数生成 数值代数 多元非线性方程组求解</li>
</ul>
<p><strong>研究方向</strong></p>
<ol>
<li>图像处理</li>
</ol>
<ul>
<li>图像分割 图像识别</li>
<li>图像加密 做的比较好</li>
</ul>
<ol start="2">
<li>语义识别</li>
</ol>
<ul>
<li>三维点式数据（无人驾驶、激光雷达）</li>
</ul>
<ol start="3">
<li>社区发现</li>
</ol>
<ul>
<li>复杂网络</li>
<li>拟牛顿</li>
</ul>
<h2 id="遗传算法" class="headerLink">
    <a href="#%e9%81%97%e4%bc%a0%e7%ae%97%e6%b3%95" class="header-mark"></a>遗传算法</h2><h2 id="统计学习方法" class="headerLink">
    <a href="#%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95" class="header-mark"></a>统计学习方法</h2><h2 id="首次展示knn" class="headerLink">
    <a href="#%e9%a6%96%e6%ac%a1%e5%b1%95%e7%a4%baknn" class="header-mark"></a>首次展示：kNN</h2><h3 id="分类问题1" class="headerLink">
    <a href="#%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%981" class="header-mark"></a>分类问题<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h3><ul>
<li>
<p>一种监督学习问题，旨在对数据分类</p>
</li>
<li>
<p>将输入数据映射到预定义的类别或标签</p>
</li>
<li>
<p>从已知的训练数据中学习一个分类模型，然后将该模型应用于新的、未知的数据，以预测其所属的类别</p>
</li>
<li>
<p>垃圾邮件过滤、金融风险评估</p>
</li>
<li>
<p>医学诊断、生物信息学</p>
</li>
<li>
<p>情感分析、客户分类</p>
</li>
<li>
<p>图像识别</p>
</li>
</ul>
<h3 id="knn模型构建" class="headerLink">
    <a href="#knn%e6%a8%a1%e5%9e%8b%e6%9e%84%e5%bb%ba" class="header-mark"></a>kNN模型构建</h3><h4 id="提出2" class="headerLink">
    <a href="#%e6%8f%90%e5%87%ba2" class="header-mark"></a>提出<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></h4><p>$$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$
$$y_i\in\mathcal{Y}={c_1,c_2,\cdots,c_K}$$
$$y=\text{arg}\max\limits_{c_j} \sum\limits_{x_i\in N_k(x)} I(y_i=c_j)$$</p>
<ul>
<li>
<p>输入：特征向量（空间点）</p>
</li>
<li>
<p>输出：类别（可以取多类）</p>
</li>
<li>
<p>已标注的训练集</p>
</li>
<li>
<p>预测：多数表决（“近朱者赤” ）</p>
</li>
<li>
<p>不具有显式的学习过程</p>
</li>
</ul>
<p><strong>适用范围</strong></p>
<ul>
<li>数值型和标称型<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></li>
</ul>
<p><strong>优点</strong><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<ol>
<li>直观、非参数化</li>
<li>对异常值不敏感</li>
<li>支持多类别</li>
</ol>
<p><strong>缺点</strong><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<ol>
<li>时间复杂度高</li>
<li>存储成本高</li>
<li>“维度灾难”和数据不平衡</li>
</ol>
<h4 id="构建流程" class="headerLink">
    <a href="#%e6%9e%84%e5%bb%ba%e6%b5%81%e7%a8%8b" class="header-mark"></a>构建流程</h4><p>给定距离度量，k值与决策规则 [输入训练集T]</p>
<ol>
<li>在训练集 T 中找出与 x 最邻近的 k 个点，涵盖这 个点的 x 的邻域记作 $N_k(a)$</li>
<li>在 $N_k(a)$ 中根据分类决策规则决定 x 的类别 y</li>
</ol>
<p><strong>基本要素</strong></p>
<ol>
<li>k 值选择</li>
<li>距离度量</li>
<li>决策规则</li>
</ol>
<p>特殊情况：最近邻（k=1）</p>
<p><figure><a class="lightgallery" href="https://z1.ax1x.com/2023/11/01/piuKE6K.png" title="特征空间划分" data-thumbnail="https://z1.ax1x.com/2023/11/01/piuKE6K.png">
        
    </a></figure></p>
<h3 id="模型要素" class="headerLink">
    <a href="#%e6%a8%a1%e5%9e%8b%e8%a6%81%e7%b4%a0" class="header-mark"></a>模型要素</h3><h4 id="k-值选择" class="headerLink">
    <a href="#k-%e5%80%bc%e9%80%89%e6%8b%a9" class="header-mark"></a>k 值选择</h4><table>
<thead>
<tr>
<th>k值</th>
<th>偏小</th>
<th>偏大</th>
</tr>
</thead>
<tbody>
<tr>
<td>近似误差</td>
<td>减小</td>
<td>增大</td>
</tr>
<tr>
<td>估计误差</td>
<td>增大</td>
<td>减小</td>
</tr>
</tbody>
</table>
<p>交叉验证以提高泛化性能。<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></p>
<h4 id="距离度量9" class="headerLink">
    <a href="#%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f9" class="header-mark"></a>距离度量<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></h4><p>对于
$$x_i=(x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})$$</p>
<ul>
<li>$L_p$ 距离
$$L_p(x_i,x_j)=(\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{1/p}$$</li>
<li>欧氏距离
$$L_2(x_i,x_j)=(\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^2)^{1/2}$$</li>
<li>曼哈顿距离
$$L_1(x_i,x_j)=\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|$$</li>
<li>$L_\infty$ 距离
$$L_\infty (x_i,x_j)=(\max\limits_l |x_i^{(l)}-x_j^{(l)}|$$</li>
</ul>
<h4 id="决策规则" class="headerLink">
    <a href="#%e5%86%b3%e7%ad%96%e8%a7%84%e5%88%99" class="header-mark"></a>决策规则</h4><p><strong>多数表决</strong>
由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。</p>
<ul>
<li>分类函数
$$f:\mathbb{R}^n\rightarrow {c_1,c_2,\cdots,c_K}$$</li>
<li>误分类概率
$$P(Y\ne f(X))=1-P(Y=f(X))$$</li>
<li>等价于风险经验最小化
$$\frac{1}{k}\sum\limits_{x_i\in N_k(x)} I(y_i\ne c_j)=1-\frac{1}{k}\sum\limits_{x_i\in N_k(x)} I(y_i=c_j)$$</li>
</ul>
<h3 id="模型预测" class="headerLink">
    <a href="#%e6%a8%a1%e5%9e%8b%e9%a2%84%e6%b5%8b" class="header-mark"></a>模型预测</h3><h4 id="预测流程" class="headerLink">
    <a href="#%e9%a2%84%e6%b5%8b%e6%b5%81%e7%a8%8b" class="header-mark"></a>预测流程</h4><p>首先引入最简单的思路：线性扫描的方法。</p>
<ol>
<li>对未知类别的数据集中的每个点：</li>
</ol>
<ul>
<li>计算已知类别数据集众多点与当前点之间的距离；</li>
<li>按照距离递增次序排序。</li>
</ul>
<ol start="2">
<li>选取与当前点距离最小的k个点：</li>
</ol>
<ul>
<li>选定前k个点所在类别的出现频率</li>
<li>返回前k个点出现频率最高的类别作为当前点的预测分类</li>
</ul>
<ol start="3">
<li>重复步骤，完成对所有点的预测分类</li>
</ol>
<h4 id="python实现" class="headerLink">
    <a href="#python%e5%ae%9e%e7%8e%b0" class="header-mark"></a>Python实现</h4><p><figure><a class="lightgallery" href="https://z1.ax1x.com/2023/11/01/piuKkSx.png" title="kNN算法流程可视化" data-thumbnail="https://z1.ax1x.com/2023/11/01/piuKkSx.png">
        
    </a></figure></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_nearest_labels</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">most_common</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">k_nearest_labels</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">most_common</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 一个简单的例子：</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">clf</span> <span class="o">=</span> <span class="n">KNN</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="改进" class="headerLink">
    <a href="#%e6%94%b9%e8%bf%9b" class="header-mark"></a>改进</h3><h4 id="主要挑战" class="headerLink">
    <a href="#%e4%b8%bb%e8%a6%81%e6%8c%91%e6%88%98" class="header-mark"></a>主要挑战</h4><ol>
<li>前置处理：特征的选择</li>
<li>模型</li>
</ol>
<ul>
<li>合适的度量函数</li>
<li>合适的K值</li>
<li>降低训练和预测的复杂度</li>
</ul>
<h4 id="kd树" class="headerLink">
    <a href="#kd%e6%a0%91" class="header-mark"></a>kd树</h4><p>一种二叉树数据结构，用于优化搜索算法。</p>
<p><strong>优势：</strong></p>
<ol>
<li>降低搜索维度</li>
<li>提高搜索效率</li>
<li>更少的存储需求</li>
<li>支持范围搜索</li>
</ol>
<p>可能因数据的特定分布而表现不佳。<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<h5 id="构造11" class="headerLink">
    <a href="#%e6%9e%84%e9%80%a011" class="header-mark"></a>构造<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></h5><ol>
<li>构造根结点，使根结点对应于 k 维空间中包含所有实例点的超矩形区域。</li>
<li>递归（生成子结点）：</li>
</ol>
<ul>
<li>选择坐标轴和切分点，确定一个超平面</li>
<li>将当前超矩形区域切分为左右两个子区域</li>
<li>直到子区域内没有实例时终止。</li>
</ul>
<ol start="3">
<li>实例保存在相应的结点上。</li>
</ol>
<p><strong>如何选择：</strong></p>
<ul>
<li>空间切分参照：坐标轴</li>
<li>切分点的选择：中位数</li>
</ul>
<p><figure><a class="lightgallery" href="https://z1.ax1x.com/2023/11/01/piuKif1.png" title="kd树的构造" data-thumbnail="https://z1.ax1x.com/2023/11/01/piuKif1.png">
        
    </a></figure></p>
<h5 id="搜索" class="headerLink">
    <a href="#%e6%90%9c%e7%b4%a2" class="header-mark"></a>搜索</h5><p><figure><a class="lightgallery" href="https://z1.ax1x.com/2023/11/01/piuKAl6.png" title="kd树的搜索" data-thumbnail="https://z1.ax1x.com/2023/11/01/piuKAl6.png">
        
    </a></figure></p>
<h5 id="算法" class="headerLink">
    <a href="#%e7%ae%97%e6%b3%95" class="header-mark"></a>算法</h5><p>[输入] 已构造的 kd 树，目标点 x;</p>
<p>[输出] x 的 k 近邻。</p>
<ol>
<li>在 kd 树中找出包含目标点 x 的叶结点：从根结点出发，递归地向下访问 kd 树。若目标点 x 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止。</li>
<li>构建“当前 k 近邻点集”，将该叶结点插入“当前 k 近邻点集”，并计算该结点到目标点 x 的距离。</li>
<li>递归地向上回退，在每个结点进行以下操作:</li>
</ol>
<ul>
<li>如果“当前 k 近邻点集”的元素数量 &lt; k，则将该结点插入“当前 k 近邻点集”，并计算该结点到目标点 x 的距离;</li>
<li>如果“当前 k 近邻点集”的元素数量 = k，但该结点到目标点 x 的距离小于“当前 k 近邻点集”中最远 点到目标点 x 的距离，则将该结点插入“当前 k 近邻点集”，并删除原先的最远点。</li>
<li>检查另一子结点对应的区域是否与以目标点 x 为球心、以目标点 x 与“当前 k 近邻点集”中最远点的距离为半径的超球体相交。 如果相交，可能在另一个子结点对应的区域内存在距离目标点更近的点，移动到另一个子结点，接着，递归地进行 k 近邻搜索; 如果不相交，向上回退。</li>
</ul>
<ol start="4">
<li>当回退到根结点时，搜索结束(若此时“当前 k 近邻点集”中的元素不足 k 个，则需要访问另一半树的结点)。</li>
<li>最后的“当前 k 近邻点集”中的 k 个点即为 x 的 k 近邻点。</li>
</ol>
<h5 id="python实现-1" class="headerLink">
    <a href="#python%e5%ae%9e%e7%8e%b0-1" class="header-mark"></a>Python实现</h5><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">left</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">left</span> <span class="o">=</span> <span class="n">left</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">right</span> <span class="o">=</span> <span class="n">right</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">KdTree</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">create_Tree</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">dataset</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="n">mid_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="n">axis</span> <span class="o">=</span> <span class="n">depth</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span>
</span></span><span class="line"><span class="cl">        <span class="n">sort_dataset</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="n">axis</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">        <span class="n">mid_data</span> <span class="o">=</span> <span class="n">sort_dataset</span><span class="p">[</span><span class="n">mid_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_node</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">mid_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">left_data</span> <span class="o">=</span> <span class="n">sort_dataset</span><span class="p">[:</span><span class="n">mid_index</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">right_data</span> <span class="o">=</span> <span class="n">sort_dataset</span><span class="p">[</span><span class="n">mid_index</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_node</span><span class="o">.</span><span class="n">left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_Tree</span><span class="p">(</span><span class="n">left_data</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">cur_node</span><span class="o">.</span><span class="n">right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_Tree</span><span class="p">(</span><span class="n">right_data</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">cur_node</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">new_data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">near_point</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="n">node</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">return</span>
</span></span><span class="line"><span class="cl">            <span class="n">axis</span> <span class="o">=</span> <span class="n">depth</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">new_data</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">[</span><span class="n">axis</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dfs</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">dfs</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">right</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span> <span class="ow">or</span> <span class="n">dist</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span> <span class="o">=</span> <span class="n">dist</span>
</span></span><span class="line"><span class="cl">                <span class="bp">self</span><span class="o">.</span><span class="n">near_point</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_data</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">-</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">near_val</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">new_data</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">node</span><span class="o">.</span><span class="n">val</span><span class="p">[</span><span class="n">axis</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">dfs</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">right</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">                <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">dfs</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">dfs</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">near_point</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">point_1</span><span class="p">,</span> <span class="n">point_2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">res</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">res</span> <span class="o">+=</span> <span class="p">(</span><span class="n">point_1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">point_2</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">res</span> <span class="o">**</span> <span class="mf">0.5</span>
</span></span><span class="line"><span class="cl"><span class="n">data_set</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">9</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">7</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">2</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">new_data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_set</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">kd_tree</span> <span class="o">=</span> <span class="n">KdTree</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">our_tree</span> <span class="o">=</span> <span class="n">kd_tree</span><span class="o">.</span><span class="n">create_Tree</span><span class="p">(</span><span class="n">data_set</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">predict</span> <span class="o">=</span> <span class="n">kd_tree</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">our_tree</span><span class="p">,</span> <span class="n">new_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="马氏距离" class="headerLink">
    <a href="#%e9%a9%ac%e6%b0%8f%e8%b7%9d%e7%a6%bb" class="header-mark"></a>马氏距离</h4><p>由P.C. Mahalanobis提出；基于样本分布的一种距离测量。</p>
<ul>
<li>考虑特征之间的相关性</li>
<li>对数据的缩放不敏感</li>
<li>考虑协方差结构</li>
<li>适用于异常值和噪声数据<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup></li>
</ul>
<p>广泛用于分类和聚类分析。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>垃圾邮件过滤：自动将电子邮件分为垃圾邮件和非垃圾邮件。<br>
医学诊断：基于患者的症状数据来诊断疾病或预测病人的疾病风险。<br>
金融风险评估：根据客户的财务和信用记录来评估客户的信用风险。<br>
情感分析：根据文本数据中的情感内容对文本进行情感分类，如积极、消极或中性。<br>
图像识别：对图像进行分类，例如识别数字、物体或人脸等。<br>
生物信息学：基因序列分类，如预测蛋白质功能或基因表达模式。<br>
客户分类：根据客户的行为和偏好将客户分成不同的市场细分。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>KNN算法于1948年由Cover和Hart提出。<br>
存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，只选择样本数据集中前k个最相似的数据。k一般不大于20，最后，选择k个中出现次数最多的分类，作为新数据的分类。<br>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>数值型数据是指具有数量意义的数据，可以进行数学运算和比较。这种数据通常表示为数字，例如年龄、温度、身高等。在机器学习中，数值型数据常用于回归分析和连续变量的预测。标称型数据则是指无序分类的数据，其中每个值代表一个类别而没有数量意义。标称型数据通常表示为符号或字符串，例如血型、性别、品种等。在机器学习中，标称型数据通常用于分类问题，其中算法需要将输入数据映射到预定义的类别或标签。<br>
k最近邻算法 (kNN) 适用于处理这两种类型的数据。对于数值型数据，它可以基于数值之间的距离进行分类；对于标称型数据，它可以根据邻近样本的标签进行投票，并将测试样本分类为获得最多投票的类别。因此，kNN 算法对于这两种数据类型都有较好的适用性。<br>
还有其他类型：<br>
顺序型数据：顺序型数据是一种具有顺序或等级关系的数据类型，其中数据值之间存在某种顺序关系，但没有明确的数值差异。例如，学历等级（如小学、初中、高中、大学等）可以被视为顺序型数据。<br>
时间序列数据：时间序列数据是按照时间顺序排列的数据集合，通常是在一系列连续时间点上收集的数据。例如，股票价格、天气数据、经济指标等都属于时间序列数据。<br>
区间型数据：区间型数据是指数据值表示某个范围内的值，而不是特定的数值。这种数据类型通常用于表示测量的范围。例如，温度范围、年龄段等可以被视为区间型数据。<br>
比率型数据：比率型数据是具有固定比例关系的数据类型，其中数据之间存在明确的比率关系。比率型数据具有绝对零点，可以进行比较和数学运算。例如，长度、重量、时间间隔等都属于比率型数据。<br>
文本数据：文本数据是指以自然语言形式表示的数据，通常包含语句、段落或文档。处理文本数据通常需要使用自然语言处理技术来提取、转换和分析文本信息。<br>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>老师反馈：这里的表述并不严谨，模型需要通过交叉验证来确认参数 k&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>简单直观，非参数化：kNN 是一种非参数化方法，不对数据的分布做任何假设。因此，在处理复杂的数据集和未知的数据分布时，它通常具有很好的适应性。<br>
对异常值鲁棒：kNN 对异常值比较鲁棒，因为它基于周围数据点的多数投票来确定分类，可以减少异常值对结果的影响。<br>
适应多类别问题：kNN 能够很好地适应多类别分类问题，因为它可以通过投票的方式来确定一个实例所属的类别。<br>
但是对高维数据的处理效率较低，需要大量的存储空间和计算时间；在数据不平衡或噪声较多的情况下，它可能会产生较差的分类结果。<br>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>计算成本高：kNN 算法需要计算每个测试点与所有训练点之间的距离，因此在处理大规模数据集时，计算成本会变得非常高。<br>
存储成本高：除了计算成本高外，kNN 算法还需要存储整个训练集，这对于大规模数据集来说会占用大量的存储空间。<br>
维度灾难：随着数据维度的增加，kNN 算法的性能可能会下降，因为在高维空间中，数据点之间的距离变得更加稀疏，导致算法的效率降低。<br>
数据不平衡问题：在处理数据不平衡或噪声较多的数据集时，kNN 算法可能会受到数据分布的影响，从而导致分类性能下降。<br>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>近似误差是指模型用于近似真实关系的误差，通常表示模型与真实值之间的差异。<br>
估计误差是指使用样本数据估计整体数据集特征时产生的误差。在 kNN 中，估计误差通常与样本的选择和样本的分布有关。<br>
K值的减小：模型变得复杂，容易发生过拟合。相当于用较小的邻域中的训练实例进行预测，只有与输入实例较近的(相似的)训练实例才会对预测结果起作用，对噪声敏感，即预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。<br>
K值的增大：就意味着整体的模型变得简单.产生更平滑的决策边界，但可能会忽略数据的局部特征。这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误。k 值的增大就意味着整体的模型变得简单。<br>
k 值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k 值。具体可以取部分训练集作为测试集，在不同取值条件下观察最优值。&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>特征空间中两个实例点的距离是两个实例点相似程度的反映。k 近邻模型的特征空间一般是 n 维实数向量空间 $R^n$，使用的距离是欧氏距离，但也可以是其他距离，如更一般的 $L_p$ 距离或 Minkowski 距离。&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>实现 k 近邻法时，主要考虑的问题是如何对训练数据进行快速 k 近邻搜索。这点在特征空间的维数大及训练数据容量大时尤其必要。k 近邻法最简单的实现方法是线性扫描。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。为了提高k 近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。具体方法很多，下面介绍其中的 kd 树方法。<br>
使用 kd 树相比直接计算方法的主要好处在于它可以有效地减少计算量。kd 树是一种二叉树数据结构，它可以用于优化搜索算法，特别是在高维空间中。<br>
以下是 kd 树相对于直接计算方法的一些优势：<br>
降低搜索维度：kd 树能够将搜索范围缩小到与搜索点最近的局部区域，从而避免不必要的计算。<br>
提高搜索效率：在具有大量数据点的高维空间中，kd 树可以更快地定位最近邻居，因为它可以避免对所有数据点进行逐一比较。<br>
更少的存储需求：相对于直接计算方法，kd 树通常需要更少的存储空间，因为它可以通过二叉树结构有效地组织数据。<br>
支持范围搜索：除了最近邻搜索之外，kd 树还可以很容易地扩展到支持范围搜索，以查找在给定半径内的所有邻居。<br>
尽管 kd 树具有这些优势，但它可能会因数据的特定分布而表现不佳。例如，在存在大量密集聚集数据点的区域，kd 树的性能可能会下降。因此，在实际应用中，应该根据数据集的特点选择合适的算法来进行近邻搜索。&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>kd 树是一种对 k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd 树是二叉树，表示对 k 维空间的一个划分。构造 kd 树相当于不断地用垂直于坐标轴的超平面将k 维空间切分，构成一系列的飞 维超矩形区域。kd树的每个结点对应于一个k 维超矩形区域。<br>
构造 kd 树的方法如下:构造根结点，使根结点对应于 k 维空间中包含所有实例点的超矩形区域:通过下面的递归方法，不断地对 k 维空间进行切分，生成子结点。在超矩形区域(结点)上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。<br>
平衡树：使用中位数作为划分点可以保证树的相对平衡，避免出现极端情况下的不平衡树结构，从而使得搜索效率总体比较高，但未必最优。考虑一些离群点。&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>考虑特征之间的相关性：马氏距离能够考虑数据特征之间的相关性，而欧氏距离只考虑各个维度之间的直线距离。这意味着马氏距离在具有相关特征的数据集上能够提供更加准确的距离度量。<br>
对数据的缩放不敏感：在某些情况下，数据的不同特征可能具有不同的度量单位或尺度。马氏距离能够对数据的缩放不敏感，因此可以更好地处理这种情况，而欧氏距离可能受到数据尺度的影响。<br>
考虑协方差结构：马氏距离考虑了数据的协方差结构，因此可以更好地捕捉数据特征之间的线性关系。这使得马氏距离在处理多元正态分布数据时能够提供更加准确的距离度量。<br>
适用于异常值和噪声数据：马氏距离能够对异常值和噪声数据具有更好的鲁棒性，因为它考虑了数据的协方差结构，可以减少这些异常值对距离度量的影响。&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>]]></description>
</item><item>
    <title>CSS 强化</title>
    <link>https://blog.ralvines.top/blog-css/</link>
    <pubDate>Tue, 12 Sep 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/blog-css/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/11/01/pinHqnH.png" referrerpolicy="no-referrer">
            </div><h2 id="css" class="headerLink">
    <a href="#css" class="header-mark"></a>CSS</h2><h3 id="与-html-的关系" class="headerLink">
    <a href="#%e4%b8%8e-html-%e7%9a%84%e5%85%b3%e7%b3%bb" class="header-mark"></a>与 HTML 的关系</h3><ul>
<li>内容与结构的分离</li>
<li>在 HTML 上嵌入层叠样式表
<ul>
<li>外部引用 .css</li>
<li>内部/局部使用<code>&lt;style&gt;</code>, <code>&lt;p style=&quot;&quot;&gt;</code></li>
</ul>
</li>
</ul>
<h3 id="基本语法" class="headerLink">
    <a href="#%e5%9f%ba%e6%9c%ac%e8%af%ad%e6%b3%95" class="header-mark"></a>基本语法</h3><ul>
<li>
<p>属性内分隔符<code>,</code></p>
</li>
<li>
<p>属性间分隔符<code>;</code></p>
</li>
<li>
<p>允许叠加属性符 <code>ex: top right</code></p>
</li>
<li>
<p><code>background</code><strong>背景</strong></p>
<p><code>-color</code> 背景色</p>
<p><code>-image: url(pic.file)</code> 背景图片</p>
<p><code>-repeat: repeat/no-repeat</code> 重复性</p>
<p><code>-position: center/right/top/100px 100px</code> 位置</p>
<p><code>-attchment: scroll/fixed</code> 固定性</p>
</li>
<li>
<p><strong>文本段落</strong></p>
<ul>
<li>
<p>格式</p>
<p><code>text-indent: -2em/10%/2in/2cm</code> 首行缩进</p>
<p><code>padding: 2em </code> 悬挂缩进 即非首行的缩进</p>
<p><code>line-height: normal/1.5</code> 行距 倍数</p>
<p><code>text-align: left/right/center/justify</code> 对齐 (justify: 两端对齐)</p>
<p><code>word-spacing: 10px</code> 空格大小</p>
<p><code>letter-spacing: 10px</code> 字符间距</p>
<p><code>text-transform: uppercase/lowcase/capitalize</code> 变形: 全大写/小写/首字母大写</p>
<p><code>text-decoration: underline/overline/linethrough </code> 文本装饰</p>
<p><code>white-space: normal/pre/wrap/no-wrap/pre-line</code> 空白符样式</p>
<p><code>direction: ltr/rtl</code> 方向</p>
</li>
<li>
<p><strong>字体</strong></p>
<p><code>font-family: serif/sans-serif/nomospace</code></p>
<p><code>font-style: italic/obique</code> 斜体/计算斜体</p>
<p><code>font-variant: small-caps</code> 大写字母缩小</p>
<p><code>font-weight: 400/bold</code> 字重</p>
<p><code>font-size: 0.5em/10px</code></p>
<p><code>text-shadow: 3px 5px 5px rgba(r,g,b,a)</code> 阴影: x, y, z 轴延伸值</p>
<p><code>outline-color: redl; outline-style: solid/double; outline-width: 1</code> 轮廓</p>
</li>
<li>
<p><strong>列表</strong></p>
<p><code>&lt;ul style=&quot;list-style[keyword]: [keyword]&quot;&gt;</code></p>
<p><code> -type: disc/circle/square</code></p>
<p><code>-position: </code></p>
</li>
<li>
<p>表格</p>
</li>
<li></li>
</ul>
</li>
<li>
<p>颜色</p>
<p><code>transparent</code> 关键字 (透明)</p>
<p><code>#ffffff</code> 十六进制</p>
<p><code>rgb(255,255,0)</code> 十进制 (0~255)</p>
<p><code>rgba(r,g,b,a)</code> a=alpha 透明度 (0~1)</p>
</li>
</ul>
<h2 id="html-markdown-对照表" class="headerLink">
    <a href="#html-markdown-%e5%af%b9%e7%85%a7%e8%a1%a8" class="header-mark"></a>HTML-MarkDown 对照表</h2><table>
<thead>
<tr>
<th>描述</th>
<th>markdown</th>
<th>HTML TAG</th>
</tr>
</thead>
<tbody>
<tr>
<td>标题</td>
<td><code># 一级标题</code> <code>## 二级标题</code>…</td>
<td>h1,h2…h6</td>
</tr>
<tr>
<td>粗体</td>
<td><code>**粗体**</code></td>
<td>b , strong</td>
</tr>
<tr>
<td>斜体</td>
<td><code>*斜体*</code></td>
<td>em, i</td>
</tr>
<tr>
<td>分割线</td>
<td><code>---</code></td>
<td>hr</td>
</tr>
<tr>
<td>无序列表</td>
<td><code>* 1</code> <code>- 1</code> <code>+ 1</code></td>
<td>ul</td>
</tr>
<tr>
<td>有序列表</td>
<td><code>1.</code></td>
<td>ol</td>
</tr>
<tr>
<td>引用</td>
<td><code>&gt;</code></td>
<td>blockquote</td>
</tr>
<tr>
<td>超链接</td>
<td><code>[url](url)</code></td>
<td>a</td>
</tr>
<tr>
<td>图片</td>
<td><code>![img](imgUrl)</code></td>
<td>img</td>
</tr>
<tr>
<td>表格</td>
<td>见下</td>
<td>table</td>
</tr>
<tr>
<td>代码段</td>
<td>见下</td>
<td>code</td>
</tr>
<tr>
<td>段落</td>
<td>无特殊表示</td>
<td>p</td>
</tr>
</tbody>
</table>]]></description>
</item><item>
    <title>C&#43;&#43; 面向对象之面试与机考技巧</title>
    <link>https://blog.ralvines.top/c-interview/</link>
    <pubDate>Wed, 30 Aug 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/c-interview/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/11/01/pinHqnH.png" referrerpolicy="no-referrer">
            </div><h2 id="笔试技巧" class="headerLink">
    <a href="#%e7%ac%94%e8%af%95%e6%8a%80%e5%b7%a7" class="header-mark"></a>笔试技巧</h2><ol>
<li>输入输出</li>
</ol>
<ul>
<li>带空格 getline</li>
<li>字符串处理 string</li>
<li>大小写转换
<ul>
<li>string: <code>&lt;algorithm&gt;</code> <code>transform(str.begin(),str.end(),str.begin(),::tolower);</code></li>
<li>char: <code>-'a'+'A'</code></li>
</ul>
</li>
<li>进制转换
<ul>
<li><code>bitset</code></li>
<li>8(oct), 10(dec), 16(hex), 2(bitset(num))</li>
</ul>
</li>
<li>取整 (int)直接去除小数点后的部分</li>
</ul>
<ol start="2">
<li>排序 sort 默认升序 自定义规则</li>
</ol>
<ul>
<li><code>functional greater&lt;Type&gt;</code></li>
</ul>
<ol start="3">
<li>质数 因数分解</li>
</ol>
<ul>
<li>时间复杂度 先判断n不是素数再进入计算循环</li>
<li>分解到sqrt(n) 剩下的留n即可</li>
</ul>
<h2 id="经典面试题" class="headerLink">
    <a href="#%e7%bb%8f%e5%85%b8%e9%9d%a2%e8%af%95%e9%a2%98" class="header-mark"></a>经典面试题</h2><ol>
<li>new/delete和malloc/free的区别</li>
</ol>
<ul>
<li>malloc/free是C/C++的库函数，需要stdlib.h；new/delete是C++的关键字；</li>
<li>都可用于申请动态内存和释放内存，new/delete在对象创建的时候自动执行构造函数，对象消亡前自动执行析构函数，底层实现其实也是malloc/free</li>
<li>new无需指定内存块的大小，编译器会根据类型信息自行计算；malloc需要显式地支持所需内存的大小</li>
<li>new返回<strong>指定类型</strong>的指针，无需进行类型转换；malloc默认返回类型为<strong>void</strong>*，必须强行转换为实际类型的指针</li>
<li>new内存分配失败时会抛出bad_alloc异常；malloc失败时返回NULL</li>
</ul>
<ol start="2">
<li>malloc的底层实现</li>
</ol>
<p>Linux下：</p>
<ul>
<li>开辟空间小于128K时，通过<strong>brk()函数</strong>
<ul>
<li>将数据段.data的最高地址指针**_edata<strong>向高地址移动，即</strong>增加堆**的有效区域来申请内存空间</li>
<li>brk分配的内存需要等到高地址内存释放以后才能释放，这也是内存碎片产生的原因</li>
</ul>
</li>
<li>开辟空间大于128K时，通过<strong>mmap()函数</strong>
<ul>
<li>利用mmap系统调用，在堆和栈之间<strong>文件映射区域</strong>申请一块虚拟内存</li>
<li>128K限制可由M_MMAP_THRESHOLD选项进行修改</li>
<li>mmap分配的内存可以单独释放</li>
</ul>
</li>
<li>以上只涉及虚拟内存的分配，直到进程第一次访问其地址时，才会通过缺页中断机制分配到物理页中</li>
</ul>
<ol start="3">
<li>指针和引用的异同点；如何相互转换</li>
</ol>
<ul>
<li>本质：引用是别名，而指针是地址</li>
<li>指针在运行时可以改变所指向的值，而引用一旦与某个对象绑定之后就不再改变(指向的地址不能改变，但指向的内容可以改变)</li>
<li>指针变量在符号表上对应的地址值为<strong>指针变量的地址值</strong>，而引用在符号表上对应的地址值为<strong>引用对象的地址值</strong>；因此指针可以改变指向的对象，而引用的对象不能修改</li>
<li>由于硬件通过地址访问内存位置，因此引用可以理解为一个常量指针，只能绑定到初始化它的对象上</li>
</ul>
<ol start="4">
<li>struct、union的异同</li>
</ol>
<ul>
<li>struct中每个变量依次存储；union中，每个变量都是从偏移地址零开始存储，同一时刻只有一个成员存储于该地址</li>
<li>struct内存大小遵循<strong>结构对齐</strong>原则
<ul>
<li>数据成员对齐规则：每个数据成员存储的起始位置要从该成员大小的整数倍开始</li>
<li>数据成员包含结构体：结构体成员要从其内部最大元素对象的整数倍地址开始存储</li>
<li>结构体总大小：其内部最大基本成员的整数倍，不足则要补齐</li>
</ul>
</li>
<li>union内存大小为其最大成员的整数倍</li>
</ul>
<ol start="5">
<li>extern C的作用</li>
</ol>
<p>C++支持<strong>函数重载</strong>，即不同名字空间namespace的两个函数原型声明可以完全相同，或者两个函数同名但参数列表不同；g++编译器会对此进行<strong>name mangling</strong>，生成全局唯一的符号名称，使链接器可以准确识别</p>
<p>C语言不支持函数重载，即不允许同名符号，所以不需要这些工作，因此在C++代码中加入extern C，是为了<strong>链接规范</strong></p>
<ol start="6">
<li>memcpy()函数需要注意哪些问题</li>
</ol>
<ul>
<li>
<p>函数原型声明void *memcpy(void *dest, void *src, unsigned int count);</p>
</li>
<li>
<p>memcpy函数用于把资源内存（src所指向的内存区域）中连续的count个字节数据拷贝到目标内存（dest所指向的内存区域）</p>
</li>
<li>
<p>数据长度count的单位是字节，1byte = 8bit</p>
</li>
<li>
<p>数据类型为char，则数据长度就等于元素的个数；其他数据类型则要注意数据长度的值</p>
</li>
<li>
<p>n * sizeof(type_name)的写法</p>
</li>
</ul>
<ol start="7">
<li>strcat、strncat、strcmp、strcpy函数</li>
</ol>
<ul>
<li>strcpy拷贝函数，不会判断拷贝大小，也没有任何安全检查，不会检查目的地址内存是否够用；</li>
<li>strncpy拷贝函数，会计算复制字符串的大小，但没有检查目标的边界；</li>
<li>strcmp比较函数，把src所指向的字符串与dest所指向的字符串进行比较，若dest与src的前n个字符相同，则返回0；若dest大于src，则返回大于0的值；若dest小于src，则返回小于0的值</li>
<li>strcat功能是将两个char类型连接；strncat功能是在字符串的结尾追加n个字符</li>
</ul>
<ol start="8">
<li>机器大小端问题</li>
</ol>
<p>大端指数据的<strong>高字节</strong>保存在内存的<strong>低地址</strong>中，数据的<strong>低字节</strong>保存在内存的<strong>高地址</strong>中；小端与此相反。</p>
<ul>
<li>小端：强制转换数据不需要调整字节内容，1、2、4字节的存储方式一样</li>
<li>大端：符号位的判定固定为第一个字节，很容易判断正负</li>
</ul>
<p>union判断大小端的方法</p>
<ul>
<li>union从低地址开始存，同一时间内只有一个成员占用内存；修改其中一个成员的值必然会影响另一个成员的值</li>
</ul>
<ol start="9">
<li>static的用法（定义和用途）</li>
</ol>
<p>修饰</p>
<ul>
<li>局部变量：使其变为<strong>静态存储方式</strong>（静态数据区），函数执行完成之后不会被释放，而是继续保存在内存中；</li>
<li>全局变量：使其只在本文件内部有效，其他文件不可链接或引用该变量；</li>
<li>函数：静态函数，即函数只在本文件内部有效，对其他文件不可见；避免同名干扰，同时保护</li>
</ul>
<ol start="10">
<li>const的用法（定义和用途）</li>
</ol>
<p>const起到<strong>强制保护</strong>的修饰作用，可以预防意外改动，提高程序的健壮性</p>
<ul>
<li>const修饰常量：定义时就初始化，以后不能更改；</li>
<li>const修饰形参：func(const int a); 该形参在函数里不能改变；</li>
<li>const修饰类成员函数：const类成员函数不能改变成员变量的数值</li>
</ul>
<ol start="11">
<li>const常量和define的区别（编译阶段、安全性、内存占用等）</li>
</ol>
<ul>
<li>const定义的常量有类型名字，存放在内存的静态区域中，在编译时确定其值；</li>
<li>define定义的常量是没有类型的一个<strong>立即数</strong>，编译器会在预处理阶段将程序中所有使用到该常量的地方进行<strong>拷贝替换</strong>；</li>
<li>由于define的拷贝有很多份，故宏定义的内存占用要高得多</li>
</ul>
<ol start="12">
<li>volatile的用法</li>
</ol>
<p>被定义为volatile的变量可能会被意想不到地改变，编译器不会对volatile变量有关的运算进行<strong>编译优化</strong>：每次使用该变量必须从内存地址中读取，而不是保存在寄存器中的备份</p>
<p>用到volatile的几种情况</p>
<ul>
<li>并行设备的硬件寄存器（如状态寄存器）</li>
<li>中断服务子程序会访问到的非自动变量</li>
<li>多线程应用中被几个任务共享的变量</li>
</ul>
<ol start="13">
<li>常量指针、指针常量、常量引用（没有引用常量）</li>
</ol>
<ul>
<li>常量指针即常量的指针，指针所指向的是个常量，可以被赋值为变量的地址，但是不能通过这个指针来修改</li>
<li>指针常量本质是一个常量，指针所指向的值不可以改变，但指向的地址所对应的内容可以变化</li>
</ul>
<p>（具体参考问题17）</p>
<ol start="14">
<li>变量的作用域（全局变量和局部变量）</li>
</ol>
<ul>
<li>全局变量：在所有函数体外部定义的，程序所在部分都可以使用，不受作用域的影响（生命期一直到程序的结束）</li>
<li>局部变量：局限于作用域内，默认为auto关键字修饰，即进入作用域时自动生成，离开作用域时自动消失；</li>
<li>局部变量可以和全局变量重名，在局部变量作用域范围内，全局变量失效，采用的是局部变量的值</li>
</ul>
<ol start="15">
<li>sizeof和strlen</li>
</ol>
<ul>
<li>sizeof是一个操作符或关键字，不是一个函数，而strlen是一个函数</li>
<li>sizeof返回一个对象或类型所占的内存字节数，不会对其中的数据或指针做运算</li>
<li>strlen返回一个字符串的长度，不包括&rsquo;/0'</li>
</ul>
<ol start="16">
<li>sizeof(struct)和内存对齐</li>
</ol>
<p>内存对齐作用：</p>
<ul>
<li>移植原因：某些硬件平台只能在某些特定地址处取特定类型的数据；</li>
<li>性能原因：数据结构(尤其是栈)应尽可能在自然边界上对齐，未对齐内存需要做两次内存访问，对齐内存仅需要一次</li>
</ul>
<p>struct内存对齐原则：</p>
<ul>
<li>结构体成员中，第一个成员偏移量是0，排列在后面的成员的当前偏移量必须是当前成员类型的整数倍</li>
<li>结构体本身占用内存大小，应是结构体内最大数据成员的最小整数倍</li>
<li>**pragma pack(n)**预编译指令，所有成员对齐以n字节为准，不再考虑当前类型和最大结构体内类型</li>
</ul>
<p>union内存对齐原则：</p>
<ul>
<li>union字节数必须是占用字节数最多的成员的字节数的倍数，而且需要能够容纳其他成员</li>
</ul>
<ol start="17">
<li>char * const，const char *</li>
</ol>
<ul>
<li>const char <em>ptr指向字符常量的指针，ptr是一个char</em>类型的常量，所指向的内容不能修改；</li>
<li>char * const ptr指向字符的指针常数，即const指针，不能修改ptr指针，但可以修改该指针指向的内容</li>
</ul>]]></description>
</item><item>
    <title>泛函分析</title>
    <link>https://blog.ralvines.top/fhfx/</link>
    <pubDate>Wed, 01 Mar 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/fhfx/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2022-2023 春夏<br>
🧑‍🏫 王伟<br>
📝 20%小测，20%作业，60%期末</div>
        </div>
    </div>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><ul>
<li>《实变函数与泛函分析概要（第五版）》王声望，郑维行</li>
<li>课程讲义
<ul>
<li>Ch1.1</li>
<li>Ch1.2</li>
<li>Ch2.1</li>
<li>Ch2.2</li>
<li>Ch3.1</li>
<li>Ch3.2</li>
</ul>
</li>
<li>泛函分析笔记 @Reichtum</li>
<li>课后习题讲解 @Reichtum
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/486354129" target="_blank" rel="noopener noreferrer">度量空间</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/524355026" target="_blank" rel="noopener noreferrer">Banach&amp;Hilbert</a></li>
</ul>
</li>
<li><a href="https://classroom.zju.edu.cn/coursedetail?course_id=48021&amp;tenant_code=112" target="_blank" rel="noopener noreferrer"><em>智云课堂回放</em></a></li>
</ul>
<h2 id="ch1-距离空间" class="headerLink">
    <a href="#ch1-%e8%b7%9d%e7%a6%bb%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch1. 距离空间</h2><ul>
<li>
<p>定义</p>
<ul>
<li>距离公理：非负、对称、三角不等式</li>
<li>非空即可定义、不唯一</li>
<li>离散距离空间</li>
<li>$\mathbb{R}^n$
<ul>
<li>欧氏距离、复数域</li>
<li><strong>柯西不等式</strong></li>
<li>max定义</li>
</ul>
</li>
<li>连续函数空间$C[a,b]$</li>
<li>$l^p$
<ul>
<li>元素：无限数列、级数绝对收敛</li>
<li>距离定义
<ul>
<li>Holder不等式</li>
<li><strong>Minkowski不等式</strong>、证明</li>
</ul>
</li>
</ul>
</li>
<li>$l^\infty$</li>
<li>$L^p(F)$
<ul>
<li>可测集F</li>
<li>距离定义
<ul>
<li>Holder不等式</li>
<li>Minkowski不等式</li>
</ul>
</li>
</ul>
</li>
<li>$L^\infty$
<ul>
<li>本性有界</li>
<li>本性有界可测、几乎处处相等看作同元素</li>
<li>距离定义$essinf_F$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>收敛</p>
<ul>
<li>点列收敛
<ul>
<li>性质：极限唯一、有界</li>
<li>子列收敛</li>
</ul>
</li>
<li>欧式空间$\mathbb{R}^n$的收敛（如何证）
<ul>
<li>点列收敛、坐标收敛</li>
</ul>
</li>
<li>$C[a,b]$ 的收敛
<ul>
<li>按照距离导出收敛</li>
<li>某距离收敛等价函数列一致收敛</li>
</ul>
</li>
</ul>
</li>
<li>
<p>点集</p>
<ul>
<li>开球、闭球</li>
<li>开集、闭包、闭集</li>
<li>内点、内部</li>
<li>聚点、导集、孤立点</li>
<li>稠密性</li>
<li>可分性
<ul>
<li>$L^\infty[a,b]$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>连续映射</p>
<ul>
<li>连续性等价条件</li>
<li>归结原则、集合描述</li>
<li>同胚、等距</li>
</ul>
</li>
<li>
<p>完备性</p>
<ul>
<li>柯西基本列</li>
<li>完备性
<ul>
<li>三条定理</li>
<li>$C[a,b]$、$l^p$、$L^\infty(F)$ 完备</li>
<li>$S$ 三角不等式及<strong>完备性证明</strong></li>
</ul>
</li>
<li>完备化
<ul>
<li>完备扩展定理</li>
<li>$C[a,b]\rightarrow L^2[a,b]$</li>
<li>$P\rightarrow C[a,b]$</li>
</ul>
</li>
</ul>
</li>
<li>
<p>稀疏集</p>
<ul>
<li>与球的的充要条件</li>
</ul>
</li>
<li>
<p>闭球套定理、第一/二类型集</p>
</li>
<li>
<p>$l_0^p$</p>
<ul>
<li>子空间、不完备、稠密</li>
</ul>
</li>
<li>
<p>$S$、$s$、$P$</p>
</li>
<li>
<p>准紧集、紧集、全有界集</p>
<ul>
<li>$\epsilon-$ 网</li>
<li>相互关系</li>
<li>紧集套</li>
<li>开覆盖</li>
<li>有限交</li>
<li>连续映射</li>
</ul>
</li>
<li>
<p>不动点定理、压缩映射</p>
</li>
<li>
<p><strong>重点梳理</strong></p>
<ul>
<li>常见度量空间，及其距离、收敛、可分性、准紧条件
<ul>
<li>$\mathbb{R}^n$ （所有分量收敛）</li>
<li>$C[a,b]$（一致收敛）、$C^k[a,b]$、$C^\infty [a,b]$</li>
<li>$l^p$、$l^\infty$（不可分）</li>
<li>$L^p$、$L^\infty$（不可分）</li>
<li>$S$（测度收敛）、$s$（按坐标收敛）</li>
</ul>
</li>
<li>重要证明
<ul>
<li>Cauchy、Holder、Minkowski、Young</li>
</ul>
</li>
<li>各类点集、球、稠密、可分</li>
<li>基本列、收敛、完备</li>
<li>连续映射</li>
<li>不动点、压缩映射</li>
</ul>
</li>
</ul>
<h2 id="ch2-巴拿赫空间希尔伯特空间" class="headerLink">
    <a href="#ch2-%e5%b7%b4%e6%8b%bf%e8%b5%ab%e7%a9%ba%e9%97%b4%e5%b8%8c%e5%b0%94%e4%bc%af%e7%89%b9%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch2. 巴拿赫空间、希尔伯特空间</h2><ul>
<li>赋范线性空间
<ul>
<li>距离空间、范数/强收敛</li>
<li>Banach</li>
<li>商空间</li>
<li>直和</li>
</ul>
</li>
<li>内积空间
<ul>
<li>导出范数
<ul>
<li>Schwarz不等式、极化恒等式</li>
<li>平行四边形公式</li>
</ul>
</li>
<li>$l^2,L^2$</li>
<li>正交、正交补、推广勾股</li>
<li>规范正交系
<ul>
<li>Bessel</li>
<li>完备、完全</li>
<li>Schmidt正交化</li>
<li>最佳逼近</li>
</ul>
</li>
</ul>
</li>
<li>Hilbert
<ul>
<li>凸集、正交分解</li>
<li>E.S.Fischer</li>
<li>Parseval</li>
<li>可分同构</li>
</ul>
</li>
</ul>
<h2 id="ch3-有界线性算子巴拿赫空间" class="headerLink">
    <a href="#ch3-%e6%9c%89%e7%95%8c%e7%ba%bf%e6%80%a7%e7%ae%97%e5%ad%90%e5%b7%b4%e6%8b%bf%e8%b5%ab%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch3. 有界线性算子：巴拿赫空间</h2><h2 id="ch4-有界线性算子希尔伯特空间" class="headerLink">
    <a href="#ch4-%e6%9c%89%e7%95%8c%e7%ba%bf%e6%80%a7%e7%ae%97%e5%ad%90%e5%b8%8c%e5%b0%94%e4%bc%af%e7%89%b9%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch4. 有界线性算子：希尔伯特空间</h2><h2 id="前辈经验" class="headerLink">
    <a href="#%e5%89%8d%e8%be%88%e7%bb%8f%e9%aa%8c" class="header-mark"></a>前辈经验</h2><p>小测：https://www.cc98.org/topic/5321722</p>]]></description>
</item><item>
    <title>复变函数</title>
    <link>https://blog.ralvines.top/fbhs/</link>
    <pubDate>Wed, 01 Mar 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/fbhs/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2023-2024 秋冬<br>
🧑‍🏫 毕惟红<br>
📝 50%读书报告，10%考勤，10%编程，30%两次展示</div>
        </div>
    </div>
<p>!!! quote &quot;&quot;
- 🎓 数学科学学院
- 🕙 2022-2023 春夏
- 🧑‍🏫 齐治
- 📝 40%作业，60%期末</p>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><ul>
<li>《Complex Analysis》, Stein.</li>
<li><a href="https://classroom.zju.edu.cn/coursedetail?course_id=47986&amp;tenant_code=112" target="_blank" rel="noopener noreferrer">智云课堂回放</a></li>
<li>复变函数华师版讲义 @陆俊</li>
</ul>
<h2 id="ch1-复分析预备知识" class="headerLink">
    <a href="#ch1-%e5%a4%8d%e5%88%86%e6%9e%90%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86" class="header-mark"></a>Ch1. 复分析预备知识</h2><h2 id="ch2-柯西定理及应用" class="headerLink">
    <a href="#ch2-%e6%9f%af%e8%a5%bf%e5%ae%9a%e7%90%86%e5%8f%8a%e5%ba%94%e7%94%a8" class="header-mark"></a>Ch2. 柯西定理及应用</h2><h2 id="ch3-亚纯函数及对数" class="headerLink">
    <a href="#ch3-%e4%ba%9a%e7%ba%af%e5%87%bd%e6%95%b0%e5%8f%8a%e5%af%b9%e6%95%b0" class="header-mark"></a>Ch3. 亚纯函数及对数</h2><h2 id="ch5-全函数" class="headerLink">
    <a href="#ch5-%e5%85%a8%e5%87%bd%e6%95%b0" class="header-mark"></a>Ch5. 全函数</h2><h2 id="ch7-留数定理" class="headerLink">
    <a href="#ch7-%e7%95%99%e6%95%b0%e5%ae%9a%e7%90%86" class="header-mark"></a>Ch7. 留数定理*</h2><h2 id="ch8-椭圆函数" class="headerLink">
    <a href="#ch8-%e6%a4%ad%e5%9c%86%e5%87%bd%e6%95%b0" class="header-mark"></a>Ch8. 椭圆函数*</h2><h2 id="ch10-theta函数" class="headerLink">
    <a href="#ch10-theta%e5%87%bd%e6%95%b0" class="header-mark"></a>Ch10. $\Theta$函数*</h2><h2 id="华师版" class="headerLink">
    <a href="#%e5%8d%8e%e5%b8%88%e7%89%88" class="header-mark"></a>华师版</h2>]]></description>
</item><item>
    <title>实变函数</title>
    <link>https://blog.ralvines.top/sbhs/</link>
    <pubDate>Wed, 01 Mar 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/sbhs/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2022-2023 春夏<br>
🧑‍🏫 贾厚玉<br>
📝 20%小测，20%作业，60%期末</div>
        </div>
    </div>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><ul>
<li>《实变函数》，周性伟</li>
<li>PPT
<ul>
<li>Ch1.1</li>
<li>Ch1.2</li>
<li>Ch3</li>
<li>Ch4</li>
<li>Ch5</li>
<li>Ch6</li>
</ul>
</li>
<li><a href="https://classroom.zju.edu.cn/coursedetail?course_id=51173&amp;tenant_code=112" target="_blank" rel="noopener noreferrer"><em>智云课堂回放</em></a></li>
<li><a href="https://www.bilibili.com/video/BV1MX4y1w7fm/?spm_id_from=333.788&amp;vd_source=e81e93bc6892fd0d7e19b265d26a2b3a" target="_blank" rel="noopener noreferrer">实变函数习题十讲</a></li>
<li><a href="https://www.bilibili.com/video/BV1MX4y1w7fm/?spm_id_from=333.788&amp;vd_source=e81e93bc6892fd0d7e19b265d26a2b3a" target="_blank" rel="noopener noreferrer"></a></li>
</ul>
<h2 id="ch1-集合" class="headerLink">
    <a href="#ch1-%e9%9b%86%e5%90%88" class="header-mark"></a>Ch1. 集合</h2><ul>
<li>三次完备化
<ul>
<li>有理数&amp;实数（极限封闭）</li>
<li>黎曼几分&amp;勒贝格积分</li>
<li>广义函数（Dirac）</li>
</ul>
</li>
<li>集合运算
<ul>
<li>表示、集族</li>
<li>幂集</li>
<li>交、并、差、对称差、无穷、间断点集</li>
<li>DeMorgan</li>
</ul>
</li>
<li>集合序列、极限
<ul>
<li>单调集列</li>
<li>上下限集、上下极限</li>
<li>笛卡尔乘积、性质</li>
</ul>
</li>
<li>映射
<ul>
<li>映射（单、满、逆）、像/原像（性质）、复合</li>
</ul>
</li>
<li>特征函数</li>
<li>集合等价、基数
<ul>
<li>集合对等</li>
<li>基数（势）</li>
<li>有限/可数集
<ul>
<li>[0,1]不可数</li>
<li>A~A$\cup$B</li>
</ul>
</li>
<li>连续统势
<ul>
<li>n元数列全体</li>
<li>可数集子集全体</li>
<li>至多可数直积全体</li>
</ul>
</li>
<li>基数比较
<ul>
<li>$A_2\subset A_1\subset A_0,A_0$~$A_2\Rightarrow A_0$~$A_1$</li>
<li>Banach分解、分离集</li>
<li>Cantor-Bernstein定理（真子集对等）</li>
</ul>
</li>
</ul>
</li>
<li>$\mathbb{R}^n$
<ul>
<li>笛卡尔乘积、加法、数乘、内积、模、距离</li>
<li>性质（交换、柯西不等式、系数、三角不等式）</li>
<li>邻域</li>
<li>极限描述（距离、邻域、$\epsilon-N$、分量）</li>
<li>点集
<ul>
<li>内点、内域</li>
<li>外点、外域</li>
<li>边界点</li>
<li>聚点、导集</li>
<li>闭包</li>
<li>孤立点、孤立集</li>
<li>离散集</li>
<li>稠密集、无处稠密集、疏朗集</li>
<li>开集、闭集</li>
<li>自密集、完备集</li>
</ul>
</li>
<li>一些性质、Bolzano-Weierstrass定理</li>
<li>开集构造定理</li>
<li>Cantor完备集
<ul>
<li>性质（无内点、连续统势c、稠子集、开区间长度和）</li>
<li>Cantor函数</li>
</ul>
</li>
<li>长方体（矩体）、方体</li>
</ul>
</li>
<li>连续映射
<ul>
<li>距离函数、性质</li>
<li>开覆盖、紧集
<ul>
<li>充要条件：有界闭集</li>
</ul>
</li>
<li>连续延拓定理</li>
<li>连续函数的集合特征</li>
</ul>
</li>
</ul>
<h2 id="ch2-l可测集" class="headerLink">
    <a href="#ch2-l%e5%8f%af%e6%b5%8b%e9%9b%86" class="header-mark"></a>Ch2. L可测集</h2><ul>
<li>外测度
<ul>
<li>L覆盖（开区间、可有限）</li>
<li>集合函数</li>
<li>单点集（利用数列）</li>
<li>非负、单调、次可数可加、平移不变</li>
<li>区间</li>
<li>$[0,1]$Cantor集</li>
<li>$m^*_\delta (E)$</li>
<li>不相交可加性、介值</li>
</ul>
</li>
<li>可测集$\mathcal{M}$
<ul>
<li>卡氏条件</li>
<li>充要条件</li>
</ul>
</li>
<li>测度
<ul>
<li>零测集（单点、有理数、任意子集）、可测</li>
<li>区间可测</li>
<li>性质（空、交并差、可数交并、可数可加）</li>
<li>单调可测集列</li>
<li>平移不变性</li>
<li>不可测集</li>
<li>Borel集</li>
<li>$G_\delta, F_\sigma$</li>
<li>可测集构造（开/闭集逼近）、等价命题</li>
<li>可测集特征</li>
</ul>
</li>
<li>代数、$\sigma$代数
<ul>
<li>定义</li>
<li>Borel可测、非Borel可测、关系</li>
</ul>
</li>
<li>$\mathbb{R}^n$可测集
<ul>
<li>直积可测性问题</li>
</ul>
</li>
</ul>
<h2 id="ch3-可测函数" class="headerLink">
    <a href="#ch3-%e5%8f%af%e6%b5%8b%e5%87%bd%e6%95%b0" class="header-mark"></a>Ch3. 可测函数</h2><ul>
<li>定义
<ul>
<li>广义实数</li>
<li>可测函数</li>
<li>特征函数可测</li>
<li>稠密和可测例</li>
</ul>
</li>
<li>性质
<ul>
<li>运算</li>
<li>几乎处处</li>
<li>局部有界</li>
</ul>
</li>
<li>连续函数逼近
<ul>
<li>简单函数</li>
<li>支集</li>
</ul>
</li>
<li>测度收敛
<ul>
<li>逐点收敛</li>
<li>一致收敛</li>
<li>几乎处处收敛
<ul>
<li>Egoroff</li>
</ul>
</li>
<li>依测度收敛
<ul>
<li>依测度基本列</li>
<li>Riesz</li>
<li>Lusin、逆命题</li>
</ul>
</li>
<li>连续扩张定理</li>
<li>连续逼近、Frechet</li>
</ul>
</li>
</ul>
<h2 id="ch4-l积分" class="headerLink">
    <a href="#ch4-l%e7%a7%af%e5%88%86" class="header-mark"></a>Ch4. L积分</h2><ul>
<li>非负简单函数
<ul>
<li>分划</li>
<li>L积分、性质</li>
</ul>
</li>
<li>非负可测函数
<ul>
<li>等价定义</li>
<li>性质</li>
<li>Chebyshev不等式</li>
<li>几乎处处有限</li>
<li>积分为0条件</li>
<li>绝对连续性</li>
<li>分布函数</li>
<li>Levi（非负渐升列）</li>
<li>逐项积分</li>
<li>Fatou</li>
</ul>
</li>
<li>一般可测函数
<ul>
<li>有界、控制函数</li>
<li>性质</li>
<li>绝对连续性</li>
<li>LDCT</li>
<li>有界收敛</li>
<li>逐项积分</li>
<li>分片积分</li>
<li>含参</li>
</ul>
</li>
<li>R积分与L积分
<ul>
<li>广义R</li>
<li>重积分、累次积分</li>
<li>Tonelli</li>
<li>分布函数表达</li>
</ul>
</li>
<li>Fubini</li>
<li>可积与连续
<ul>
<li>卷积</li>
</ul>
</li>
</ul>
<h2 id="ch5-l微分" class="headerLink">
    <a href="#ch5-l%e5%be%ae%e5%88%86" class="header-mark"></a>Ch5. L微分</h2><ul>
<li>单调可微
<ul>
<li>Vitali覆盖</li>
<li>覆盖定理</li>
<li>单调微分定理</li>
<li>Dini微商</li>
<li>单调可微性</li>
<li>逐项微分</li>
</ul>
</li>
<li>有界变差</li>
<li>不定积分的微分</li>
<li>绝对连续函数、微积分基本定理</li>
<li>密度、全密点、近似连续点</li>
</ul>
<h2 id="ch6-lp空间" class="headerLink">
    <a href="#ch6-lp%e7%a9%ba%e9%97%b4" class="header-mark"></a>Ch6. $L^p$空间</h2><ul>
<li>定义
<ul>
<li>本性有界（上界、上确界）</li>
<li>$L^\infty$、$||\ ||_\infty$</li>
<li>线性空间</li>
</ul>
</li>
<li>Holder不等式
<ul>
<li>共轭指标</li>
<li>Young不等式</li>
</ul>
</li>
<li>Minkowski不等式</li>
<li>完备距离空间</li>
<li>极限
<ul>
<li>收敛列</li>
<li>柯西基本列</li>
<li>稠密、可分</li>
</ul>
</li>
</ul>
<h2 id="前辈经验" class="headerLink">
    <a href="#%e5%89%8d%e8%be%88%e7%bb%8f%e9%aa%8c" class="header-mark"></a>前辈经验</h2><blockquote>
<p>以下基于个人学习经历与其他情况写一些关于实变函数的学习建议（普适）零.实变函数是近代分析学的起点，起着“地基”的作用。首先明确两个问题，一是这门课所要研究的数学对象，二是为了研究这个数学对象我们引进了哪些数学工具。在数学分析中我们的研究对象是一个单独的函数，所使用的数学工具是微分、积分、极限。实变函数课提出了一个全新的观点：我们不再单独研究一个函数，而是把一些函数打包成一个集合，组成“函数空间”这样一个整体。我们的研究对象就是各种“函数空间”，所使用的工具是“<strong>测度”与“积分</strong>”。分析中的一个重要支柱是利用“函数空间”去解“各种偏微分方程”，这门课是分析学的一个重要基础</p>
<p><strong>一.参考书：</strong></p>
<p>1.学院用的教材是周性伟先生的教材（与同济大学相同），但习题较困难，建议配上周民强的实变函数论，尤其强调例题（大部分与教材中类似或重合）</p>
<p>2.英文参考书：Folland（实分析教材，写的很好，但对初学者阅读难度较大）、Stein（主线清晰，但部分重要结论在习题中，正文直接引用）</p>
<p><strong>二.这门课的核心内容是：</strong></p>
<p>（1）测度论 （2）积分论 （3）利用测度与积分去研究函数空间（主要研究Lp空间与L2空间）</p>
<p>注记1：教材会在最开始花笔墨讲解集合论的东西，引入集合论的原因是我们在这门课的学习中会遇到不可数集，进行不可数的运算，为了避免逻辑上的自相矛盾，我们需要引入选择公理。事实上如果你承认一些基本事实，那么即使不学这部分的内容也无伤大雅。但是对于一些有精神洁癖的同学，凡事都想刨根问底，那么跳过这部分直接学测度论可能就会有一些难受，但是，切记<strong>集合论不是这门课的重点！</strong></p>
<p>注记2：一个很重要的观点：<strong>集合是特殊的函数，测度是特殊的积分</strong></p>
<p>1.**测度：**如何测量一个集合的“度”，这个“度”是“长度”、“面积”、“体积”等概念的推广。教材是从欧式空间的Lebesgue测度讲起，它是欧式空间上最自然最canonical的测度，是最符合我们直觉的测度 它的构造测度的步骤是：“（长方体的）体积-&gt;（任意集合的）外测度/内测度-&gt;（Lebesgue可测集的）Lebesgue测度” 在对一个具体的空间定义好了什么是测度后，我们很自然地要考虑更加整体的性质，即把所有带有测度结构的空间放到一起研究。注意到可测函数的复合依然是可测函数，这是一个重要的性质，我们很多年后也许会忘记这门课具体内容，但应该会记得Littleword三原理。这是一种哲学上的观点：可测集差不多是开集，可测函数差不多是连续函数，依测度收敛差不多是一致收敛。如果我们用范畴的观点去看，Littleword三原理其实就是在比较拓扑范畴与测度范畴的关系。作为范畴中的对象，拓扑结构由开集刻画，测度结构由可测集刻画，Caratheodory定理描述了这两个结构的关系。作为范畴中的态射，拓扑范畴的态射是连续函数，测度范畴的态射是可测函数，Lusin定理描述了这两个态射的关系。这两个范畴中又都有极限结构，Egorov定理描述了这两种收敛的关系</p>
<p>2.**.积分：**对比较特殊的欧式空间，我们会学习Lebesgue积分，它是数学分析中黎曼积分的推广。这里大家要理解：数学分析中学的黎曼积分有哪些不足，Lebesgue积分如何弥补了这些不足；黎曼积分中的许多定理如何推广到Lebesgue积分上去 如何从测度定义积分，是积分论要掌握的核心知识，集合对应于特征函数，所以我们可以定义特征函数的积分，再由sigma可加性，我们可以定义简单函数的积分，再利用Levi单调收敛定理，我们进而可以定义非负可测函数的积分，最后利用绝对值可积，我们定义可测函数的积分。值得注意的是，这不仅做到了“把测度的定义推广到积分的定义”，同时保留了很多良好的性质与定理</p>
<p>3.<strong>Lebesgue积分VSRiemann积分：</strong> 一个自然的问题：黎曼积分中的一些定理和性质是不是在Lebesgue积分的框架下依然成立呢？我们在数学分析中学过以下四个理论：牛顿莱布尼兹定理、局部积分公式、中值定理、链式法则。在推广到Lebesgue积分论的过程中，最重要的一定要知道，我们是在什么框架下推广的！</p>
<p>4.计算题 这门课的计算题大致有：计算Lebesgue积分、计算有界变差。计算Lebesgue积分的常见方法有换元法、局部积分公式、中值定理、单调收敛定理、Lebesgue控制收敛定理。计算有界变差的常见方法有导函数的黎曼积分、计数函数的积分。每种计算的每种方法都可以在书上找到课后习题，这部分的内容就是靠做题练的，<strong>没有捷径</strong>。</p>
</blockquote>
<h2 id="参考书目" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83%e4%b9%a6%e7%9b%ae" class="header-mark"></a>参考书目</h2><ul>
<li>周性伟</li>
<li>周民强</li>
<li>郑维声、王声望</li>
</ul>]]></description>
</item><item>
    <title>数据建模与分析</title>
    <link>https://blog.ralvines.top/sjjm/</link>
    <pubDate>Wed, 01 Mar 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/sjjm/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2022-2023 春夏<br>
🧑‍🏫 郭正初<br>
📝 20%课后作业，15%读书报告，15%编程作业，50%期末考试</div>
        </div>
    </div>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><ul>
<li>PPT
<ul>
<li>Ch1. 机器学习概论</li>
<li>Ch2. 感知机</li>
<li>Ch3. k近邻</li>
<li>Ch4. 朴素贝叶斯</li>
<li>Ch5. 决策树</li>
<li>Ch6. 逻辑斯蒂回归、最大熵模型</li>
<li>Ch7. 支持向量机</li>
<li>Ch8. AdaBoost</li>
<li>Ch13. 无监督学习概论</li>
<li>Ch14. 聚类方法</li>
<li>谱聚类</li>
<li>Ch15. 奇异值分解</li>
<li>Ch16. 主成分分析</li>
<li>Ch19. 马尔可夫链蒙特卡罗法</li>
</ul>
</li>
<li>《统计学习方法（第二版）》，李航</li>
<li><a href="https://classroom.zju.edu.cn/coursedetail?course_id=51611&amp;tenant_code=112" target="_blank" rel="noopener noreferrer"><em>智云课堂回放</em></a></li>
</ul>
<h2 id="ch1-机器学习概论" class="headerLink">
    <a href="#ch1-%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e6%a6%82%e8%ae%ba" class="header-mark"></a>Ch1. 机器学习概论</h2><blockquote>
<p>春一周</p>
</blockquote>
<ul>
<li>人工智能
<ul>
<li>研究目的、内容、表现形式</li>
<li>发展历程、现状</li>
<li>顶刊、顶会</li>
</ul>
</li>
<li>机器学习（统计学习理论）
<ul>
<li>定义（经验）</li>
<li>顶刊、顶会</li>
<li>应用：NLP、CV&hellip;</li>
<li>区别联系
<ul>
<li>数据挖掘（噪声、仓储）</li>
<li>模式识别</li>
</ul>
</li>
</ul>
</li>
<li>大数据
<ul>
<li>4&quot;V&quot;: 量大、类多、实时、密度低</li>
</ul>
</li>
<li>深度学习（ML分支）
<ul>
<li>深度神经网络，假设空间</li>
<li>特征学习</li>
</ul>
</li>
<li>统计机器学习（数据预测与分析）
<ul>
<li>数据驱动</li>
<li><strong>分类</strong>
<ul>
<li>监督/半监督/无监督/强化学习
<ul>
<li>数据标注，概率分布</li>
<li>连续互动</li>
</ul>
</li>
<li>概率/非，线性/非，参数/非</li>
<li>条件概率分布/函数
<ul>
<li>参数维度</li>
</ul>
</li>
<li>在线/批量/离线</li>
<li>贝叶斯/核方法</li>
</ul>
</li>
<li>三要素
<ul>
<li>模型（决策函数/条件概率/参数空间）</li>
<li>策略（损失/风险函数，经验/结构风险最小化）</li>
<li>算法（最优化问题）</li>
</ul>
</li>
<li>模型评估和选择
<ul>
<li>训练误差、测试误差</li>
<li>过拟合、欠拟合</li>
<li>正则化、交叉验证</li>
<li>泛化能力/误差（对未知数据）</li>
<li>集中不等式</li>
</ul>
</li>
<li>生成与判别模型
<ul>
<li>判别方法（直接学习决策函数/概率分布）</li>
<li>生成方法（从联合概率分布到条件概率分布）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ch2-感知机" class="headerLink">
    <a href="#ch2-%e6%84%9f%e7%9f%a5%e6%9c%ba" class="header-mark"></a>Ch2. 感知机</h2><blockquote>
<p>春二周</p>
</blockquote>
<ul>
<li>线性可分性</li>
<li>点到超平面距离、损失函数</li>
<li>随机梯度下降
<ul>
<li>学习率</li>
<li>不唯一（初值、误分类点顺序）</li>
<li><strong>收敛性证明</strong></li>
</ul>
</li>
<li>对偶形式（优点）
<ul>
<li><strong>Gram 矩阵</strong></li>
</ul>
</li>
</ul>
<h2 id="ch3-k近邻" class="headerLink">
    <a href="#ch3-k%e8%bf%91%e9%82%bb" class="header-mark"></a>Ch3. k近邻</h2><blockquote>
<p>春三周</p>
</blockquote>
<ul>
<li>三要素：k，度量，决策规则</li>
<li>优点、缺点（复杂度）</li>
<li>选 k （误差最小、k小复杂过拟合）</li>
<li>kd树（k维）
<ul>
<li>构造</li>
<li>对kNN检索</li>
</ul>
</li>
</ul>
<h2 id="ch4-朴素贝叶斯" class="headerLink">
    <a href="#ch4-%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af" class="header-mark"></a>Ch4. 朴素贝叶斯</h2><blockquote>
<p>春四周</p>
</blockquote>
<ul>
<li>样本空间、全概率公式</li>
<li>期望风险最小化、后验概率最大化</li>
<li>极大似然法
<ul>
<li>对数似然</li>
<li>估计值/量</li>
<li>朴素贝叶斯法的参数估计</li>
</ul>
</li>
<li>贝叶斯估计（极大似然估计、拉普拉斯平滑）</li>
<li></li>
</ul>
<h2 id="ch5-决策树" class="headerLink">
    <a href="#ch5-%e5%86%b3%e7%ad%96%e6%a0%91" class="header-mark"></a>Ch5. 决策树</h2><blockquote>
<p>春五周</p>
</blockquote>
<ul>
<li>分类和回归</li>
<li>CLS</li>
<li>ID3
<ul>
<li>熵、信息量</li>
<li>条件熵、经验熵/条件熵</li>
<li>信息增益、互信息</li>
<li>计算信息增益、选择最优特征</li>
</ul>
</li>
<li>C4.5（信息增益比）
<ul>
<li>连续属性：二元分割</li>
</ul>
</li>
<li>剪枝
<ul>
<li>损失函数</li>
</ul>
</li>
<li>CART
<ul>
<li>基尼指数</li>
<li>回归树、分类树</li>
<li><strong>剪枝</strong></li>
</ul>
</li>
</ul>
<h2 id="ch6-逻辑斯蒂回归最大熵模型" class="headerLink">
    <a href="#ch6-%e9%80%bb%e8%be%91%e6%96%af%e8%92%82%e5%9b%9e%e5%bd%92%e6%9c%80%e5%a4%a7%e7%86%b5%e6%a8%a1%e5%9e%8b" class="header-mark"></a>Ch6. 逻辑斯蒂回归、最大熵模型</h2><blockquote>
<p>春六周</p>
</blockquote>
<ul>
<li>Logistic分布
<ul>
<li>分布函数、密度函数</li>
<li>Sigmoid、tanh</li>
</ul>
</li>
<li>Logistic回归
<ul>
<li>二项</li>
<li>似然函数</li>
<li>多项</li>
</ul>
</li>
<li>最大熵模型
<ul>
<li>学习</li>
<li>极大似然估计</li>
</ul>
</li>
<li>最优化
<ul>
<li>梯度下降</li>
<li>牛顿、拟牛顿
<ul>
<li>黑塞矩阵</li>
<li>正定矩阵（近似）</li>
</ul>
</li>
<li>DFP</li>
<li>BFGS</li>
<li>Broyden</li>
<li>改进迭代尺度</li>
<li>梯度上升、随机梯度上升</li>
</ul>
</li>
</ul>
<h2 id="ch7-支持向量机" class="headerLink">
    <a href="#ch7-%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba" class="header-mark"></a>Ch7. 支持向量机</h2><blockquote>
<p>春七周</p>
</blockquote>
<ul>
<li>线性可分、硬间隔最大化</li>
<li>线性不可分、软间隔最大化</li>
<li>非线性、核函数</li>
<li>序列最小化优化算法</li>
<li>误差分析</li>
</ul>
<h2 id="ch8-adaboost" class="headerLink">
    <a href="#ch8-adaboost" class="header-mark"></a>Ch8. AdaBoost</h2><blockquote>
<p>春八周</p>
</blockquote>
<ul>
<li>强可学习、弱可学习</li>
<li>Boosting、AdaBoost
<ul>
<li>权重、系数</li>
<li>误差分析</li>
<li>前向分步算法</li>
<li>提升树算法
<ul>
<li>回归问题</li>
</ul>
</li>
<li>梯度提升算法</li>
</ul>
</li>
</ul>
<h2 id="ch13-无监督学习概论" class="headerLink">
    <a href="#ch13-%e6%97%a0%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0%e6%a6%82%e8%ae%ba" class="header-mark"></a>Ch13. 无监督学习概论</h2><blockquote>
<p>夏一周</p>
</blockquote>
<ul>
<li>损失最小压缩
<ul>
<li>聚类：硬、软</li>
<li>降维</li>
</ul>
</li>
<li>概率模型
<ul>
<li>混合、概率图（有向、无向）</li>
<li><strong>估计</strong></li>
</ul>
</li>
<li>三要素：模型、策略、方法</li>
<li>话题分析（LDA）</li>
<li>图分析
<ul>
<li>PageRank 计算</li>
</ul>
</li>
</ul>
<h2 id="ch14-聚类方法谱聚类" class="headerLink">
    <a href="#ch14-%e8%81%9a%e7%b1%bb%e6%96%b9%e6%b3%95%e8%b0%b1%e8%81%9a%e7%b1%bb" class="header-mark"></a>Ch14. 聚类方法、谱聚类</h2><blockquote>
<p>夏二周，夏三周</p>
</blockquote>
<ul>
<li>距离
<ul>
<li>Minkowski、欧式、曼哈顿、Chebyshev</li>
<li>马氏、协方差矩阵、相关系数、夹角余弦</li>
</ul>
</li>
<li>簇
<ul>
<li>各种定义</li>
<li>特征划分：散布矩阵、协方差矩阵</li>
<li>类间距离（连接）：最短（单）、最长（完全）、中心、平均</li>
</ul>
</li>
<li>层次聚类
<ul>
<li>聚合、分裂</li>
<li>合并规则、停止条件</li>
</ul>
</li>
<li>k均值
<ul>
<li>欧氏距离、损失函数</li>
<li>初始中心选取</li>
<li>k的选取（平均直径不再增加）</li>
</ul>
</li>
</ul>
<h2 id="ch15-奇异值分解" class="headerLink">
    <a href="#ch15-%e5%a5%87%e5%bc%82%e5%80%bc%e5%88%86%e8%a7%a3" class="header-mark"></a>Ch15. 奇异值分解</h2><blockquote>
<p>夏四周</p>
</blockquote>
<ul>
<li>特征分解/谱分解
<ul>
<li>特征向量、特征值、特征多项式</li>
<li>方阵可对角化&amp;特征向量线性无关</li>
<li>分解: Q, $\Lambda$</li>
<li>实对称情形: 逆</li>
</ul>
</li>
<li>定义
<ul>
<li>分解: U, V, $\Sigma$</li>
<li>奇异值、左右奇异向量</li>
<li>不唯一</li>
<li>存在性、<strong>证明</strong>（从V到U的构造）</li>
</ul>
</li>
<li>类型
<ul>
<li>完全分解</li>
<li>紧凑分解（r，等秩）</li>
<li>截断分解（k，实际）</li>
</ul>
</li>
<li>性质
<ul>
<li>线性变换解释（分解：旋转、缩放、旋转）</li>
<li>等价特征分解（V、U代表的特征向量）</li>
<li>奇异向量构成的标准正交基（由正交性）</li>
</ul>
</li>
<li>计算</li>
<li>矩阵近似
<ul>
<li>F范数</li>
<li>矩阵的F范数与其奇异值的关系</li>
<li>平方损失下的最优近似、<strong>证明</strong></li>
<li>外积展开式、最优近似矩阵的计算</li>
</ul>
</li>
</ul>
<h2 id="ch16-主成分分析" class="headerLink">
    <a href="#ch16-%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90" class="header-mark"></a>Ch16. 主成分分析</h2><blockquote>
<p>夏五周</p>
</blockquote>
<ul>
<li>数据分析、机器学习预处理</li>
<li>思路
<ul>
<li>规范化：平均值0，方差1</li>
<li>正交变换、线性相关转无关变量（主成分）</li>
<li>方差和最大化的正交变换（椭圆长轴）</li>
</ul>
</li>
<li>定义
<ul>
<li>均值向量$\mu$、协方差矩阵、</li>
</ul>
</li>
<li>总体PCA</li>
<li>样本PCA</li>
</ul>
<h2 id="ch19-马尔可夫链蒙特卡罗法" class="headerLink">
    <a href="#ch19-%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%93%be%e8%92%99%e7%89%b9%e5%8d%a1%e7%bd%97%e6%b3%95" class="header-mark"></a>Ch19. 马尔可夫链蒙特卡罗法</h2><ul>
<li>蒙特卡洛法
<ul>
<li>直接抽样</li>
<li>接受-拒绝抽样（建议分布）</li>
<li>期望/积分计算</li>
</ul>
</li>
<li>马尔可夫链
<ul>
<li>时间齐次</li>
<li>高阶</li>
<li>转移概率矩阵、随机矩阵</li>
<li>平稳分布（充要条件）</li>
<li>连续状态、转移核</li>
<li>性质
<ul>
<li>不可约</li>
<li>非周期</li>
<li>正常返</li>
<li>唯一平稳分布（有限、无限）</li>
<li>遍历定理</li>
<li>可逆</li>
</ul>
</li>
</ul>
</li>
<li>马尔可夫链蒙特卡罗法
<ul>
<li>燃烧期</li>
<li>步骤</li>
</ul>
</li>
<li>Metropolis-Hastings
<ul>
<li>单分量</li>
</ul>
</li>
<li>吉布斯抽样
<ul>
<li>抽样计算</li>
</ul>
</li>
</ul>
<h2 id="历年卷" class="headerLink">
    <a href="#%e5%8e%86%e5%b9%b4%e5%8d%b7" class="header-mark"></a>历年卷</h2><h3 id="20-21-春夏" class="headerLink">
    <a href="#20-21-%e6%98%a5%e5%a4%8f" class="header-mark"></a>20-21 春夏</h3><ul>
<li>Kd树 书上例题 找最近邻</li>
<li>熵H(p)的定义，证明H(p)在0到log(n)之间</li>
<li>朴素贝叶斯 书上例题</li>
<li>SVM含义以及与感知机的区别
<ul>
<li>推导出 SVM 的对偶问题</li>
<li>如何通过对偶问题的解得到原问题的解</li>
</ul>
</li>
<li>聚类 书上例题</li>
<li>看图求马尔科夫链的转移概率矩阵和平稳分布</li>
<li>奇异值分解存在性唯一性讨论，并给出分解过程</li>
<li>给了一个矩阵，对其进行主成分分析</li>
<li>决策树中的信息增益g(D,A)的用处
<ul>
<li>剪枝的意义</li>
</ul>
</li>
<li>课程建议</li>
</ul>
<h3 id="21-22-春夏" class="headerLink">
    <a href="#21-22-%e6%98%a5%e5%a4%8f" class="header-mark"></a>21-22 春夏</h3><ul>
<li>简述决策树的一种特征选择准则的定义，说明准则对决策树的影响（大概是这个意思，考信息增益和基尼指数的定义）</li>
<li>kd树构造
<ul>
<li>k近邻模型三要素是什么，k值选择需要注意什么（过拟合和误差）</li>
<li>给定样本数据集，构造kd树</li>
<li>按照构造的kd树求出实例点 （2，4.5）的最近邻</li>
</ul>
</li>
<li>支持向量机：给定线性不可分支持向量机的学习问题
<ul>
<li>软间隔SVM含义</li>
<li>写出对偶形式</li>
<li>求支持向量（好像是，当时只复习了硬间隔，软间隔就摆了）</li>
</ul>
</li>
<li>聚类问题：给定5个样本集合X，选定两个中心点，用k均值聚类算法 将X分成两类 （参考教材例题14.2）</li>
<li>马尔可夫链 和 蒙特卡洛法
<ul>
<li>大致说明 E[f(x)] (概率分布函数为p(x) ) 的计算方法 （大数定理近似样本均值）</li>
<li>给出一条马尔科夫链，求平稳分布（考的书上例题19.7）</li>
</ul>
</li>
<li>主成分分析
<ul>
<li>给定一个m维度的随机变量，求出k个主成分（1&lt;= k &lt;=m)，并且证明</li>
</ul>
</li>
<li>简述感知机，Adaboost，朴素贝叶斯法，logistic模型的学习策略和算法</li>
<li>奇异值分解：矩阵数据忘了 给定一个2*3矩阵A，求A的奇异值分解和紧奇异值分解，并且说明奇异值分解的几何意义</li>
</ul>
<h2 id="论文精读" class="headerLink">
    <a href="#%e8%ae%ba%e6%96%87%e7%b2%be%e8%af%bb" class="header-mark"></a>论文精读</h2><blockquote>
<p>Distance metric learning for large margin nearest neighbor classification.pdf</p>
</blockquote>
<ul>
<li>大边距近邻分类的距离度量学习</li>
</ul>
<h2 id="前辈经验" class="headerLink">
    <a href="#%e5%89%8d%e8%be%88%e7%bb%8f%e9%aa%8c" class="header-mark"></a>前辈经验</h2><blockquote>
<p>期末考比较中规中矩，无小测。</p>
<p>上课就是讲《统计学习方法》中的几章，作业做书的课后题，没有代码作业。 不过去年很多人提建议说要增加代码训练和作业量，今年可能会有所改变</p>
</blockquote>
<p>习题与代码参考：</p>
<ul>
<li>统计学习方法（第二版）习题解答 <a href="https://github.com/datawhalechina/statistical-learning-method-solutions-manual" target="_blank" rel="noopener noreferrer">https://github.com/datawhalechina/statistical-learning-method-solutions-manual</a></li>
<li><a href="https://blog.csdn.net/qq_42911960/article/details/115255714" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_42911960/article/details/115255714</a></li>
<li><a href="https://blog.csdn.net/qq_41562704/article/details/106540274" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/qq_41562704/article/details/106540274</a></li>
<li><a href="https://blog.csdn.net/wang_xinyu/article/details/111497444" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/wang_xinyu/article/details/111497444</a></li>
<li><a href="https://blog.csdn.net/breeze_blows/article/details/85469944" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/breeze_blows/article/details/85469944</a></li>
</ul>
<p>回忆卷：</p>
<ul>
<li><a href="https://www.cc98.org/topic/5356728" target="_blank" rel="noopener noreferrer">https://www.cc98.org/topic/5356728</a></li>
<li><a href="https://www.cc98.org/topic/5116266" target="_blank" rel="noopener noreferrer">https://www.cc98.org/topic/5116266</a></li>
</ul>
<h2 id="参考书目" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83%e4%b9%a6%e7%9b%ae" class="header-mark"></a>参考书目</h2><ul>
<li>《机器学习》，周志华，清华大学出版社，2016.</li>
<li>《The Elements of Statistical Learning》2nd edition, Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springer 2008.</li>
<li>《Pattern Recognition and Machine Learning》, Chris Bishop,  Springer 2006.</li>
<li>《Learning Theory：An Approximation Theory Viewpoint》, Felipe
Cucker and Ding-Xuan Zhou, Cambridge Univesity Press, 2007.</li>
</ul>]]></description>
</item><item>
    <title>量子信息与量子计算</title>
    <link>https://blog.ralvines.top/quan/</link>
    <pubDate>Thu, 01 Sep 2022 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/quan/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2023-2024 秋冬<br>
🧑‍🏫 毕惟红<br>
📝 50%读书报告，10%考勤，10%编程，30%两次展示</div>
        </div>
    </div>
<p>!!! quote &quot;&quot;
- 🎓 数学科学学院
- 🕙 2022-2023 秋冬
- 🧑‍🏫 武俊德
- 📝 签到，作业，课程论文</p>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><ul>
<li>讲义</li>
</ul>
<h2 id="课程论文" class="headerLink">
    <a href="#%e8%af%be%e7%a8%8b%e8%ae%ba%e6%96%87" class="header-mark"></a>课程论文</h2><p>!!! abstract &ldquo;Decomposition of arbitrary unitary transformations in quantum computing&rdquo;</p>
<pre><code>### Abstract

In order to achieve universal quantum computing in the way of classical computers, this paper focuses on the method of decomposing any Unitary transformation under n qubits into the most basic Hadamard gate, phase shift gate and CNOT gate, so that based on the physical construction of these three gates, most computing situations can be generally realized. We first demonstrate the decomposability of single Qubit gates based on Hadamard gates and phase shift gates, and consider the fact that dual Qubits gates can be constructed by CNOT gates, further verify that the Unitary transform under arbitrary n qubits can be decomposed into single Qubit gates and double Qubits gates, which provides a basis for discussing the physical implementation of quantum computers.

### Problem narrative

#### Background

We already know that in classical computing, all logical operations can be completed with only three operations, or and not, so as long as the physical construction of these three logic gates is realized, it lays the foundation for the implementation of general-purpose computers. Similarly, in quantum computing, for arbitrary quantum operations under n qubits, they are all Unitary variations due to the requirement of normalization, so the discussion of their universality can be equivalent to whether arbitrary unitary operations can be achieved by a combination of several elementary gates or approximated with arbitrary precision.

#### Goal

It is not an easy task to approximate the general unitary operation or find a set of quantum gates from the above propositions, so this paper is based on the electron spin quantum computing system with electron z-direction spin as the main measurement parameter, and takes the Hadamard gate, phase shift gate and CNOT gate as three basic gates, focusing on verifying the decomposition of any Unitary transform to the above three gates, so as to provide a basis for further discussion of general quantum computing.

### Gate decomposition of single qubit

#### Operational equivalence

We already know that Qubit's Unitary variation can be expressed in terms of a Unitary matrix. 

According to the knowledge of abstract algebra and complex functions, due to

$$U=e^{i\phi}SO_3,$$

i.e. $U\sim SO_3.$

That is, we can use one $SO_3$ matrix to describe it, and then any operation in Qubit is essentially an isometric transformation, that is, the rotation of the Bloch ball.

#### Hadamard gate

Based on the preceding hypothesis, we can define the basis vector of a quantum computing system as

$$|0\rangle_z=(1,0)^T,|1\rangle_z=(0,1)^T.$$

Then define the Hadamard gate as

$$H=\displaystyle\frac{1}{\sqrt{2}}
\left[\begin{array}{cc}
1&amp;1\\
1&amp;-1\\
\end{array}\right]$$

Because of $H^2=I,$ the Hadamard gate is unitary.

Since the given quantum computing system works with the spin of the electron as a parameter, it can be seen that the base vector of the spin is the eigenstate of the electron spin in all directions, and after passing through the Hadamard gate, the resulting transformation is

$$|0\rangle_x\rightarrow|0\rangle_z,|1\rangle_x\rightarrow |1\rangle_z,$$

$$|0\rangle_y\rightarrow-|1\rangle_y,|1\rangle_y\rightarrow -|0\rangle_y,$$

$$|0\rangle_z\rightarrow|0\rangle_x,|1\rangle_z\rightarrow |1\rangle_x.$$

Thus, the Hadamard gate is essentially a turn of the Bloch ball around $(\displaystyle\frac{1}{\sqrt{2}},0,\displaystyle\frac{1}{\sqrt{2}})$ where $\delta=\pi$.

#### Phase shift gate

Define the Phase shift gate as 

$$R_z(\delta)=
\left[\begin{array}{cc}
1&amp;0\\
0&amp;e^{i\delta}\\
\end{array}\right]$$

It is essentially a rotation of the Bloch ball around the z-axis at a counterclockwise angle of $\delta$.

#### Geometric validation of decomposition

When we use the Hadamard gate, we can change the rotation on the z-axis provided by the Phase shift gate to the rotation on the x-axis; Similarly, the y-axis can be moved to the x-axis position with a Phase shift gate, and then the Hadamard gate and the Phase shift gate can be used to obtain the rotation of the y-axis.

Next, by following the inference process of the Euler angle, a single point on the Bloch sphere can be moved to any position on the sphere, and the rigid body transformation $SO_3$ of the entire Bloch sphere can be further described. It is proved that only the Hadamard gate and the phase shift gate can complete any operation on a single Qubit, that is, a single Qubit gate can be successfully constructed.

### Gate decomposition of n qubits

Since dual Qubits gates are controlled single qubits that can be constructed from CNOT gates, it is only necessary to verify that any Unitary transformation under n qubits can be decomposed into a combination of single Qubit gates and double Qubits gates.

#### Construct a single controlled $C-U$ gate

The core is that it has a control qubit, when controlling position 1, do the Unitary transformation $U$ for the operation bit, otherwise do not do the transformation.

For a second-order unitary matrix acting on a single qubit

$$U=\left[\begin{array}{cc}
e^{i(\delta-\frac{\alpha}{2}-\frac{\beta}{2})}cos\frac{\theta}{2}&amp;-e^{i(\delta-\frac{\alpha}{2}+\frac{\beta}{2})}sin\frac{\theta}{2}\\
e^{i(\delta+\frac{\alpha}{2}-\frac{\beta}{2})}sin\frac{\theta}{2}&amp;e^{i(\delta+\frac{\alpha}{2}+\frac{\beta}{2})}cos\frac{\theta}{2}\\
\end{array}\right]$$

by decomposition, we get

$$U=\left[\begin{array}{cc}
e^{i\delta}&amp;0\\
0&amp;e^{i\delta}\\
\end{array}\right]
\left[\begin{array}{cc}
e^{-i\frac{\alpha}{2}}&amp;0\\
0&amp;e^{i\frac{\alpha}{2}}\\
\end{array}\right]
\left[\begin{array}{cc}
cos\frac{\theta}{2}&amp;-sin\frac{\theta}{2}\\
sin\frac{\theta}{2}&amp;cos\frac{\theta}{2}\\
\end{array}\right]
\left[\begin{array}{cc}
e^{-i\frac{\beta}{2}}&amp;0\\
0&amp;e^{i\frac{\beta}{2}}\\
\end{array}\right]
$$

Note

$$\Phi(\delta)=\left[\begin{array}{cc}
e^{i\delta}&amp;0\\
0&amp;e^{i\delta}\\
\end{array}\right],
R_z(\delta)=\left[\begin{array}{cc}
e^{-i\frac{\delta}{2}}&amp;0\\
0&amp;e^{i\frac{\delta}{2}}\\
\end{array}\right],
R_y(\delta)=\left[\begin{array}{cc}
cos\frac{\delta}{2}&amp;-sin\frac{\delta}{2}\\
sin\frac{\delta}{2}&amp;cos\frac{\delta}{2}\\
\end{array}\right].$$

and

$$A=R_z(\alpha)R_y(\frac{\theta}{2}),B=R_y(-\frac{\theta}{2})R_z(-\frac{\alpha+\beta}{2}),C=R_z(\frac{\beta-\alpha}{2}).$$

Then we know

$$U=\Phi(\delta)A\sigma_x B\sigma_x C, I=ABC.$$

\begin{figure}[h] 
\centering
\includegraphics[width=11cm]{fig1.png}
\caption{Circuit diagram of $C-U$ gate} \label{fig1}
\end{figure}

From this, the $C-U$ gate can be constructed.

#### Construct a multi-controlled $C^k-U$ gate

Given $k\in \mathbb{N},k\ge2.$ Back to classical computers, we can construct Toffoli gates and use them to construct k-weight AND gates, similarly, here we need to construct a generality $C^2-U$ gate, and then we can further construct a multi-controlled $C^k-U$ gate to verify its feasibility.

From algebraic knowledge, for any second-order unitary matrix $U$, after Jordan diagonalization and square, we can get the decomposition 

$$U=V^2.$$

Further, there is

$$VV^{\dag}=I.$$

\begin{figure}[h] 
\centering
\includegraphics[width=11cm]{fig2.png}
\caption{Circuit diagram of $C^2-U$ gate} \label{fig2}
\end{figure}

So we get the $C^2-U$ gate that meets the requirements.

#### General decomposition

In order to extend the conclusion to any unitary transformation in n qubits, we need to verify that it can be implemented into two quantum states, that is, we only need to proof that

$$U^{(2^n)}=\prod\limits_{i=1}^{2^n-1}\prod\limits_{j=0}^{i-1} V_{ij}.$$

Since direct processing is difficult, we follow the process of proving odd and even arrangement in mathematics, swap quantum states one by one, and design the transformation gate shown in the figure, so that the exchange process is until the two quantum states to be transformed through the gate and then return to the original position in turn.

\begin{figure}[h] 
\centering
\includegraphics[width=11cm]{fig3.png}
\caption{Circuit diagram of swap gate} \label{fig3}
\end{figure}

Thus, we achieve the feasibility of the above operation, that is, the decomposition in the general case is also achievable.

### Further discussion of universal quantum computing implementation

The proposition studied in this paper ultimately serves the universality of quantum computing. In fact, when we construct the $C^k-U$ gate, it has been proved that quantum computing can achieve universal classical computing, and the conclusion extension of general decomposition can be better applied to quantum simulation and special quantum computing, so as to meet the completeness of the conclusion in the case of quantum operations under n qubits.

The interesting thing about the above work is that we can feel the inheritance of quantum computing to many theoretical ideas in classical computing. In the course of the author's study this semester, the course has been deeply involved in classical computing, so when it is introduced into the theory of quantum computing, many works have extensive similarities in the core, compared to the calculation itself, how to better prepare quantum states and better use the physical achievements of quantum mechanics is a more profound proposition left to us by this discipline.
</code></pre>]]></description>
</item><item>
    <title>数据科学的数学基础</title>
    <link>https://blog.ralvines.top/ds/</link>
    <pubDate>Thu, 01 Sep 2022 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/ds/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2022-2023 秋冬<br>
🧑‍🏫 赖俊<br>
📝 20%作业，20%上机，60%考试</div>
        </div>
    </div>
<h2 id="参考资料" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99" class="header-mark"></a>参考资料</h2><ul>
<li>《Mathematical Foundations for Data Analysis》Jeff M. Phillips</li>
</ul>
<h2 id="ch1概率论回顾" class="headerLink">
    <a href="#ch1%e6%a6%82%e7%8e%87%e8%ae%ba%e5%9b%9e%e9%a1%be" class="header-mark"></a>Ch1.概率论回顾</h2><ul>
<li>样本空间</li>
<li>条件概率和独立性</li>
<li>分布函数</li>
<li>期望和方差</li>
<li>联合边际概率和条件概率</li>
<li>贝叶斯法则</li>
<li>极大似然估计和贝叶斯推断</li>
</ul>
<h2 id="ch2-收敛和采样" class="headerLink">
    <a href="#ch2-%e6%94%b6%e6%95%9b%e5%92%8c%e9%87%87%e6%a0%b7" class="header-mark"></a>Ch2. 收敛和采样</h2><ul>
<li>采样和估计</li>
<li>概率近似正确</li>
<li>Markov, Chebyshev, Chernoff-Hoeffding 不等式</li>
<li>Union Bound</li>
<li>重采样</li>
</ul>
<h2 id="ch4-距离" class="headerLink">
    <a href="#ch4-%e8%b7%9d%e7%a6%bb" class="header-mark"></a>Ch4. 距离</h2><ul>
<li>度量</li>
<li>$L_p$ 距离</li>
<li>M 距离, Cosine 距离, Angular 距离和 KL 散度</li>
<li>集合的 Jaccard 距离</li>
<li>相似度及其衍生的距离</li>
</ul>
<h2 id="ch5-线性回归" class="headerLink">
    <a href="#ch5-%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92" class="header-mark"></a>Ch5. 线性回归</h2><ul>
<li>简单线性回归</li>
<li>多解释变量</li>
<li>多项式回归</li>
<li>交叉验证</li>
<li>Tikhonov 正则化</li>
<li>Lasso 算法和岭回归</li>
<li>匹配追踪</li>
</ul>
<h2 id="ch6-梯度下降" class="headerLink">
    <a href="#ch6-%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d" class="header-mark"></a>Ch6. 梯度下降</h2><ul>
<li>凸函数</li>
<li>梯度下降法, 线搜索和 backtracking</li>
<li>学习率</li>
<li>随机梯度下降法</li>
</ul>
<h2 id="ch7-降维" class="headerLink">
    <a href="#ch7-%e9%99%8d%e7%bb%b4" class="header-mark"></a>Ch7. 降维</h2><ul>
<li>数据矩阵</li>
<li>SSE</li>
<li>投影和范数意义下的k阶最佳逼近</li>
<li>主成分分析</li>
<li>MDS</li>
</ul>
<h2 id="ch8-聚类" class="headerLink">
    <a href="#ch8-%e8%81%9a%e7%b1%bb" class="header-mark"></a>Ch8. 聚类</h2><ul>
<li>维诺图</li>
<li>Delaunay 三角剖分</li>
<li>k-中心/均值/Medium/Mediod 聚类</li>
<li>冈萨雷斯算法</li>
<li>Lloyd 算法</li>
<li>软聚类</li>
</ul>]]></description>
</item><item>
    <title>当代文学前沿问题研究</title>
    <link>https://blog.ralvines.top/wxqy/</link>
    <pubDate>Tue, 30 Mar 2021 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about/praise/</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/wxqy/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piAW5eH.png" referrerpolicy="no-referrer">
            </div><p><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2023-2024 秋冬<br>
🧑‍🏫 毕惟红<br>
📝 50%读书报告，10%考勤，10%编程，30%两次展示</div>
        </div>
    </div> true</p>
<p>!!! quote &quot;&quot;
- 🎓 数学科学学院
- 🕙 2020-2021 春夏
- 🧑‍🏫 吴秀明，陈力君
- 📝 签到，讨论课发言，展示，期末考试</p>
<h2 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h2><h2 id="大纲" class="headerLink">
    <a href="#%e5%a4%a7%e7%ba%b2" class="header-mark"></a>大纲</h2><p>!!! note &quot;&quot;
- <strong>CH1.</strong> 导论及背景介绍
- <strong>CH2.</strong> 《青春之歌》解读
- <strong>CH3.</strong> 金庸武侠小说及其定位
- <strong>CH4.</strong> 金庸武侠小说思想艺术特点
- <strong>CH5.</strong> 影片《新龙门客栈》
- <strong>CH6.</strong> “后金庸”时代的武侠小说
- <strong>CH7.</strong> 诺贝尔文学奖与中国当代文学
- <strong>CH8.</strong> 纪实文学热与领袖传记文学
- <strong>CH9.</strong> “80后”文学创作（一）
- <strong>CH10.</strong> “80后”文学创作（二）</p>
<h2 id="学习笔记" class="headerLink">
    <a href="#%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0" class="header-mark"></a>学习笔记</h2><h3 id="名词解释" class="headerLink">
    <a href="#%e5%90%8d%e8%af%8d%e8%a7%a3%e9%87%8a" class="header-mark"></a>名词解释</h3><ul>
<li>
<p>**三红一创，青山保林：**指的是对新中国十七年小说的坚持政治艺术统一的高度概括，也是十七年红色经典小说的高度集中。毛泽东文艺思想《在延安文艺座谈会上的讲话》在1949年后逐渐成为指导全国文学艺术工作的唯一正确的文艺思想。这时期的文艺路线是：文学为政治服务，文学为工农兵服务。文学别无选择的充当了生活教科书的任务。</p>
</li>
<li>
<p>**范式：**范式从本质上讲是一种理论体系、理论框架。在该体系框架之内的该范式的理论、法则、定律都被人们普遍接受。</p>
</li>
<li>
<p>**撰论：**金庸社论？</p>
</li>
<li>
<p>**超验：**超出一切可能的经验之上，超越时间、空间等存在形式，不能用因果、属性、存在、不存在等范畴进行思考的东西。强调直觉，超越感觉理性。</p>
</li>
<li>
<p>**暴力美学：**暴力美学主要是在感官上，使暴力以美学的方式呈现，诗意的画面，甚至幻想中的镜头来表现人性暴力面和暴力行为。观赏者本身往往惊叹于艺术化的表现形式，无法对内容产生具体的不舒适感。</p>
</li>
<li>
<p>伟人生活化模式、政治人格化模式、社会心理化模式。</p>
</li>
<li>
<p>**社会学意义的80后：**家庭结构、伦理观念、经济结构、人与物关系、当代中国历史和技术环境的变化。</p>
</li>
<li>
<p><strong>80后作家：</strong>(2010年前80后作家群概念扩展而来) 指1980年后出生的一些作家写手，互联网写作环境对他们影响深刻，创作初期主要以同龄人为阅读对象。萌芽作家和非萌芽作家。体制外写作和第二渠道发行。娱乐原则和交换原则下的大众传媒的扩张。</p>
</li>
<li>
<p>**第二渠道：**二渠道是指除主渠道(传统国营书店如新华书店等)以外的其他发行渠道，主要是民营的图书批发和零售通道。还有“特殊渠道”指网上书店、系统发行等新兴发行渠道。</p>
</li>
<li>
<p>**唯理性教学模式：**将充满人性之美和生活趣味的语文变成机械枯燥的应试训练。“唯理性教学模式”纵横贯穿于语文教学领域。这种模式崇尚抽象、概括、提炼、崇尚逻辑思维能力，却忽略情感、意志和审美情趣的介入；重视将一切语文知识加以解构和量化，却忽略了从文本和人本的整体角度高屋建瓴地培养学生的语文能力；重视语文学科的互补性。</p>
</li>
<li>
<p>**50后到90后：**建构大历史，解构，生活，虚拟，无视社会-自我，群体-小我，大众-私我，世界-亲我。</p>
</li>
<li>
<p>**80后风格：**现代生活，速度生活，都市风情、酒吧咖啡商场职场等文化、国际化视角、叛逆与自我立场。</p>
</li>
<li>
<p>**耽美文学：**耽美一词最早是出现在日本近代文学中，为反对“自然主义”文学而呈现的另一种文学写作风格。有“耽美派”，它的最初本意是反对以暴露人性的丑恶面为主的自然主义，并想找出官能美、陶醉其中追求文学的意义。引申为代指一切美形的男性，以及男性与男性之间不涉及繁殖的恋爱感情，最后更发展为同性恋漫画的代称之一。</p>
</li>
<li>
<p>**同人小说：**指的是利用原有的漫画、动画、小说、影视作品中的人物角色、故事情节或背景设定等元素进行的二次创作小说。同人小说一般是以网络小说为载体，近年来，伴随体育人物、娱乐人物、政治人物等社会人物的高密集度曝光，同人小说中的真人同人小说也逐渐兴起。</p>
</li>
<li>
<p>**韩寒：**创作原则现实主义、文学反映人生的传统文学观念、讽喻，批判、叛逆、个性化表达，社会游荡者流浪者傻傻的同行者纯情女友人物形象，松散离奇骑士情结滑稽人生经历，反讽夸大双关荒诞。</p>
</li>
<li>
<p>**80后电影：**青春成长题材，情感与成长阵痛，独立电影影像表达，梦幻暴力场面，较强的影响语言把握运用能力。</p>
</li>
<li>
<p>**生态文学：**关于生态问题的文学，具有生态维度的文学，反思和批判生态思想和观念表达，基于当下和现实生态状况的文学作品，具有生态标准或尺度的文学创作和批评。揭示问题，试图摆脱危机。</p>
</li>
<li>
<p>**生态危机：**整体而非局部的生态系统出现问题，被破坏。人类文明历史是一部人类与自然的关系史</p>
</li>
<li>
<p>**人类中心主义：**又译“人类中心论”，是以人类为事物的中心的学说。同其他文化观念一样，人类中心主义的观念也具有历史发展的连续性和间断性，这一概念曾在三个意义上使用人类中心主义总是作为一种价值和价值尺度而被采用的，把人类的利益作为价值原点和道德评价的依据，有且只有人类才是价值判断的主体。</p>
</li>
<li>
<p>**土地伦理：**土地伦理是环境伦理的视角之一，是由奥尔多·利奥波德在他的《沙乡年鉴》一书中首次倡导的。其中他写道，需要一种“新的伦理”，“一种处理人与土地，以及人与在土地上生长的动物和植物之间的伦理观”。在他那个时代，美国林业局的主流观念从创立者吉福德·平肖开始，就是追求经济利益的和功利主义的，而利奥波德则主张一种“生态学”的态度(这个词由芝加哥大学的亨利·钱德勒·考尔斯于20世纪初在对印地安那沙丘的研究中提出，利奥波德是该术语最早的推广者之一)。资源保护主义在更偏向人类中心主义的资源管理范式中获得了它的首要地位，而与此同时，利奥波德的著作和启发与约翰·缪尔一起引发了环境主义的发展。</p>
</li>
<li>
<p>**生态文学价值取向：**否定强权、人类中心主义重构人和自然关系，同情关怀弱势种群探寻非人类种群精神世界，坚持开放评判姿态倡导土地伦理以孩童或他者视角，强调人类责任探寻生态危机原因预测世界未来。</p>
</li>
<li>
<p>**景别：**景别是指由于在焦距一定时，摄影机与被摄体的距离不同，而造成被摄体在摄影机录像器中所呈现出的范围大小的区别。景别的划分，一般可分为五种，由近至远分别为特写(指人体肩部以上)、近景(指人体胸部以上)、中景(指人体膝部以上)、全景(人体的全部和周围部分环境)、远景(被摄体所处环境)。</p>
</li>
<li>
<p>**镜头语言：**镜头语言就是用镜头像语言一样去表达我们的意思，我们通常可经由摄影机所拍摄出来的画面看出拍摄者的意图，因为可从它拍摄的主题及画面的变化，去感受拍摄者透过镜头所要表达的内容。</p>
</li>
<li>
<p>**女性意识与南方文化：**江南经济发展带来新的时尚，经济发展带来坊刻文化繁荣，文字图画共存产生新欣赏口味满足情感需求</p>
</li>
<li>
<p>**女性作家群：**闺塾师，妇女诗社。</p>
</li>
<li>
<p>**女性文学规模形成：**情趣化和美感合理运用冲破科场文化限制，产生新文类新空间。至此成为被拘囿和局限了空间和观念的充满通融和争执的存在。</p>
</li>
<li>
<p>**传统女性文学：**零星存在的才女创作，文以载道成为中国文学核心价值，教育资源性别倾斜。特点：女性只能通过男性角色完成心愿才能展示，女性意识表现不能超越社会价值观念，女性意识只能委婉曲折地得以传达传统和新女性文学的审美表达、文类、视角区别：国事政事到家事情事，优美与壮美，亚文类诗-词，史传-小说。</p>
</li>
<li>
<p>**亚文类：**从属于&hellip;&hellip;</p>
</li>
<li>
<p>科场：科举场所·五四女性文学：批家庭家族文化反抗父权表达爱情个人反抗夫权，女教师学生形象，如萧红、丁玲、白薇。</p>
</li>
<li>
<p>**三四十年代女性文学：**彼时女性普遍生存现实的书写、生命意识和精神世界的探索，如丁玲、萧红、张爱玲</p>
</li>
</ul>
<h3 id="讨论课发言" class="headerLink">
    <a href="#%e8%ae%a8%e8%ae%ba%e8%af%be%e5%8f%91%e8%a8%80" class="header-mark"></a>讨论课发言</h3><h4 id="3月11日---时代与经典的关系" class="headerLink">
    <a href="#3%e6%9c%8811%e6%97%a5---%e6%97%b6%e4%bb%a3%e4%b8%8e%e7%bb%8f%e5%85%b8%e7%9a%84%e5%85%b3%e7%b3%bb" class="header-mark"></a>3月11日 - 时代与经典的关系</h4><p>我以为，作为裹挟于时代浪潮之中的个体，我们很难不受到一定时期社会经济基础、主流意识形态及生活方式的影响，也自然难以超脱于一定的历史桎梏。</p>
<p>譬如春秋战国时期松散的权力结构和相对适足的经济基础营造出一段思想多元的历史浪潮，诸子百家各自收揽大批追随者。而在秦代之后的君主专制中央集权时期，帝王及其朝廷所采取的治理模式很大程度上影响到社会的主流思潮，因而，生前郁郁不得志的孔子也难以预料到罢黜百家后的中原王朝将在千年的历史长河中将儒家经典奉为圭臬。</p>
<p>诗歌、书法形式的变革亦如是，而伴随着市井文化和较为稳定社会环境下的新兴商贸体系的形成、社会生活的日益丰富和各阶层群体利益的复杂化，明清文人又将主流视角从曾经强调主观情调与高洁理想的抒情范式转向了更多强调矛盾与故事性的长篇小说与戏曲。</p>
<p>而对于《青春之歌》来说，我认为它的现实意义在于其以爱情主线展现出时代变迁中的社会主流意识形态演化脉络及沉浮其中的小人物的精神蜕变，而前者恰恰属于对青春主题所作出的极富那个年代历史语境下的价值解构。</p>
<p>如前所述，随着时代的变革，传统社会形态和价值取向的消解，自然优先作用于文艺创作的土壤。建国前期，传统氏族结构的崩溃和产缘纽带的建构加速歧化社会群体生活理念之于以高级知识分子代表的精英与广泛的劳动大众的落差。而五四以降，特别是以左翼革命为主线的元叙事成为中国社会意识形态的中流砥柱之后，民国早期以引入、吸收、传播乃至照搬西方启蒙思想的文人群体与“农村包围城市”的无产阶级觉醒产生了精英与平民视角的对立。在新民主主义革命者在天安门城楼宣布胜利的历史帷幕之下，主流话语体系自然如毛泽东文艺思想所展现——强调“人民性”与“革命性”，对“高高在上”教化愚昧无知大众的旧知识分子群体赋予了批判和改造的内在使命，而这恰恰寓于彼时文学作品的内核之中。正如林道静爱情主线下的身份转变，从资产阶级知识分子到无产阶级革命者，从同余永泽相处时之于革命较为纯粹的浪漫主义和理想主义到最后彻底脱离旧体系束缚、成为更为独立坚毅的女性共产主义战士，极具彼时时代特征的宏大思想理念作为政治隐喻内化于情感叙事之中，对于当代的我们理解这一历史阶段具有生动的意义。</p>
<h4 id="3月18日---金庸小说的意义与影响" class="headerLink">
    <a href="#3%e6%9c%8818%e6%97%a5---%e9%87%91%e5%ba%b8%e5%b0%8f%e8%af%b4%e7%9a%84%e6%84%8f%e4%b9%89%e4%b8%8e%e5%bd%b1%e5%93%8d" class="header-mark"></a>3月18日 - 金庸小说的意义与影响</h4><p>作为00后，金庸的武侠作品在我们这一代人的青年时段中似乎正在悄然式微，相较文学本身，留给我们的更多的往往是电视上《射雕英雄传》《天龙八部》的浮光掠影。因此，当我们提到金庸武侠小说的影响时，必然绕不开它和它贯穿的整个文学时代给现在的我们留下了什么。
首先，金庸的武侠小说开启了此类文学作品的全新范式。在当今许多新兴网络文学作品中，那些英雄、玄幻色彩背后，往往蕴含着金庸时代武侠小说的叙事风格和情节架构。古代传统社会的背景下，具有现代主义特征的爱情、仁善元素包裹着其特有的畅快淋漓之词藻，将打斗场面描绘得无比生动。阅读时，我能感受到它与读者偌大想象空间的完美契合。
其次，金庸的武侠小说影响了一代人，特别是在世界观的形成层面。“侠之大者，为国为民。”相较于《水浒》等古代经典小说，金庸文学作品更突出救死扶伤、保家卫国、同情弱者的人文关怀，这与他创作时的时代背景是密不可分的。它不仅仅强调传统的忠义兄弟情，更展现了人性的复杂一面。即便是江湖英雄，也有脆弱的一面；如此仗义的侠客，也总有种种缺点和软肋。而善恶亦不是纯粹的二元对立，大量“圆形人物”的塑造往往透露出老庄辩证哲学的转化思想。正所谓“有人的地方就有江湖”，金庸作品似乎善于解构正义与邪恶对立背后的复杂利益因素，在人情世故中将丰富的感性成分融入于前者。</p>
<h4 id="5月27日---80后文学的时代风格" class="headerLink">
    <a href="#5%e6%9c%8827%e6%97%a5---80%e5%90%8e%e6%96%87%e5%ad%a6%e7%9a%84%e6%97%b6%e4%bb%a3%e9%a3%8e%e6%a0%bc" class="header-mark"></a>5月27日 - 80后文学的时代风格</h4><p>或许因为代际差异，我个人对80后作家的文学作品并不是很熟悉。就80后文学作品的风格、选材及作者审美诉求的变化，正如此前数周大家所呈现和讨论，“计划生育”、改革开放、外来文化冲击、互联网萌芽、去政治化等等要素建构了这一代人的集体意识形态。这一点也在伤痕文学中有所体现。</p>
<p>80后作家们的视角更趋向“小我”，在各种新鲜事物或文化的强烈冲击之下，80后有一种置于承前启后时代的独特孤独感，一方面受到社会传统领域的束缚牵制，另一方面到处孕育着多元的机会。我想这注定是彷徨、探索着的一代，具体来说，《上海公园》中同学聚会上大量对白的表达所呼应的悲伤、压抑的意象，正是这一代人冲动而迷茫的缩影。那些旧日同窗有的谈吐中时刻夹杂着外语，眼神里显现出无尽的却又有些幼稚的渴望与追求感；有的弹奏吉他，沉醉于自我的表达，也奋力于谋求出路……总而言之，是对宏大叙事的回避，是对小我情态的捕捉，贯穿其中、处处可见的则是都市、西方文化与懵懂青年的朦胧交错。</p>
<h4 id="6月10日---生态文学叙事" class="headerLink">
    <a href="#6%e6%9c%8810%e6%97%a5---%e7%94%9f%e6%80%81%e6%96%87%e5%ad%a6%e5%8f%99%e4%ba%8b" class="header-mark"></a>6月10日 - 生态文学叙事</h4><p>在我看来，冷峻而真实的场景建构、人性与生命脆弱性的呈现似乎是《可可西里》代表的这一类生态影片的叙事共性。在生命的禁区，善恶界限愈发模糊、道德伦理与法律秩序渐隐，刘栋和日泰的突然死亡深刻地揭示了人类在自然面前的渺小，而日泰不得不卖皮子与马占林复杂的人物形象体现着上述生存遇到威胁的情境下现实之于个体的矛盾。在某种意义上说，在当下人与自然的和谐共生本质上仍然是人类基于自身主观利益的诉求，当自然反噬人类的侵略行为或是人类惊觉自然资源的稀缺性时，其才展现出可悲的保护意识；人类的理性往往屈从于求生的基础本能，这是一种妥协与无奈，也振聋发聩地促使我们去反思，我们究竟需要追求一种怎样的同自然生态的交互反馈。</p>
<p>**课后随想：**本次讨论课的主题是生态文学，并比较分析了《狼图腾》和《怀念狼》两部作品。命题的核心如老师在周二课上提出，生态文学是“退步的文学”，是人类文学发展进程中的一次“回眸”。要理解生态文学，自然需要聚焦于其意识产生的基础，即人类发展中“进步的模式”同“文明的模式”之间存在的必然的矛盾及人改造、征服自然过程中受到反噬从而产生的问题。而从文学角度讲，生态文学所要反思的本质上是社会生态、精神生态的问题，探讨人类自身在相应认知上存在的问题。由此而言，生态文学无疑是对客观现实的一种进步反映，也是人类在工业化浪潮下环保意识、天人合一意识的觉醒。然而，这一时期的生态文学又有一种人类生活模式的保守化倾向，毕竟在工业化对自然显著的、触目惊心的破坏现实之中，文学创作者们又难以找到一条清晰的解决路径，从而容易陷入到“返璞归真”的思维陷阱之中。无论如何，发展的问题恐怕只能用发展解决，我认为人类应当顺应生产力发展的方向，通过科技的进步为人与自然和谐共生提供强力的正反馈。</p>
<h4 id="6月17日---文艺作品中的女性角色" class="headerLink">
    <a href="#6%e6%9c%8817%e6%97%a5---%e6%96%87%e8%89%ba%e4%bd%9c%e5%93%81%e4%b8%ad%e7%9a%84%e5%a5%b3%e6%80%a7%e8%a7%92%e8%89%b2" class="header-mark"></a>6月17日 - 文艺作品中的女性角色</h4><p>对第三小组的第二个问题，我倾向于认为这是一个伪命题。</p>
<p>当代对女性群体的关注与女性这一群体概念的强化本身是平权主义的分支，也就是消除传统桎梏尚未完全解体的性别歧视与刻板印象。然而，刚才当我们谈论这些性别标签下的群体特质时，却恰恰是在潜移默化地在制造刻板印象。</p>
<p>但是它为什么存在？课堂上有同学提到的铁娘子撒切尔夫人，这一套“强硬”形象难道就是男性固有的特质吗？我更相信这是一种文化惯性的结果，是传统男性主导地位导致的。比如过去的领导人绝大部分都是男性，既有文质彬彬儒雅随和者、也有政治强人、强硬做派者，凡此种种，其实本质上是群体的历史记忆让在过去占有主导地位的男性垄断了绝大部分特质的标签。</p>
<p>而对于所谓的性别视角下作家的思维特质，我认为这其实属于个体化的范畴，而非由性别因素所决定。譬如同为男性作家，木心和刘慈欣拥有着迥然不同的创作风格与叙事模式。这些实质上是作者在职业、文化氛围、生活环境、宗教、思维模式乃至意识形态上的差别，而思维模式等要素本身很少受到性别本身的影响。
值得一提的是，从生物学上说，男女之间的基因演化最终是趋同的，男性与女性不同基因对其认知与行为模式差异的影响远远小于外在环境等因素。</p>
<p>个人认为，当代女权主义中的反抗精神仿佛是一种“弱势者的群体皈依，抱团取暖”的表现，这自然有一定的道理，然而也不能忽视事实上掌握话语权的核心并非性别。如当代西方普遍的种族问题，往往掩盖了根本的阶级矛盾；又比如80后的标签，是这一代人在改革开放浪潮、独生子女政策等集体记忆和社会背景下交互下的产物，而不能简单归咎于在80年代出生这一基本定义。</p>
<p>至于性别话语权的垄断确实存在，但在当代更多地是表现在刻板印象之中，比如所谓的男性阳刚女性阴柔，以及女性生理特质带来的部分劣势之中，比如由于生育、产假、体力问题导致的女性在职场中的弱势地位，不过不要忽视此种差异是双向的，正如随着社会发展，尽管简单的体力工作男性占据了主体优势，却也有一些专属于女性的职业存在。</p>
<p>课上谈论女性悲剧形象时，就有不少同学指出去除性别标签后，悲剧的形象仍然有着共性。而女性文学中普遍提到的封建社会下如翠翠独守闺房与祥林嫂式的悲剧，前者是女性在生活上的依附现实，后者则是生产上的性别依附。</p>
<p>总体来说，此种话语权的不匹配在现代社会下是在不断消解的。我比较认同平权问题用辩证唯物主义那套理论框架来阐释，当代妇女解放与女性意识的觉醒本身就是在后工业化时代，社会生产关系中的性别差异被机器的运用所逐渐抹平，女性大规模参与到现代生产活动之中的结果。</p>
<p>男女的差异事实上主要仍然体现在基因等生理要素之上，当代很多性别不平等现象相较封建时代已经得到了很大的进步，相反，很多问题是文化惯性、历史遗留，这在生产力落后的农村和欠发达地区仍有呈现。我们知道即便是在原始社会也存在着母系社会，所谓的父权社会很大程度上是战争频繁的农耕时代男性由于生理上具有的体力优势的社会经济基础所衍生出的一套伦理体系。而这些差异已经被证明是能够随着科技进步而式微的，因此，对于“女性的悲剧结局能否避免”这一论题，我相信这将随着时代的发展获得肯定的答案。</p>]]></description>
</item></channel>
</rss>
