<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>温铁军 - 标签 - 暮瞻</title>
        <link>https://blog.ralvines.top/tags/%E6%B8%A9%E9%93%81%E5%86%9B/</link>
        <description>温铁军 - 标签 - 暮瞻</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>ralvine@163.com (Ralvine)</managingEditor>
            <webMaster>ralvine@163.com (Ralvine)</webMaster><lastBuildDate>Mon, 30 Oct 2023 20:20:40 &#43;0800</lastBuildDate><atom:link href="https://blog.ralvines.top/tags/%E6%B8%A9%E9%93%81%E5%86%9B/" rel="self" type="application/rss+xml" /><item>
    <title>数学前沿专题讨论：遗传算法和统计学习方法</title>
    <link>https://blog.ralvines.top/qianyan/</link>
    <pubDate>Mon, 30 Oct 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about.md</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/qianyan/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/11/01/pinHqnH.png" referrerpolicy="no-referrer">
            </div><div class="details admonition quote open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-quote-right fa-fw"></i>课程信息<i class="details-icon fas fa-angle-right fa-fw"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content">🎓 数学科学学院<br>
🕙 2023-2024 秋冬<br>
🧑‍🏫 毕惟红<br>
📝 50%读书报告，10%考勤，10%编程，30%两次展示</div>
        </div>
    </div>
<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark"></a>背景</h2><ol>
<li>浙大 计算数学</li>
<li>研究生</li>
</ol>
<ul>
<li>数字模拟 投影法求曲面面积</li>
<li>随机数生成 数值代数 多元非线性方程组求解</li>
</ul>
<p><strong>研究方向</strong></p>
<ol>
<li>图像处理</li>
</ol>
<ul>
<li>图像分割 图像识别</li>
<li>图像加密 做的比较好</li>
</ul>
<ol start="2">
<li>语义识别</li>
</ol>
<ul>
<li>三维点式数据（无人驾驶、激光雷达）</li>
</ul>
<ol start="3">
<li>社区发现</li>
</ol>
<ul>
<li>复杂网络</li>
<li>拟牛顿</li>
</ul>
<h2 id="遗传算法" class="headerLink">
    <a href="#%e9%81%97%e4%bc%a0%e7%ae%97%e6%b3%95" class="header-mark"></a>遗传算法</h2><h2 id="统计学习方法" class="headerLink">
    <a href="#%e7%bb%9f%e8%ae%a1%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95" class="header-mark"></a>统计学习方法</h2><h2 id="首次展示knn" class="headerLink">
    <a href="#%e9%a6%96%e6%ac%a1%e5%b1%95%e7%a4%baknn" class="header-mark"></a>首次展示：kNN</h2><h3 id="分类问题1" class="headerLink">
    <a href="#%e5%88%86%e7%b1%bb%e9%97%ae%e9%a2%981" class="header-mark"></a>分类问题<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h3><ul>
<li>
<p>一种监督学习问题，旨在对数据分类</p>
</li>
<li>
<p>将输入数据映射到预定义的类别或标签</p>
</li>
<li>
<p>从已知的训练数据中学习一个分类模型，然后将该模型应用于新的、未知的数据，以预测其所属的类别</p>
</li>
<li>
<p>垃圾邮件过滤、金融风险评估</p>
</li>
<li>
<p>医学诊断、生物信息学</p>
</li>
<li>
<p>情感分析、客户分类</p>
</li>
<li>
<p>图像识别</p>
</li>
</ul>
<h3 id="knn模型构建" class="headerLink">
    <a href="#knn%e6%a8%a1%e5%9e%8b%e6%9e%84%e5%bb%ba" class="header-mark"></a>kNN模型构建</h3><h4 id="提出2" class="headerLink">
    <a href="#%e6%8f%90%e5%87%ba2" class="header-mark"></a>提出<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></h4><p>$$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$$
$$y_i\in\mathcal{Y}={c_1,c_2,\cdots,c_K}$$
$$y=\text{arg}\max\limits_{c_j} \sum\limits_{x_i\in N_k(x)} I(y_i=c_j)$$</p>
<ul>
<li>
<p>输入：特征向量（空间点）</p>
</li>
<li>
<p>输出：类别（可以取多类）</p>
</li>
<li>
<p>已标注的训练集</p>
</li>
<li>
<p>预测：多数表决（“近朱者赤” ）</p>
</li>
<li>
<p>不具有显式的学习过程</p>
</li>
</ul>
<p><strong>适用范围</strong></p>
<ul>
<li>数值型和标称型<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></li>
</ul>
<p><strong>优点</strong><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<ol>
<li>直观、非参数化</li>
<li>对异常值不敏感</li>
<li>支持多类别</li>
</ol>
<p><strong>缺点</strong><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<ol>
<li>时间复杂度高</li>
<li>存储成本高</li>
<li>“维度灾难”和数据不平衡</li>
</ol>
<h4 id="构建流程" class="headerLink">
    <a href="#%e6%9e%84%e5%bb%ba%e6%b5%81%e7%a8%8b" class="header-mark"></a>构建流程</h4><p>给定距离度量，k值与决策规则 [输入训练集T]</p>
<ol>
<li>在训练集 T 中找出与 x 最邻近的 k 个点，涵盖这 个点的 x 的邻域记作 $N_k(a)$</li>
<li>在 $N_k(a)$ 中根据分类决策规则决定 x 的类别 y</li>
</ol>
<p><strong>基本要素</strong></p>
<ol>
<li>k 值选择</li>
<li>距离度量</li>
<li>决策规则</li>
</ol>
<p>特殊情况：最近邻（k=1）</p>
<h3 id="模型要素" class="headerLink">
    <a href="#%e6%a8%a1%e5%9e%8b%e8%a6%81%e7%b4%a0" class="header-mark"></a>模型要素</h3><h4 id="k-值选择" class="headerLink">
    <a href="#k-%e5%80%bc%e9%80%89%e6%8b%a9" class="header-mark"></a>k 值选择</h4><table>
<thead>
<tr>
<th>k值</th>
<th>偏小</th>
<th>偏大</th>
</tr>
</thead>
<tbody>
<tr>
<td>近似误差</td>
<td>减小</td>
<td>增大</td>
</tr>
<tr>
<td>估计误差</td>
<td>增大</td>
<td>减小</td>
</tr>
</tbody>
</table>
<p>交叉验证以提高泛化性能。<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></p>
<h4 id="距离度量9" class="headerLink">
    <a href="#%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8f9" class="header-mark"></a>距离度量<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></h4><p>对于
$$x_i=(x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})$$</p>
<ul>
<li>$L_p$ 距离
$$L_p(x_i,x_j)=(\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{1/p}$$</li>
<li>欧氏距离
$$L_2(x_i,x_j)=(\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^2)^{1/2}$$</li>
<li>曼哈顿距离
$$L_1(x_i,x_j)=\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|$$</li>
<li>$L_\infty$ 距离
$$L_\infty (x_i,x_j)=(\max\limits_l |x_i^{(l)}-x_j^{(l)}|$$</li>
</ul>
<h4 id="决策规则" class="headerLink">
    <a href="#%e5%86%b3%e7%ad%96%e8%a7%84%e5%88%99" class="header-mark"></a>决策规则</h4><p><strong>多数表决</strong>
由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。</p>
<ul>
<li>分类函数
$$f:\mathbb{R}^n\rightarrow {c_1,c_2,\cdots,c_K}$$</li>
<li>误分类概率
$$P(Y\ne f(X))=1-P(Y=f(X))$$</li>
<li>等价于风险经验最小化
$$\frac{1}{k}\sum\limits_{x_i\in N_k(x)} I(y_i\ne c_j)=1-\frac{1}{k}\sum\limits_{x_i\in N_k(x)} I(y_i=c_j)$$</li>
</ul>
<h3 id="模型预测" class="headerLink">
    <a href="#%e6%a8%a1%e5%9e%8b%e9%a2%84%e6%b5%8b" class="header-mark"></a>模型预测</h3><h4 id="预测流程" class="headerLink">
    <a href="#%e9%a2%84%e6%b5%8b%e6%b5%81%e7%a8%8b" class="header-mark"></a>预测流程</h4><p>首先引入最简单的思路：线性扫描的方法。</p>
<ol>
<li>对未知类别的数据集中的每个点：</li>
</ol>
<ul>
<li>计算已知类别数据集众多点与当前点之间的距离；</li>
<li>按照距离递增次序排序。</li>
</ul>
<ol start="2">
<li>选取与当前点距离最小的k个点：</li>
</ol>
<ul>
<li>选定前k个点所在类别的出现频率</li>
<li>返回前k个点出现频率最高的类别作为当前点的预测分类</li>
</ul>
<ol start="3">
<li>重复步骤，完成对所有点的预测分类</li>
</ol>
<h4 id="python实现" class="headerLink">
    <a href="#python%e5%ae%9e%e7%8e%b0" class="header-mark"></a>Python实现</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_train</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">X_train</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">k_nearest_labels</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">k_indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">most_common</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">k_nearest_labels</span><span class="p">)</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">most_common</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 一个简单的例子：</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">clf</span> <span class="o">=</span> <span class="n">KNN</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="改进" class="headerLink">
    <a href="#%e6%94%b9%e8%bf%9b" class="header-mark"></a>改进</h3><h4 id="主要挑战" class="headerLink">
    <a href="#%e4%b8%bb%e8%a6%81%e6%8c%91%e6%88%98" class="header-mark"></a>主要挑战</h4><ol>
<li>前置处理：特征的选择</li>
<li>模型</li>
</ol>
<ul>
<li>合适的度量函数</li>
<li>合适的K值</li>
<li>降低训练和预测的复杂度</li>
</ul>
<h4 id="kd树" class="headerLink">
    <a href="#kd%e6%a0%91" class="header-mark"></a>kd树</h4><p>一种二叉树数据结构，用于优化搜索算法。</p>
<p><strong>优势：</strong></p>
<ol>
<li>降低搜索维度</li>
<li>提高搜索效率</li>
<li>更少的存储需求</li>
<li>支持范围搜索</li>
</ol>
<p>可能因数据的特定分布而表现不佳。<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<h5 id="构造11" class="headerLink">
    <a href="#%e6%9e%84%e9%80%a011" class="header-mark"></a>构造<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></h5><ol>
<li>构造根结点，使根结点对应于 k 维空间中包含所有实例点的超矩形区域。</li>
<li>递归（生成子结点）：</li>
</ol>
<ul>
<li>选择坐标轴和切分点，确定一个超平面</li>
<li>将当前超矩形区域切分为左右两个子区域</li>
<li>直到子区域内没有实例时终止。</li>
</ul>
<ol start="3">
<li>实例保存在相应的结点上。</li>
</ol>
<p><strong>如何选择：</strong></p>
<ul>
<li>空间切分参照：坐标轴</li>
<li>切分点的选择：中位数</li>
</ul>
<h5 id="搜索" class="headerLink">
    <a href="#%e6%90%9c%e7%b4%a2" class="header-mark"></a>搜索</h5><div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>垃圾邮件过滤：自动将电子邮件分为垃圾邮件和非垃圾邮件。<br>
医学诊断：基于患者的症状数据来诊断疾病或预测病人的疾病风险。<br>
金融风险评估：根据客户的财务和信用记录来评估客户的信用风险。<br>
情感分析：根据文本数据中的情感内容对文本进行情感分类，如积极、消极或中性。<br>
图像识别：对图像进行分类，例如识别数字、物体或人脸等。<br>
生物信息学：基因序列分类，如预测蛋白质功能或基因表达模式。<br>
客户分类：根据客户的行为和偏好将客户分成不同的市场细分。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>KNN算法于1948年由Cover和Hart提出。<br>
存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，只选择样本数据集中前k个最相似的数据。k一般不大于20，最后，选择k个中出现次数最多的分类，作为新数据的分类。<br>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>数值型数据是指具有数量意义的数据，可以进行数学运算和比较。这种数据通常表示为数字，例如年龄、温度、身高等。在机器学习中，数值型数据常用于回归分析和连续变量的预测。标称型数据则是指无序分类的数据，其中每个值代表一个类别而没有数量意义。标称型数据通常表示为符号或字符串，例如血型、性别、品种等。在机器学习中，标称型数据通常用于分类问题，其中算法需要将输入数据映射到预定义的类别或标签。<br>
k最近邻算法 (kNN) 适用于处理这两种类型的数据。对于数值型数据，它可以基于数值之间的距离进行分类；对于标称型数据，它可以根据邻近样本的标签进行投票，并将测试样本分类为获得最多投票的类别。因此，kNN 算法对于这两种数据类型都有较好的适用性。<br>
还有其他类型：<br>
顺序型数据：顺序型数据是一种具有顺序或等级关系的数据类型，其中数据值之间存在某种顺序关系，但没有明确的数值差异。例如，学历等级（如小学、初中、高中、大学等）可以被视为顺序型数据。<br>
时间序列数据：时间序列数据是按照时间顺序排列的数据集合，通常是在一系列连续时间点上收集的数据。例如，股票价格、天气数据、经济指标等都属于时间序列数据。<br>
区间型数据：区间型数据是指数据值表示某个范围内的值，而不是特定的数值。这种数据类型通常用于表示测量的范围。例如，温度范围、年龄段等可以被视为区间型数据。<br>
比率型数据：比率型数据是具有固定比例关系的数据类型，其中数据之间存在明确的比率关系。比率型数据具有绝对零点，可以进行比较和数学运算。例如，长度、重量、时间间隔等都属于比率型数据。<br>
文本数据：文本数据是指以自然语言形式表示的数据，通常包含语句、段落或文档。处理文本数据通常需要使用自然语言处理技术来提取、转换和分析文本信息。<br>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>老师反馈：这里的表述并不严谨，模型需要通过交叉验证来确认参数 k&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>简单直观，非参数化：kNN 是一种非参数化方法，不对数据的分布做任何假设。因此，在处理复杂的数据集和未知的数据分布时，它通常具有很好的适应性。<br>
对异常值鲁棒：kNN 对异常值比较鲁棒，因为它基于周围数据点的多数投票来确定分类，可以减少异常值对结果的影响。<br>
适应多类别问题：kNN 能够很好地适应多类别分类问题，因为它可以通过投票的方式来确定一个实例所属的类别。<br>
但是对高维数据的处理效率较低，需要大量的存储空间和计算时间；在数据不平衡或噪声较多的情况下，它可能会产生较差的分类结果。<br>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>计算成本高：kNN 算法需要计算每个测试点与所有训练点之间的距离，因此在处理大规模数据集时，计算成本会变得非常高。<br>
存储成本高：除了计算成本高外，kNN 算法还需要存储整个训练集，这对于大规模数据集来说会占用大量的存储空间。<br>
维度灾难：随着数据维度的增加，kNN 算法的性能可能会下降，因为在高维空间中，数据点之间的距离变得更加稀疏，导致算法的效率降低。<br>
数据不平衡问题：在处理数据不平衡或噪声较多的数据集时，kNN 算法可能会受到数据分布的影响，从而导致分类性能下降。<br>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>近似误差是指模型用于近似真实关系的误差，通常表示模型与真实值之间的差异。<br>
估计误差是指使用样本数据估计整体数据集特征时产生的误差。在 kNN 中，估计误差通常与样本的选择和样本的分布有关。<br>
K值的减小：模型变得复杂，容易发生过拟合。相当于用较小的邻域中的训练实例进行预测，只有与输入实例较近的(相似的)训练实例才会对预测结果起作用，对噪声敏感，即预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。<br>
K值的增大：就意味着整体的模型变得简单.产生更平滑的决策边界，但可能会忽略数据的局部特征。这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误。k 值的增大就意味着整体的模型变得简单。<br>
k 值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k 值。具体可以取部分训练集作为测试集，在不同取值条件下观察最优值。&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>特征空间中两个实例点的距离是两个实例点相似程度的反映。k 近邻模型的特征空间一般是 n 维实数向量空间 $R^n$，使用的距离是欧氏距离，但也可以是其他距离，如更一般的 $L_p$ 距离或 Minkowski 距离。&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>实现 k 近邻法时，主要考虑的问题是如何对训练数据进行快速 k 近邻搜索。这点在特征空间的维数大及训练数据容量大时尤其必要。k 近邻法最简单的实现方法是线性扫描。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。为了提高k 近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。具体方法很多，下面介绍其中的 kd 树方法。<br>
使用 kd 树相比直接计算方法的主要好处在于它可以有效地减少计算量。kd 树是一种二叉树数据结构，它可以用于优化搜索算法，特别是在高维空间中。<br>
以下是 kd 树相对于直接计算方法的一些优势：<br>
降低搜索维度：kd 树能够将搜索范围缩小到与搜索点最近的局部区域，从而避免不必要的计算。<br>
提高搜索效率：在具有大量数据点的高维空间中，kd 树可以更快地定位最近邻居，因为它可以避免对所有数据点进行逐一比较。<br>
更少的存储需求：相对于直接计算方法，kd 树通常需要更少的存储空间，因为它可以通过二叉树结构有效地组织数据。<br>
支持范围搜索：除了最近邻搜索之外，kd 树还可以很容易地扩展到支持范围搜索，以查找在给定半径内的所有邻居。<br>
尽管 kd 树具有这些优势，但它可能会因数据的特定分布而表现不佳。例如，在存在大量密集聚集数据点的区域，kd 树的性能可能会下降。因此，在实际应用中，应该根据数据集的特点选择合适的算法来进行近邻搜索。&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>kd 树是一种对 k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd 树是二叉树，表示对 k 维空间的一个划分。构造 kd 树相当于不断地用垂直于坐标轴的超平面将k 维空间切分，构成一系列的飞 维超矩形区域。kd树的每个结点对应于一个k 维超矩形区域。<br>
构造 kd 树的方法如下:构造根结点，使根结点对应于 k 维空间中包含所有实例点的超矩形区域:通过下面的递归方法，不断地对 k 维空间进行切分，生成子结点。在超矩形区域(结点)上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。<br>
平衡树：使用中位数作为划分点可以保证树的相对平衡，避免出现极端情况下的不平衡树结构，从而使得搜索效率总体比较高，但未必最优。考虑一些离群点。&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>]]></description>
</item><item>
    <title>政治经济学：现代化的代价</title>
    <link>https://blog.ralvines.top/baciweiji/</link>
    <pubDate>Wed, 28 Jun 2023 20:20:40 &#43;0800</pubDate><author>
                        <name>Ralvine</name><uri>https://blog.ralvines.top/about.md</uri><email>ralvine@163.com</email></author><guid>https://blog.ralvines.top/baciweiji/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="https://z1.ax1x.com/2023/10/23/piA8fr4.png" referrerpolicy="no-referrer">
            </div><p>作为在“三农”领域深耕多年的乡建派学者，温铁军教授的《八次危机》一书深刻阐释了新中国经济社会发展的辉煌成果背后，所经历的八次“经济危机”，同时探讨了乡村对缓冲共和国工业化进程中多次危机的重要贡献。本书还剖析了宏观微观双重视角下的中国式现代化历程，提出了成本转嫁论，即城乡二元结构体制下的“三农”承载着城市资本危机的“软着陆”代价。</p>
<p>去意识形态化是本书的精髓。从序言开始，温教授便深入浅出地解构了当代经济学体系中西方话语体系的固有定势，并基于马克思主义政治经济学和几十年的本土研究成果阐述了数个振聋发聩的核心观点，以历史唯物主义精神力图还原被利益集团所扭曲的厚重历史。揆诸当下，百年未有之大变局下，各种黑天鹅、灰犀牛事件交织发生，国际竞争格局日益复杂，我国经济发展也到了新的攻坚期，书中对乡村振兴和城乡融合发展的思路对时下具有深刻的借鉴意义。</p>
<p>“工业化的背后必然有成本转嫁。”工业革命以来，工业化是各国现代化建设的主线，工业实力奠定了国家实力的基础，而工业化必然伴随着资本的原始积累。和中国类似的诸多后发独立国家在融入资本主义的世界生产体系后，为了获得建设的大量资金，不得不以牺牲部分主权来引入外部资本，而这种模式成为了后殖民主义时代老牌工业国家剥削它们的高明手段，在此类路径依赖中，发达国家得以将经济发展的成本与危机在开放市场的情境下转嫁给缺乏消化危机制度成本的后者，使得各国之间的马太效应更佳显著。</p>
<p>书中花费了大量篇幅分析新中国前三十年的工业化建设模式，从中可以清晰地认识到奠基我国重工业基础的代价是工农“剪刀差”和“全盘苏化”带来的部分权益之牺牲。通过举国动员和全国人民的辛苦建设，我们用农矿产品换来了开启工业化大门的最核心、最底层的支持，但也因为路径依赖在外交环境恶化后，在财政危机下迫不得已开展上山下乡运动。在这里，农村第一次展现了其化解城市危机的重要价值，为缺乏资本和技术的城市与供给过剩的知识分子提供了短暂的庇护所，“劳动力的集中投入成功地代替了长期绝对稀缺的资金，农村由其内生的稳定性成功地承接并化解了城市的危机”，深刻分析了当时我国的经济基础、上层建筑，特别是城乡生产关系对经济发展的影响。</p>
<p>城乡二元体制也是本书重点着墨的话题。温教授一针见血地指出我国“地方化”资源资本发展的客观事实。在很长时间内，地方的高度分权是央地财政分配体系的重要特征，“政府公司化”自1958年放权地方以来延续至今，使得政府成为经济活动中的另一重要引擎。而城乡二元结构带来的割裂，为工农产品“剪刀差”提供了社会条件。改革开放以来，广袤乡村的农民在非市场化制度下平均分配到无风险的资产，又将风险收益拱手让读于企业家和沿海发达地区的政府，他们为我国迈入社会主义市场经济体制亦做出了不可磨灭的贡献。“新中国成立后，农民仅通过这三种方式为国家建设积累资金就至少达到17.3万亿元”，可见“三农”在我国发展历程中的贡献如此之重。</p>
<p>回望当下，随着对外开放的进一步深入和金融资本同经济社会更深层的嵌合，为了防范化解重大危机，实现可持续发展，解决好“三农”问题成为时代的新命题。新世纪以来，资本相较于产业不断过剩，旧的经济发展主义向更为绿色、科学的精细化模式转变。进入免除了农业税的“后税费时代”，农业人口结构的老龄化所引发的劳动力稀缺危机弱化了农村的社会调节功能，如何以生态文明建设引领农村新型工业化和城镇化，将山水资源以货币形式自我资本化，将广袤农村的自然要素锚定于更具流通性的价值度量，推动城乡产业融合发展、促进商品流通和农村内需延扩，都是更为实质性的问题。而在知识阶层再一次渐趋过剩的今天，除了城市服务业蓄水池之外，如何进一步发挥好农村在解决产能和劳动力匹配性问题的作用，也是令人值得深思的问题。</p>]]></description>
</item></channel>
</rss>
