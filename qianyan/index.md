# 数学前沿专题讨论：遗传算法和统计学习方法


<!--more-->

{{< admonition quote "课程信息" true >}}
🎓 数学科学学院<br>
🕙 2023-2024 秋冬<br>
🧑‍🏫 毕惟红<br>
📝 50%读书报告，10%考勤，10%编程，30%两次展示
{{< /admonition >}}

## 背景

1. 浙大 计算数学
2. 研究生
- 数字模拟 投影法求曲面面积
- 随机数生成 数值代数 多元非线性方程组求解

**研究方向**

1. 图像处理
- 图像分割 图像识别
- 图像加密 做的比较好
2. 语义识别
- 三维点式数据（无人驾驶、激光雷达）
3. 社区发现
- 复杂网络
- 拟牛顿

## 遗传算法

## 统计学习方法

### 首次展示：kNN

#### 分类问题[^1]

- 一种监督学习问题，旨在对数据分类
- 将输入数据映射到预定义的类别或标签
- 从已知的训练数据中学习一个分类模型，然后将该模型应用于新的、未知的数据，以预测其所属的类别

- 垃圾邮件过滤、金融风险评估
- 医学诊断、生物信息学
- 情感分析、客户分类
- 图像识别


[^1]: 垃圾邮件过滤：自动将电子邮件分为垃圾邮件和非垃圾邮件。<br>
医学诊断：基于患者的症状数据来诊断疾病或预测病人的疾病风险。<br>
金融风险评估：根据客户的财务和信用记录来评估客户的信用风险。<br>
情感分析：根据文本数据中的情感内容对文本进行情感分类，如积极、消极或中性。<br>
图像识别：对图像进行分类，例如识别数字、物体或人脸等。<br>
生物信息学：基因序列分类，如预测蛋白质功能或基因表达模式。<br>
客户分类：根据客户的行为和偏好将客户分成不同的市场细分。


#### 提出[^2]

$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$
$$y_i\in\mathcal{Y}=\{c_1,c_2,\cdots,c_K\}$$
$$y=\text{arg}\max\limits_{c_j} \sum\limits_{x_i\in N_k(x)} I(y_i=c_j)$$

[^2]: KNN算法于1948年由Cover和Hart提出。<br>
存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，只选择样本数据集中前k个最相似的数据。k一般不大于20，最后，选择k个中出现次数最多的分类，作为新数据的分类。<br>

- 输入：特征向量（空间点）
- 输出：类别（可以取多类）
- 已标注的训练集

- 预测：多数表决（“近朱者赤” ）
- 不具有显式的学习过程

**适用范围**
- 数值型和标称型[^3]

[^3]: 数值型数据是指具有数量意义的数据，可以进行数学运算和比较。这种数据通常表示为数字，例如年龄、温度、身高等。在机器学习中，数值型数据常用于回归分析和连续变量的预测。标称型数据则是指无序分类的数据，其中每个值代表一个类别而没有数量意义。标称型数据通常表示为符号或字符串，例如血型、性别、品种等。在机器学习中，标称型数据通常用于分类问题，其中算法需要将输入数据映射到预定义的类别或标签。<br>
k最近邻算法 (kNN) 适用于处理这两种类型的数据。对于数值型数据，它可以基于数值之间的距离进行分类；对于标称型数据，它可以根据邻近样本的标签进行投票，并将测试样本分类为获得最多投票的类别。因此，kNN 算法对于这两种数据类型都有较好的适用性。<br>
还有其他类型：<br>
顺序型数据：顺序型数据是一种具有顺序或等级关系的数据类型，其中数据值之间存在某种顺序关系，但没有明确的数值差异。例如，学历等级（如小学、初中、高中、大学等）可以被视为顺序型数据。<br>
时间序列数据：时间序列数据是按照时间顺序排列的数据集合，通常是在一系列连续时间点上收集的数据。例如，股票价格、天气数据、经济指标等都属于时间序列数据。<br>
区间型数据：区间型数据是指数据值表示某个范围内的值，而不是特定的数值。这种数据类型通常用于表示测量的范围。例如，温度范围、年龄段等可以被视为区间型数据。<br>
比率型数据：比率型数据是具有固定比例关系的数据类型，其中数据之间存在明确的比率关系。比率型数据具有绝对零点，可以进行比较和数学运算。例如，长度、重量、时间间隔等都属于比率型数据。<br>
文本数据：文本数据是指以自然语言形式表示的数据，通常包含语句、段落或文档。处理文本数据通常需要使用自然语言处理技术来提取、转换和分析文本信息。<br>

**优点**[^4][^5]
1. 直观、非参数化
2. 对异常值不敏感
3. 支持多类别

[^4]: 老师反馈：这里的表述并不严谨，模型需要通过交叉验证来确认参数 k
[^5]: 简单直观，非参数化：kNN 是一种非参数化方法，不对数据的分布做任何假设。因此，在处理复杂的数据集和未知的数据分布时，它通常具有很好的适应性。<br>
对异常值鲁棒：kNN 对异常值比较鲁棒，因为它基于周围数据点的多数投票来确定分类，可以减少异常值对结果的影响。<br>
适应多类别问题：kNN 能够很好地适应多类别分类问题，因为它可以通过投票的方式来确定一个实例所属的类别。<br>
但是对高维数据的处理效率较低，需要大量的存储空间和计算时间；在数据不平衡或噪声较多的情况下，它可能会产生较差的分类结果。<br>

**缺点**[^6]
1. 时间复杂度高
2. 存储成本高
3. “维度灾难”和数据不平衡

[^6]: 计算成本高：kNN 算法需要计算每个测试点与所有训练点之间的距离，因此在处理大规模数据集时，计算成本会变得非常高。<br>
存储成本高：除了计算成本高外，kNN 算法还需要存储整个训练集，这对于大规模数据集来说会占用大量的存储空间。<br>
维度灾难：随着数据维度的增加，kNN 算法的性能可能会下降，因为在高维空间中，数据点之间的距离变得更加稀疏，导致算法的效率降低。<br>
数据不平衡问题：在处理数据不平衡或噪声较多的数据集时，kNN 算法可能会受到数据分布的影响，从而导致分类性能下降。<br>


#### kNN模型构建流程

给定距离度量，k值与决策规则 [输入训练集T]
1. 在训练集 T 中找出与 x 最邻近的 k 个点，涵盖这 个点的 x 的邻域记作 $N_k(a)$
2. 在 $N_k(a)$ 中根据分类决策规则决定 x 的类别 y

**基本要素**
1. k 值选择
2. 距离度量
3. 决策规则

特殊情况：最近邻（k=1）

[^7]: 指标函数 $I$: 满足条件时为1.<br>
k 近邻法中，当训练集、距离度量(如欧氏距离)、k 值及分类决策规则(如多数表决)确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。这一事实从最近邻算法中可以看得很清楚。<br>
特征空间中，对每个训练实例点 2，距离该点比其他点更近的所有点组成一个区域，叫作单元(cell)。每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例 ai的类 yi 作为其单元中所有点的类标记(classlabel)。这样，每个单元的实例点的类别是确定的。二维特征空间划分的一个例子。

#### k 值选择

| k值 |偏小 | 偏大 |
|-----| ------ | ------ | 
| 近似误差 | 减小   | 增大 | 
| 估计误差 | 增大   | 减小 | 

交叉验证以提高泛化性能。[^8]

[^8]: 近似误差是指模型用于近似真实关系的误差，通常表示模型与真实值之间的差异。<br>
估计误差是指使用样本数据估计整体数据集特征时产生的误差。在 kNN 中，估计误差通常与样本的选择和样本的分布有关。<br>
K值的减小：模型变得复杂，容易发生过拟合。相当于用较小的邻域中的训练实例进行预测，只有与输入实例较近的(相似的)训练实例才会对预测结果起作用，对噪声敏感，即预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k 值的减小就意味着整体模型变得复杂，容易发生过拟合。<br>
K值的增大：就意味着整体的模型变得简单.产生更平滑的决策边界，但可能会忽略数据的局部特征。这时与输入实例较远的(不相似的)训练实例也会对预测起作用，使预测发生错误。k 值的增大就意味着整体的模型变得简单。<br>
k 值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k 值。具体可以取部分训练集作为测试集，在不同取值条件下观察最优值。

#### 距离度量[^9]

对于
$$x_i=(x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})$$

- $L_p$ 距离
$$L_p(x_i,x_j)=(\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^p)^{1/p}$$
- 欧氏距离
$$L_2(x_i,x_j)=(\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|^2)^{1/2}$$
- 曼哈顿距离
$$L_1(x_i,x_j)=\sum\limits_{l=1}^n |x_i^{(l)}-x_j^{(l)}|$$
- $L_\infty$ 距离
$$L_\infty (x_i,x_j)=(\max\limits_l |x_i^{(l)}-x_j^{(l)}|$$

[^9]: 特征空间中两个实例点的距离是两个实例点相似程度的反映。k 近邻模型的特征空间一般是 n 维实数向量空间 $R^n$，使用的距离是欧氏距离，但也可以是其他距离，如更一般的 $L_p$ 距离或 Minkowski 距离。

#### 决策规则

**多数表决**
由输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类。

- 分类函数
$$f:\mathbb{R}^n\rightarrow \{c_1,c_2,\cdots,c_K\}$$
- 误分类概率
$$P(Y\ne f(X))=1-P(Y=f(X))$$
- 等价于风险经验最小化
$$\frac{1}{k}\sum\limits_{x_i\in N_k(x)} I(y_i\ne c_j)=1-\frac{1}{k}\sum\limits_{x_i\in N_k(x)} I(y_i=c_j)$$

#### 分类模型预测流程

首先引入最简单的思路：线性扫描的方法。

1. 对未知类别的数据集中的每个点：
- 计算已知类别数据集众多点与当前点之间的距离；
- 按照距离递增次序排序。
2. 选取与当前点距离最小的k个点：
- 选定前k个点所在类别的出现频率
- 返回前k个点出现频率最高的类别作为当前点的预测分类
3. 重复步骤，完成对所有点的预测分类

#### 

